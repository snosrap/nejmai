{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400390",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400390",
      "title": "Fine-Tuning LLMs with Medical Data: Can Safety Be Ensured?",
      "summary": "This study examines the vulnerability of large-scale language models (LLMs) tuned to medical data to adversarial attacks. The study shows that such models can expose sensitive patient information. The results demonstrate the effectiveness of prompt-based attacks in compromising privacy, and underscore the critical need for enhanced defenses in the training of health care\u2013focused LLMs.",
      "content_text": "## Abstract\n\nDeveloping large-scale language models (LLMs) for health care requires fine-\ntuning with health care domain data suitable for downstream tasks. However,\nfine-tuning LLMs with medical data can expose the training data used during\nlearning to adversarial attacks. This issue is particularly important as\nmedical data contain sensitive and identifiable patient data. The prompt-based\nadversarial attack approach was employed to assess the potential for medical\nprivacy breaches in LLMs. The success rate of the attack was evaluated by\ncategorizing 71 medical questions into three key metrics. To confirm the\nexposure of LLMs training data, each case was compared with the original\nelectronic medical record. The security of the model was confirmed to be\ncompromised by the prompt attack method, resulting in a jailbreak (i.e.,\nsecurity breach). The American Standard Code for Information Interchange code\nencoding method had a success rate of up to 80.8% in disabling the guardrail.\nThe success rate of attacks that caused the model to expose part of the\ntraining data was up to 21.8%. These findings underscore the critical need for\nrobust defense strategies to protect patient privacy and maintain the\nintegrity of medical information. Addressing these vulnerabilities is crucial\nfor integrating LLMs into clinical workflows safely, balancing the benefits of\nadvanced artificial intelligence technologies with the need to protect\nsensitive patient data. (Funded by the Korea Health Industry Development\nInstitute and the Ministry of Health & Welfare, Republic of Korea.)\n\n",
      "date_published": "2024-12-24T00:00:00+00:00",
      "authors": [
        {
          "name": "M. Kim and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400390",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/f963585d-969b-411a-be34-84ea53d204e6/aics2400390_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Developing large-scale language models (LLMs) for health care requires fine-tuning with health care domain data suitable for downstream tasks. However, fine-tuning LLMs with medical data can expose the training data used during learning to adversarial attacks. This issue is particularly important as medical data contain sensitive and identifiable patient data. The prompt-based adversarial attack approach was employed to assess the potential for medical privacy breaches in LLMs. The success rate of the attack was evaluated by categorizing 71 medical questions into three key metrics. To confirm the exposure of LLMs training data, each case was compared with the original electronic medical record. The security of the model was confirmed to be compromised by the prompt attack method, resulting in a jailbreak (i.e., security breach). The American Standard Code for Information Interchange code encoding method had a success rate of up to 80.8% in disabling the guardrail. The success rate of attacks that caused the model to expose part of the training data was up to 21.8%. These findings underscore the critical need for robust defense strategies to protect patient privacy and maintain the integrity of medical information. Addressing these vulnerabilities is crucial for integrating LLMs into clinical workflows safely, balancing the benefits of advanced artificial intelligence technologies with the need to protect sensitive patient data. (Funded by the Korea Health Industry Development Institute and the Ministry of Health &amp; Welfare, Republic of Korea.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400360",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400360",
      "title": "Zero-Shot Clinical Trial Patient Matching with LLMs",
      "summary": "We investigate the accuracy, efficiency, and interpretability of using large language models for clinical trial patient matching. While prior work has focused mostly on fine-tuning and few-shot prompting, we are specifically interested in scalable approaches with an eye toward real-world deployment, and thus our study focuses on the zero-shot performance of these models to scale to arbitrary trials and we develop a two-stage retrieval pipeline to scale to arbitrary length patient records.",
      "content_text": "## Abstract\n\nMatching patients to clinical trials is a key challenge in bringing new drugs\nto market. Identifying patients who meet eligibility criteria for a trial is\nhighly manual, taking up to 1 hour per patient. Automated screening is\nchallenging, however, as it requires the ability to understand unstructured\nclinical text. To address this, we have designed a zero-shot large language\nmodel (LLM)\u2013based system that evaluates a patient\u2019s medical history (as\nunstructured clinical text) against trial inclusion criteria (also specified\nas free text). We investigate different prompting strategies and design a\nnovel two-stage retrieval pipeline to reduce the number of tokens processed by\nup to a third while sustaining high performance. Our contributions are\nthreefold. First, we achieve state-of-the-art performance on the 2018 n2c2\ncohort selection challenge, the largest public benchmark for clinical trial\npatient matching. Second, this system can improve the data and cost efficiency\nof matching patients an order of magnitude faster and more affordably than the\nstatus quo. Third, we demonstrate the interpretability of our system by\ngenerating natural language justifications for each eligibility decision,\nwhich clinicians found coherent in 97% of correct decisions and 75% of\nincorrect ones. These results establish the feasibility of using LLMs to\naccelerate clinical trial operations, with the zero-shot retrieval\narchitecture scalable to arbitrary trials and patient record length with\nminimal reconfiguration. (Funded by the Clinical Excellence Research Center at\nStanford Medicine and others.)\n\n",
      "date_published": "2024-12-24T00:00:00+00:00",
      "authors": [
        {
          "name": "M. Wornow and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400360",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/fb639701-04a9-43c6-9a35-a2a49c851016/aics2400360_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Matching patients to clinical trials is a key challenge in bringing new drugs to market. Identifying patients who meet eligibility criteria for a trial is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires the ability to understand unstructured clinical text. To address this, we have designed a zero-shot large language model (LLM)\u2013based system that evaluates a patient\u2019s medical history (as unstructured clinical text) against trial inclusion criteria (also specified as free text). We investigate different prompting strategies and design a novel two-stage retrieval pipeline to reduce the number of tokens processed by up to a third while sustaining high performance. Our contributions are threefold. First, we achieve state-of-the-art performance on the 2018 n2c2 cohort selection challenge, the largest public benchmark for clinical trial patient matching. Second, this system can improve the data and cost efficiency of matching patients an order of magnitude faster and more affordably than the status quo. Third, we demonstrate the interpretability of our system by generating natural language justifications for each eligibility decision, which clinicians found coherent in 97% of correct decisions and 75% of incorrect ones. These results establish the feasibility of using LLMs to accelerate clinical trial operations, with the zero-shot retrieval architecture scalable to arbitrary trials and patient record length with minimal reconfiguration. (Funded by the Clinical Excellence Research Center at Stanford Medicine and others.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "title": "RAG in Health Care: A Novel Framework for Improving Communication and Decision-Making by Addressing LLM Limitations",
      "summary": "This review article explores and evaluates the opportunities in using retrieval augmented generation (RAG) to overcome existing LLM limitations when used within the healthcare and pharmaceutical domain. It provides a framework of how RAG works and describes its strengths and weaknesses when applied to the field of medicine.",
      "content_text": "## Abstract\n\nWithin the field of artificial intelligence (AI), large language models (LLMs)\nhave the potential to transform the delivery of medical information. LLMs, as\na subset of generative AI, have demonstrated value in content creation, idea\ngeneration, and interactive communication. However, their inherent\nlimitations, such as the need for up-to-date information, hallucinations of\nincorrect facts, and a reliance on public-domain data, restrict the full\npotential of generative AI within the health care setting. To address these\nlimitations, retrieval-augmented generation (RAG) offers a novel framework by\nconnecting LLMs with external knowledge, enabling them to access information\nbeyond their training data. Within the health care domain, additional datasets\ncould include peer-reviewed studies, gated medical compendiums, and the\ninternal policies of health care organizations such as hospitals or\npharmaceutical companies. By leveraging RAG, existing generative AI tools gain\nthe capability to consider both public and private information, expanding\ntheir application and enhancing accuracy and relevance within the health care\nsetting. The utility of RAG in the health care setting has yet to be fully\nexplored, but it has the potential to revolutionize the industry. This article\nseeks to outline present and future use cases of RAG for health care\ninformation exchange within both clinical and industrial settings.\n\n",
      "date_published": "2024-12-23T00:00:00+00:00",
      "authors": [
        {
          "name": "K.K.Y. Ng, I. Matsuba, and P.C. Zhang"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2400380",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIra2400380_Zhang-1734963181703.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Within the field of artificial intelligence (AI), large language models (LLMs) have the potential to transform the delivery of medical information. LLMs, as a subset of generative AI, have demonstrated value in content creation, idea generation, and interactive communication. However, their inherent limitations, such as the need for up-to-date information, hallucinations of incorrect facts, and a reliance on public-domain data, restrict the full potential of generative AI within the health care setting. To address these limitations, retrieval-augmented generation (RAG) offers a novel framework by connecting LLMs with external knowledge, enabling them to access information beyond their training data. Within the health care domain, additional datasets could include peer-reviewed studies, gated medical compendiums, and the internal policies of health care organizations such as hospitals or pharmaceutical companies. By leveraging RAG, existing generative AI tools gain the capability to consider both public and private information, expanding their application and enhancing accuracy and relevance within the health care setting. The utility of RAG in the health care setting has yet to be fully explored, but it has the potential to revolutionize the industry. This article seeks to outline present and future use cases of RAG for health care information exchange within both clinical and industrial settings.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "title": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations",
      "summary": "We set out recommendations enabling those creating and/or using health care datasets to identify and transparently acknowledge underlying biases.",
      "content_text": "## Abstract\n\nWithout careful dissection of the ways in which biases can be encoded into\nartificial intelligence (AI) health technologies, there is a risk of\nperpetuating existing health inequalities at scale. One major source of bias\nis the data that underpins such technologies. The STANDING Together\nrecommendations aim to encourage transparency regarding limitations of health\ndatasets and proactive evaluation of their effect across population groups.\nDraft recommendation items were informed by a systematic review and\nstakeholder survey. The recommendations were developed using a Delphi\napproach, supplemented by a public consultation and international interview\nstudy. Overall, more than 350 representatives from 58 countries provided input\ninto this initiative. 194 Delphi participants from 25 countries voted and\nprovided comments on 32 candidate items across three electronic survey rounds\nand one in-person consensus meeting. The 29 STANDING Together consensus\nrecommendations are presented here in two parts. Recommendations for\nDocumentation of Health Datasets provide guidance for dataset curators to\nenable transparency around data composition and limitations. Recommendations\nfor Use of Health Datasets aim to enable identification and mitigation of\nalgorithmic biases that might exacerbate health inequalities. These\nrecommendations are intended to prompt proactive inquiry rather than acting as\na checklist. We hope to raise awareness that no dataset is free of\nlimitations, so transparent communication of data limitations should be\nperceived as valuable, and absence of this information as a limitation. We\nhope that adoption of the STANDING Together recommendations by stakeholders\nacross the AI health technology lifecycle will enable everyone in society to\nbenefit from technologies which are safe and effective. (Funded by The NHS AI\nLab and The Health Foundation, and supported by the National Institute for\nHealth and Care Research [NIHR].)\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "J.E. Alderman and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401088",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective. (Funded by The NHS AI Lab and The Health Foundation, and supported by the National Institute for Health and Care Research [NIHR].)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "title": "How Generalizable Are Foundation Models When Applied to Different Demographic Groups and Settings?",
      "summary": "An evaluation of the foundational model RETFound on an Asian-specific dataset indicates the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need to include more diverse data and collaborate globally on research.",
      "content_text": "## Abstract\n\nRETFound is a retinal image\u2013based foundational artificial intelligence (AI)\nmodel that can be fine-tuned to downstream tasks. However, its\ngeneralizability to Asian populations remains unclear. In this study, we fine-\ntuned RETFound on an Asian-specific dataset. We then evaluated the performance\nof RETFound versus a conventional Vision Transformer model (pretrained on\nImageNet) in diagnosing glaucoma and coronary heart disease and predicting the\n3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d\ndataset, RETFound showed no significant improvement compared with a\nconventional Vision Transformer model (area under the curves [AUCs] of 0.863,\n0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2).\nFurthermore, in scenarios with limited training data (fine-tuned on \u226425% of\nthe full dataset), RETFound showed a slight advantage (up to a maximum AUC\nincrease of 0.03). However, these improvements were not statistically\nsignificant (all P\u22650.2). These findings indicate the challenges foundational\nAI models face in adapting to diverse demographics, emphasizing the need for\nmore diverse data in current foundation models and the importance of global\ncollaboration on foundation model research.\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "Z. Xiong and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400497",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/a7df80bc-2fdc-4805-abca-a9270bb157ec/aics2400497_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">RETFound is a retinal image\u2013based foundational artificial intelligence (AI) model that can be fine-tuned to downstream tasks. However, its generalizability to Asian populations remains unclear. In this study, we fine-tuned RETFound on an Asian-specific dataset. We then evaluated the performance of RETFound versus a conventional Vision Transformer model (pretrained on ImageNet) in diagnosing glaucoma and coronary heart disease and predicting the 3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d dataset, RETFound showed no significant improvement compared with a conventional Vision Transformer model (area under the curves [AUCs] of 0.863, 0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2). Furthermore, in scenarios with limited training data (fine-tuned on \u226425% of the full dataset), RETFound showed a slight advantage (up to a maximum AUC increase of 0.03). However, these improvements were not statistically significant (all P\u22650.2). These findings indicate the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need for more diverse data in current foundation models and the importance of global collaboration on foundation model research.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401000",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401000",
      "title": "Reclaiming Voice with AI",
      "summary": "While AI is often dual use and concerns about voice cloning often center on potential misuse, we argue that suppressing this technology may inflict tangible harm on patients by denying them the chance to reclaim their voice.",
      "content_text": "## Abstract\n\nVoice impairments affect millions of Americans, with personalized text-to-\nspeech technology offering limited solutions due to the need for extensive\nvoice banking. Here, we report the case of Alexis Bogan, a 20-year-old patient\nwho acutely lost her voice after surgery to resect her brain stem\nhemangioblastoma. In a world-first application, OpenAI\u2019s Voice Engine was used\nto clone Ms. Bogan\u2019s voice from just 15 seconds of preexisting audio, sourced\nfrom a school project she had filmed a few years prior. This enabled her to\nuse a personalized text-to-speech app for daily communication while\nrehabilitating her speech. This case was highlighted on a recent episode of\nthe _NEJM AI_ Grand Rounds podcast,1 framing a broader discussion on voice\ncloning technology. While AI is often dual use and concerns about voice\ncloning often center on potential misuse, such as \u201cdeepfakes\u201d and\nmisinformation, we argue that suppressing this technology may inflict tangible\nharm on patients by denying them the chance to reclaim their voice. Inspired\nby Ms. Bogan\u2019s journey, we urge researchers, clinicians, ethicists,\npolicymakers, and tech companies to collaborate swiftly yet responsibly in\nadvancing AI voice cloning in health care. By doing so, we can empower\npatients to recover not just their voice, but also a fundamental aspect of\ntheir identity and quality of life.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "F.N. Mirza and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401000",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIp2401000_Ali-1732707614033.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Voice impairments affect millions of Americans, with personalized text-to-speech technology offering limited solutions due to the need for extensive voice banking. Here, we report the case of Alexis Bogan, a 20-year-old patient who acutely lost her voice after surgery to resect her brain stem hemangioblastoma. In a world-first application, OpenAI\u2019s Voice Engine was used to clone Ms. Bogan\u2019s voice from just 15 seconds of preexisting audio, sourced from a school project she had filmed a few years prior. This enabled her to use a personalized text-to-speech app for daily communication while rehabilitating her speech. This case was highlighted on a recent episode of the <i>NEJM AI</i> Grand Rounds podcast,<sup>1</sup> framing a broader discussion on voice cloning technology. While AI is often dual use and concerns about voice cloning often center on potential misuse, such as \u201cdeepfakes\u201d and misinformation, we argue that suppressing this technology may inflict tangible harm on patients by denying them the chance to reclaim their voice. Inspired by Ms. Bogan\u2019s journey, we urge researchers, clinicians, ethicists, policymakers, and tech companies to collaborate swiftly yet responsibly in advancing AI voice cloning in health care. By doing so, we can empower patients to recover not just their voice, but also a fundamental aspect of their identity and quality of life.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400867",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400867",
      "title": "FDA-Authorized AI/ML Tool for Sepsis Prediction: Development and Validation",
      "summary": "Sepsis is a life-threatening acute condition that requires accurate and rapid identification to guide proper treatment. This study outlines the development and validation of the first U.S. Food and Drug Administration\u2013authorized artificial intelligence\u2013based software to identify patients at risk of having sepsis.",
      "content_text": "## Abstract\n\n### Background\n\nSepsis is a life-threatening condition that demands prompt treatment for\nimproved patient outcomes. Its heterogenous presentation makes early detection\nchallenging, highlighting the need for effective risk assessment tools.\nArtificial intelligence (AI) models could potentially identify patients with\nsepsis, but none have previously been authorized by the U.S. Food and Drug\nAdministration (FDA) for commercial use. This study outlines the development\nand validation of the Sepsis ImmunoScore, the first FDA-authorized AI-based\nsoftware designed to identify patients at risk of sepsis.\n\n### Methods\n\nIn this prospective study, we enrolled adult patients (18+ years of age)\nsuspected of infection, as indicated by a blood culture order, from five U.S.\ninstitutions between April 2017 and July 2022. The participants were divided\ninto an algorithm development cohort (n=2366), an internal validation cohort\n(n=393), and an external validation cohort (n=698). The primary end point was\nsepsis presence (as defined by Sepsis-3) within 24 hours of test initiation.\nSecondary end points included length of hospital stay, intensive care unit\n(ICU) admission within 24 hours, mechanical ventilation use within 24 hours,\nvasopressor use within 24 hours, and in-hospital mortality.\n\n### Results\n\nThe Sepsis ImmunoScore demonstrated high diagnostic accuracy, with an area\nunder the curve of 0.85 (0.83 to 0.87) in the derivation cohort, 0.80 (0.74 to\n0.86) in internal validation, and 0.81 (0.77 to 0.86) in external validation.\nThe scores were categorized into four sepsis risk levels with corresponding\nlikelihood ratios: low (0.1), medium (0.5), high (2.1), and very high (8.3).\nThese risk categories also predicted in-hospital mortality: low (0.0%), medium\n(1.9%), high (8.7%), and very high (18.2%) in the external validation cohort.\nSimilar trends were observed for other metrics, such as length of hospital\nstay, ICU utilization, mechanical ventilation, and vasopressor use.\n\n### Conclusions\n\nThe Sepsis ImmunoScore demonstrated high accuracy for the identification and\nprediction of sepsis and critical illness metrics that could enable prompt\nidentification of patients at high risk of sepsis and adverse outcomes,\npotentially improving clinical decision-making and patient outcomes. (Funded\nby the Defense Threat Reduction Agency and others.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "A. Bhargava and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400867",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/23154c4d-4969-464d-962f-389b497979ee/aioa2400867_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Sepsis is a life-threatening condition that demands prompt treatment for improved patient outcomes. Its heterogenous presentation makes early detection challenging, highlighting the need for effective risk assessment tools. Artificial intelligence (AI) models could potentially identify patients with sepsis, but none have previously been authorized by the U.S. Food and Drug Administration (FDA) for commercial use. This study outlines the development and validation of the Sepsis ImmunoScore, the first FDA-authorized AI-based software designed to identify patients at risk of sepsis.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">In this prospective study, we enrolled adult patients (18+ years of age) suspected of infection, as indicated by a blood culture order, from five U.S. institutions between April 2017 and July 2022. The participants were divided into an algorithm development cohort (n=2366), an internal validation cohort (n=393), and an external validation cohort (n=698). The primary end point was sepsis presence (as defined by Sepsis-3) within 24 hours of test initiation. Secondary end points included length of hospital stay, intensive care unit (ICU) admission within 24 hours, mechanical ventilation use within 24 hours, vasopressor use within 24 hours, and in-hospital mortality.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">The Sepsis ImmunoScore demonstrated high diagnostic accuracy, with an area under the curve of 0.85 (0.83 to 0.87) in the derivation cohort, 0.80 (0.74 to 0.86) in internal validation, and 0.81 (0.77 to 0.86) in external validation. The scores were categorized into four sepsis risk levels with corresponding likelihood ratios: low (0.1), medium (0.5), high (2.1), and very high (8.3). These risk categories also predicted in-hospital mortality: low (0.0%), medium (1.9%), high (8.7%), and very high (18.2%) in the external validation cohort. Similar trends were observed for other metrics, such as length of hospital stay, ICU utilization, mechanical ventilation, and vasopressor use.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">The Sepsis ImmunoScore demonstrated high accuracy for the identification and prediction of sepsis and critical illness metrics that could enable prompt identification of patients at high risk of sepsis and adverse outcomes, potentially improving clinical decision-making and patient outcomes. (Funded by the Defense Threat Reduction Agency and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2300221",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2300221",
      "title": "Development and Validation of a Multimodal Multitask Vision\n                    Foundation Model for Generalist Ophthalmic Artificial\n                    Intelligence",
      "summary": "A proposal for an AI foundation model for ophthalmology that can process eight ophthalmic imaging modalities and adapt to a multitude of ophthalmic scenarios and applications.",
      "content_text": "## Abstract\n\n### Background\n\nSpecialized single-use, single-modality models often have limited or no\ngeneralization to new diseases, modalities, and clinical tasks. Foundation\nmodels are built for multipurpose use, enabling them to perform tasks even\nwhen not specifically pretrained for them, and to adapt to different clinical\napplications.\n\n### Methods\n\nWe present VisionFM, an artificial intelligence foundation model for\nophthalmology pretrained on 3.4 million images from over 500,000 individuals,\ncovering diverse ophthalmic diseases, imaging modalities and devices, and\nclinical scenarios. Pretrained based on eight modalities, VisionFM was tested\nfor multiple applications, including disease screening and detection,\nprognosis and prediction, and segmentation of lesions and anatomical\nstructures, on an ophthalmic database comprising 53 public and 12 private\ndatasets. We compared the model against ophthalmologists with varying\nexperience in ophthalmic and systemic disease diagnoses.\n\n### Results\n\nVisionFM outperformed baseline deep learning approaches in diagnosing ocular\ndiseases, achieving an average area under the receiver operating\ncharacteristic curve (AUROC) of 0.950 (95% confidence interval [CI], 0.941 to\n0.959) across eight disease categories and five imaging modalities in internal\nvalidation. In external validation, VisionFM achieved an AUROC of 0.945 (95%\nCI, 0.934 to 0.956) in fundus-based diabetic retinopathy recognition, and an\nAUROC of 0.974 (95% CI, 0.966 to 0.983) in optical coherence tomography\u2013based\nage-related macular degeneration recognition. In a comparative study of\ndiagnostic accuracy of 12 ocular diseases from fundus photographs, VisionFM\nshows diagnostic accuracy close to that of intermediate-level\nophthalmologists. Its generalizability extends to new imaging modalities and\ndevices, effectively handling dataset shifts. For example, VisionFM accurately\ngraded diabetic retinopathy with an AUROC of 0.935 (95% CI, 0.902 to 0.964)\nusing an imaging modality it was never exposed to during pretraining.\nFurthermore, VisionFM is able to predict both glaucoma progression and the\npresence of intracranial tumors directly from fundus photographs.\n\n### Conclusions\n\nVisionFM provides an efficient platform for diagnosis or prediction of\nmultiple diseases using multiple imaging modalities and is scalable to\nincorporate additional data, modalities, and applications via its open-sourced\nmodel weights and codebase. (Funded by the Research Grants Council (RGC) of\nHong Kong SAR and others.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Qiu and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2300221",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/4171a5db-da90-45e7-9ec3-4dd11dc5f849/aioa2300221_f3.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Specialized single-use, single-modality models often have limited or no generalization to new diseases, modalities, and clinical tasks. Foundation models are built for multipurpose use, enabling them to perform tasks even when not specifically pretrained for them, and to adapt to different clinical applications.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We present VisionFM, an artificial intelligence foundation model for ophthalmology pretrained on 3.4 million images from over 500,000 individuals, covering diverse ophthalmic diseases, imaging modalities and devices, and clinical scenarios. Pretrained based on eight modalities, VisionFM was tested for multiple applications, including disease screening and detection, prognosis and prediction, and segmentation of lesions and anatomical structures, on an ophthalmic database comprising 53 public and 12 private datasets. We compared the model against ophthalmologists with varying experience in ophthalmic and systemic disease diagnoses.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">VisionFM outperformed baseline deep learning approaches in diagnosing ocular diseases, achieving an average area under the receiver operating characteristic curve (AUROC) of 0.950 (95% confidence interval [CI], 0.941 to 0.959) across eight disease categories and five imaging modalities in internal validation. In external validation, VisionFM achieved an AUROC of 0.945 (95% CI, 0.934 to 0.956) in fundus-based diabetic retinopathy recognition, and an AUROC of 0.974 (95% CI, 0.966 to 0.983) in optical coherence tomography\u2013based age-related macular degeneration recognition. In a comparative study of diagnostic accuracy of 12 ocular diseases from fundus photographs, VisionFM shows diagnostic accuracy close to that of intermediate-level ophthalmologists. Its generalizability extends to new imaging modalities and devices, effectively handling dataset shifts. For example, VisionFM accurately graded diabetic retinopathy with an AUROC of 0.935 (95% CI, 0.902 to 0.964) using an imaging modality it was never exposed to during pretraining. Furthermore, VisionFM is able to predict both glaucoma progression and the presence of intracranial tumors directly from fundus photographs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">VisionFM provides an efficient platform for diagnosis or prediction of multiple diseases using multiple imaging modalities and is scalable to incorporate additional data, modalities, and applications via its open-sourced model weights and codebase. (Funded by the Research Grants Council (RGC) of Hong Kong SAR and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401024",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401024",
      "title": "A New Foundation Model for Multimodal Ophthalmic Images: Advancing Disease Detection and Prediction",
      "summary": "A new foundation model for ophthalmic images demonstrates important progress, particularly through its flexible approach to multimodal training and its application to image segmentation tasks.",
      "content_text": "## Abstract\n\nFoundation models are a powerful tool in ophthalmology for building\ngeneralizable systems that can be efficiently applied to a range of ocular and\nsystemic health tasks. A new foundation model for ophthalmic images\ndemonstrates important progress, particularly through its flexible approach to\nmultimodal training and its application to image segmentation tasks.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "M.A. Chia, Y. Zhou, and P.A. Keane"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401024",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Foundation models are a powerful tool in ophthalmology for building generalizable systems that can be efficiently applied to a range of ocular and systemic health tasks. A new foundation model for ophthalmic images demonstrates important progress, particularly through its flexible approach to multimodal training and its application to image segmentation tasks.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2400961",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2400961",
      "title": "Cognitive Bias in Large Language Models: Implications for Research and Practice",
      "summary": "This commentary discusses the presence of cognitive biases in the outputs of large language models and the implications for clinical decision support and human\u2013computer interaction.",
      "content_text": "## Abstract\n\nThe use of large language models (LLMs) such as ChatGPT in clinical settings\nis growing, but concerns about their susceptibility to cognitive biases\npersist. Wang and Redelmeier\u2019s study reveals that LLMs are prone to biases,\nraising important questions about their role in medical decision-making. To\nprevent errors in decision-making with LLMs, it is recommended that clinicians\naim to critically engage with LLMs (e.g. refuting their hypotheses rather than\nlooking for confirmation) researchers should focus on identifying and\nevaluating collaborative strategies between AI and human decision-making.\nFurthermore, research on context-specific implementation is important. We need\nto ensure that AI complements, rather than replicates, human cognitive\nprocesses. (Funded by the Netherlands Organisation for Health Research and\nDevelopment.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "L. Zwaan"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2400961",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">The use of large language models (LLMs) such as ChatGPT in clinical settings is growing, but concerns about their susceptibility to cognitive biases persist. Wang and Redelmeier\u2019s study reveals that LLMs are prone to biases, raising important questions about their role in medical decision-making. To prevent errors in decision-making with LLMs, it is recommended that clinicians aim to critically engage with LLMs (e.g. refuting their hypotheses rather than looking for confirmation) researchers should focus on identifying and evaluating collaborative strategies between AI and human decision-making. Furthermore, research on context-specific implementation is important. We need to ensure that AI complements, rather than replicates, human cognitive processes. (Funded by the Netherlands Organisation for Health Research and Development.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400639",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400639",
      "title": "Cognitive Biases and Artificial Intelligence",
      "summary": "Artificial intelligence models sometimes propagate racial and gender bias. This study shows that human-like cognitive biases are prevalent in generative artificial intelligence and sometimes much more so than among practicing clinicians.",
      "content_text": "## Abstract\n\nGenerative artificial intelligence (AI) models are increasingly utilized for\nmedical applications. We tested whether such models are prone to human-like\ncognitive biases when offering medical recommendations. We explored the\nperformance of OpenAI generative pretrained transformer (GPT)-4 and Google\nGemini-1.0-Pro with clinical cases that involved 10 cognitive biases and\nsystem prompts that created synthetic clinician respondents. Medical\nrecommendations from generative AI were compared with strict axioms of\nrationality and prior results from clinicians. We found that significant\ndiscrepancies were apparent for most biases. For example, surgery was\nrecommended more frequently for lung cancer when framed in survival rather\nthan mortality statistics (framing effect: 75% vs. 12%; P<0.001). Similarly,\npulmonary embolism was more likely to be listed in the differential diagnoses\nif the opening sentence mentioned hemoptysis rather than chronic obstructive\npulmonary disease (primacy effect: 100% vs. 26%; P<0.001). In addition, the\nsame emergency department treatment was more likely to be rated as\ninappropriate if the patient subsequently died rather than recovered\n(hindsight bias: 85% vs. 0%; P<0.001). One exception was base-rate neglect\nthat showed no bias when interpreting a positive viral screening test\n(correction for false positives: 94% vs. 93%; P=0.431). The extent of these\nbiases varied minimally with the characteristics of synthetic respondents, was\ngenerally larger than observed in prior research with practicing clinicians,\nand differed between generative AI models. We suggest that generative AI\nmodels display human-like cognitive biases and that the magnitude of bias can\nbe larger than observed in practicing clinicians.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Wang and D.A. Redelmeier"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400639",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/571fe939-b7d3-458d-a92f-1d3e1a00b195/aics2400639_f3.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Generative artificial intelligence (AI) models are increasingly utilized for medical applications. We tested whether such models are prone to human-like cognitive biases when offering medical recommendations. We explored the performance of OpenAI generative pretrained transformer (GPT)-4 and Google Gemini-1.0-Pro with clinical cases that involved 10 cognitive biases and system prompts that created synthetic clinician respondents. Medical recommendations from generative AI were compared with strict axioms of rationality and prior results from clinicians. We found that significant discrepancies were apparent for most biases. For example, surgery was recommended more frequently for lung cancer when framed in survival rather than mortality statistics (framing effect: 75% vs. 12%; P&lt;0.001). Similarly, pulmonary embolism was more likely to be listed in the differential diagnoses if the opening sentence mentioned hemoptysis rather than chronic obstructive pulmonary disease (primacy effect: 100% vs. 26%; P&lt;0.001). In addition, the same emergency department treatment was more likely to be rated as inappropriate if the patient subsequently died rather than recovered (hindsight bias: 85% vs. 0%; P&lt;0.001). One exception was base-rate neglect that showed no bias when interpreting a positive viral screening test (correction for false positives: 94% vs. 93%; P=0.431). The extent of these biases varied minimally with the characteristics of synthetic respondents, was generally larger than observed in prior research with practicing clinicians, and differed between generative AI models. We suggest that generative AI models display human-like cognitive biases and that the magnitude of bias can be larger than observed in practicing clinicians.</div>"
    }
  ]
}