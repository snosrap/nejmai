{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2500163",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2500163",
      "title": "More Fragmented, More Complex: State Regulation of AI in Health Care",
      "summary": "Increased state involvement \u2014 especially in the absence of strong federal legislation \u2014 could either hinder AI development through regulatory fragmentation or encourage self-regulation to meet high standards set by states and international bodies.",
      "content_text": "## Abstract\n\nSeveral U.S. states have enacted or are considering legislation to regulate\nthe use of artificial intelligence (AI) in health care. These bills focus on\nseveral key areas: establishing commissions and/or agencies to study and\nmanage health AI; ensuring data privacy and security; addressing bias and\ndiscrimination in AI; promoting transparency in AI usage; overseeing claims\nprocessing by insurance companies; and training the health care workforce\nregarding AI. State activities concerning health AI are likely to become more\nwidespread and influential if, as seems likely, the federal government reduces\nits involvement in ensuring the safety, efficacy, and reliability of health AI\napplications. A surge in diverse state regulations could impede the AI\nindustry\u2019s efforts to develop and market promising health AI applications.\nAlternatively, however, producers of AI tools may choose to self-regulate to\nmeet the stringent standards set by states and international authorities, such\nas the European Union, enabling them to confidently sell their products across\nall jurisdictions. (Funded by the Commonwealth Fund of New York City.)\n\n",
      "date_published": "2025-05-05T00:00:00+00:00",
      "authors": [
        {
          "name": "D. Blumenthal and A. Marellapudi"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2500163",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Several U.S. states have enacted or are considering legislation to regulate the use of artificial intelligence (AI) in health care. These bills focus on several key areas: establishing commissions and/or agencies to study and manage health AI; ensuring data privacy and security; addressing bias and discrimination in AI; promoting transparency in AI usage; overseeing claims processing by insurance companies; and training the health care workforce regarding AI. State activities concerning health AI are likely to become more widespread and influential if, as seems likely, the federal government reduces its involvement in ensuring the safety, efficacy, and reliability of health AI applications. A surge in diverse state regulations could impede the AI industry\u2019s efforts to develop and market promising health AI applications. Alternatively, however, producers of AI tools may choose to self-regulate to meet the stringent standards set by states and international authorities, such as the European Union, enabling them to confidently sell their products across all jurisdictions. (Funded by the Commonwealth Fund of New York City.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2500005",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2500005",
      "title": "Harnessing Moravec\u2019s Paradox in Health Care: A New Era of Collaborative Intelligence",
      "summary": "This perspective examines the ongoing challenges in integrating artificial intelligence into health care, emphasizing the persistent issues of algorithmic bias, privacy concerns, and the necessity for rigorous validation. It highlights the balance between automation and human oversight, particularly in ethical decision-making and informed consent processes.",
      "content_text": "## Abstract\n\nArtificial intelligence excels at complex data analytics, yet struggles with\nnuanced, sensorimotor tasks that humans perform almost effortlessly \u2014 a\ndichotomy encapsulated by Moravec\u2019s paradox. By strategically harnessing these\ncomplementary strengths, health care can usher in an era of collaborative\nintelligence, optimizing data-intensive workflows, such as clinical trial\nenrollment, and creating more patient-centric models of care.\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "A. Loaiza-Bonilla and S. Penberthy"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2500005",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/18aa6402-273f-4c7d-9bf0-df8ba7ca2930/aip2500005_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence excels at complex data analytics, yet struggles with nuanced, sensorimotor tasks that humans perform almost effortlessly \u2014 a dichotomy encapsulated by Moravec\u2019s paradox. By strategically harnessing these complementary strengths, health care can usher in an era of collaborative intelligence, optimizing data-intensive workflows, such as clinical trial enrollment, and creating more patient-centric models of care.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401122",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401122",
      "title": "Why Is Primary Care Different? Considerations for Machine Learning Development with Electronic Medical Record Data",
      "summary": "This perspective explains how the unique nature of primary care translates to important differences in machine learning model development and adoption for clinical use. Four key considerations are presented to address the use of ML in primary care settings.",
      "content_text": "## Abstract\n\nPrimary care is the foundation of the health care system and a core driver of\npopulation health. Machine learning (ML) has the potential to enhance primary\ncare by improving efficiency, quality, and accessibility, but ML development\nand implementation in primary care remains limited compared with other medical\nspecialties. There are rich opportunities for collaboration and activity in\nthis area. Effective integration requires methods tailored to the unique\ncharacteristics of primary care, including its core functions \u2014 first contact,\ncomprehensiveness, coordination, and continuity \u2014 as well as the distinct\nnature of electronic medical record data. Key considerations include using\nrepresentative primary care data, constructing cohorts that reflect whole-\nperson care, aligning target outcomes with primary care objectives, and\ndeveloping validation strategies suited to decentralized clinical settings.\nAddressing these challenges requires interdisciplinary collaboration and\nincreased investment in primary care-specific ML research. Optimizing ML for\nprimary care could enhance clinical decision-making, improve patient outcomes,\nand drive meaningful innovation in health care.\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "J.K. Kueper and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401122",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Primary care is the foundation of the health care system and a core driver of population health. Machine learning (ML) has the potential to enhance primary care by improving efficiency, quality, and accessibility, but ML development and implementation in primary care remains limited compared with other medical specialties. There are rich opportunities for collaboration and activity in this area. Effective integration requires methods tailored to the unique characteristics of primary care, including its core functions \u2014 first contact, comprehensiveness, coordination, and continuity \u2014 as well as the distinct nature of electronic medical record data. Key considerations include using representative primary care data, constructing cohorts that reflect whole-person care, aligning target outcomes with primary care objectives, and developing validation strategies suited to decentralized clinical settings. Addressing these challenges requires interdisciplinary collaboration and increased investment in primary care-specific ML research. Optimizing ML for primary care could enhance clinical decision-making, improve patient outcomes, and drive meaningful innovation in health care.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401116",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401116",
      "title": "Challenges in the Postmarket Surveillance of Clinical Prediction Models",
      "summary": "This paper discusses the challenges of postdeployment assessment of predictive artificial intelligence (AI) models in health care, particularly how confounding medical interventions can distort performance evaluation. It explores current limitations in monitoring strategies and suggests advanced causal modeling as a potential solution to ensure that AI models remain effective in dynamic clinical settings.",
      "content_text": "## Abstract\n\nPredictive artificial intelligence (AI) models enhance clinical workflows with\napplications such as prognostication and decision support, yet suffer from\npostdeployment performance challenges due to dataset shifts. Regulatory\nguidelines emphasize the need for continuous monitoring, but actionable\nstrategies are lacking. A significant issue is postdeployment assessment of\npredictive AI models due to confounding medical interventions where effective\ninterventions modify outcomes, introducing bias into performance assessment.\nThis can falsely suggest model decay, leading to unwarranted updates or\ndecommissioning, harming clinical outcomes.\n\nProposed solutions include withholding model outputs, monitoring outcomes as\nsurrogates, or including clinician interventions in models, each with ethical\nor practical limitations. The lack of effective solutions for this problem can\nlead to an abundance of models that cannot be later evaluated, tuned, or\nwithdrawn if they become ineffective, leading to patient harm. Advanced causal\nmodeling to assess counterfactual outcomes may offer a reliable validation\nmethod. Until effective methods for postdeployment monitoring of predictive\nmodels are developed and validated, decisions on model updates should consider\nthe causal pathways and be evidence based, ensuring the sustained utility of\nAI models in dynamic clinical environments.\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "S. Ansari and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401116",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/58aef956-e171-4b04-8a14-e4bbc32e8040/aip2401116_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Predictive artificial intelligence (AI) models enhance clinical workflows with applications such as prognostication and decision support, yet suffer from postdeployment performance challenges due to dataset shifts. Regulatory guidelines emphasize the need for continuous monitoring, but actionable strategies are lacking. A significant issue is postdeployment assessment of predictive AI models due to confounding medical interventions where effective interventions modify outcomes, introducing bias into performance assessment. This can falsely suggest model decay, leading to unwarranted updates or decommissioning, harming clinical outcomes.</div><div role=\"paragraph\">Proposed solutions include withholding model outputs, monitoring outcomes as surrogates, or including clinician interventions in models, each with ethical or practical limitations. The lack of effective solutions for this problem can lead to an abundance of models that cannot be later evaluated, tuned, or withdrawn if they become ineffective, leading to patient harm. Advanced causal modeling to assess counterfactual outcomes may offer a reliable validation method. Until effective methods for postdeployment monitoring of predictive models are developed and validated, decisions on model updates should consider the causal pathways and be evidence based, ensuring the sustained utility of AI models in dynamic clinical environments.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401059",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401059",
      "title": "Regulation of AI: Learnings from Medical Education",
      "summary": "This perspective draws parallels between artificial intelligence (AI) regulation and competency-based medical education (CBME), an existing framework that addresses similar complexities in training human clinicians. The authors propose a CBME framework as a life-cycle approach to regulating AI in health care that offers a dynamic, continuous, and outcomes-based regulatory model to address the unique challenges posed by AI technologies.",
      "content_text": "## Abstract\n\nThe rapid integration of artificial intelligence (AI) into health care\npresents regulatory challenges due to the dynamic, opaque, and adaptive nature\nof AI \u2014 particularly with generative AI models. The U.S. Food and Drug\nAdministration has proposed a life-cycle approach extending beyond\nconventional frameworks, recognizing that traditional medical device\nregulations are ill-suited for such technologies. This perspective draws\nparallels between AI regulation and competency-based medical education (CBME),\nan existing framework that addresses similar complexities in training human\nclinicians who are also dynamic general-purpose problem-solvers with opaque\ncognitive processes. We propose adopting an AI\u2013CBME life-cycle framework for\nmedical AI regulation, leveraging the five core CBME components: defining\ncompetencies, sequenced progression, tailored learning experiences,\ncompetency-focused instruction, and programmatic assessment. By applying these\nprinciples, we can implement continuous outcomes-based assessments of AI\nsystems within their operational environments. This approach embeds real-time\nvalidation and safeguards for patient safety, building accountability and\ntrust, while maximizing the potential of AI technologies to enhance health\ncare. Engaging medical educators in this process offers an immediate and\npractical pathway to develop a robust regulatory framework aligned with\nexisting educational methodologies.\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "K.N. Vokinger, D.R. Soled, and R.-E.E. Abdulnour"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401059",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/bede5653-c685-48e5-8e16-3323bb710bbb/aip2401059_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">The rapid integration of artificial intelligence (AI) into health care presents regulatory challenges due to the dynamic, opaque, and adaptive nature of AI \u2014 particularly with generative AI models. The U.S. Food and Drug Administration has proposed a life-cycle approach extending beyond conventional frameworks, recognizing that traditional medical device regulations are ill-suited for such technologies. This perspective draws parallels between AI regulation and competency-based medical education (CBME), an existing framework that addresses similar complexities in training human clinicians who are also dynamic general-purpose problem-solvers with opaque cognitive processes. We propose adopting an AI\u2013CBME life-cycle framework for medical AI regulation, leveraging the five core CBME components: defining competencies, sequenced progression, tailored learning experiences, competency-focused instruction, and programmatic assessment. By applying these principles, we can implement continuous outcomes-based assessments of AI systems within their operational environments. This approach embeds real-time validation and safeguards for patient safety, building accountability and trust, while maximizing the potential of AI technologies to enhance health care. Engaging medical educators in this process offers an immediate and practical pathway to develop a robust regulatory framework aligned with existing educational methodologies.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400703",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400703",
      "title": "Longitudinal Risk Prediction for Pediatric Glioma with Temporal Deep Learning",
      "summary": "This study describes the development and application of a self-supervised deep-learning approach called temporal learning for analyzing longitudinal medical imaging. It demonstrates that temporal learning improves longitudinal, imaging-based recurrence prediction in pediatric gliomas, and that performance increases incrementally with the number of historical scans available to the algorithm.",
      "content_text": "## Abstract\n\n### Background\n\nPediatric glioma recurrence can cause morbidity and mortality; however,\nrecurrence patterns and severity are heterogeneous and challenging to predict\nwith established clinical and genomic markers. As a result, almost all\nchildren undergo frequent, long-term, magnetic resonance imaging (MRI) brain\nsurveillance regardless of individual recurrence risk. Longitudinal deep-\nlearning analysis of serial MRI scans may be an effective approach for\nimproving individualized recurrence prediction in gliomas and other cancers,\nbut, thus far, progress has been limited by data availability and current\nmachine-learning approaches.\n\n### Methods\n\nWe developed a self-supervised temporal deep-learning approach tailored for\nlongitudinal medical imaging analysis, wherein a multistep model encodes\npatients\u2019 serial MRI scans and is trained to classify the correct\nchronological order as a pretext task. The pretrained model is then fine-tuned\nto predict the primary end point of interest \u2014 in this case, 1-year recurrence\nprediction for pediatric gliomas from the point of last scan \u2014 by leveraging a\npatient\u2019s historical postoperative surveillance scans. We apply the model\nacross 3994 scans from 715 patients followed at three separate institutions in\nthe setting of pediatric low- and high-grade gliomas.\n\n### Results\n\nLongitudinal imaging analysis with temporal learning improved recurrence\nprediction performance (F1 score) by up to 58.5% (range, 6.6 to 58.5%)\ncompared with traditional approaches across datasets, with performance\nimprovements in both low- and high-grade gliomas and area under the receiver\noperating characteristic curve of (range, 75 to 89%) across all datasets.\nRecurrence prediction performance increased incrementally with the number of\nhistorical scans available per patient, reaching plateaus between three and\nsix scans, depending on the dataset.\n\n### Conclusions\n\nTemporal deep learning enables high-performing longitudinal medical imaging\nanalysis and point-of-care decision support for pediatric brain tumors.\nTemporal learning may be broadly adaptable to track and predict risk in\npatients with other cancers and chronic diseases undergoing surveillance\nimaging. (Funded in part by the National Institutes of Health/National Cancer\nInstitute (U54 CA274516 and P50 CA165962), and Botha-Chan Low Grade Glioma\nConsortium.)\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "D. Tak and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400703",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/79a26797-4e7a-4570-8a67-8e26d478314c/aioa2400703_f4.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Pediatric glioma recurrence can cause morbidity and mortality; however, recurrence patterns and severity are heterogeneous and challenging to predict with established clinical and genomic markers. As a result, almost all children undergo frequent, long-term, magnetic resonance imaging (MRI) brain surveillance regardless of individual recurrence risk. Longitudinal deep-learning analysis of serial MRI scans may be an effective approach for improving individualized recurrence prediction in gliomas and other cancers, but, thus far, progress has been limited by data availability and current machine-learning approaches.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We developed a self-supervised temporal deep-learning approach tailored for longitudinal medical imaging analysis, wherein a multistep model encodes patients\u2019 serial MRI scans and is trained to classify the correct chronological order as a pretext task. The pretrained model is then fine-tuned to predict the primary end point of interest \u2014 in this case, 1-year recurrence prediction for pediatric gliomas from the point of last scan \u2014 by leveraging a patient\u2019s historical postoperative surveillance scans. We apply the model across 3994 scans from 715 patients followed at three separate institutions in the setting of pediatric low- and high-grade gliomas.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">Longitudinal imaging analysis with temporal learning improved recurrence prediction performance (F1 score) by up to 58.5% (range, 6.6 to 58.5%) compared with traditional approaches across datasets, with performance improvements in both low- and high-grade gliomas and area under the receiver operating characteristic curve of (range, 75 to 89%) across all datasets. Recurrence prediction performance increased incrementally with the number of historical scans available per patient, reaching plateaus between three and six scans, depending on the dataset.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Temporal deep learning enables high-performing longitudinal medical imaging analysis and point-of-care decision support for pediatric brain tumors. Temporal learning may be broadly adaptable to track and predict risk in patients with other cancers and chronic diseases undergoing surveillance imaging. (Funded in part by the National Institutes of Health/National Cancer Institute (U54 CA274516 and P50 CA165962), and Botha-Chan Low Grade Glioma Consortium.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400421",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400421",
      "title": "Calibration of ECG-Based Deep-Learning Algorithm Scores for Patients Flagged as High Risk for Hypertrophic Cardiomyopathy",
      "summary": "This study finds that an out-of-the-box risk classification by a U.S. Food and Drug Administration\u2013cleared artificial intelligence\u2013enabled algorithm for identifying hypertrophic cardiomyopathy from electrocardiograms can have a low predictive value when used for diagnosis in a screened population. Calibrated model scores approximate the probability of disease among those flagged and can both improve the interpretability of model outputs and inform decision-making about subsequent follow-up.",
      "content_text": "## Abstract\n\n### Background\n\nDespite optimism for the incorporation of deep-learning (DL) algorithms into\nclinical practice, there is a lack of prospective data on how U.S. Food and\nDrug Administration (FDA)\u2013cleared algorithms function out of the box and\nclinicians\u2019 interpretations of model predictions. This study illustrates how\nmodel calibration, which rescales model outputs to represent probabilities of\ndisease among those flagged, enables improved interpretability of the scores\nby end users.\n\n### Methods\n\nUsing data from patients flagged as being at high risk for hypertrophic\ncardiomyopathy (HCM), patients 18 years of age and over were screened from a\nsingle health system using a DL electrocardiogram (ECG)\u2013based algorithm for\nHCM detection. Cardiologists manually reviewed medical records and imaging of\npatients flagged as being suggestive of HCM to classify individuals as HCM\npositive (HCM+) or HCM negative (HCM\u2212). Logistic regression was used for\ncalibration to transform model outputs to probabilities in the flagged sample.\nWe compare calibrated probabilities with the estimated probability of disease\nin the flagged sample and estimated positive predictive value (PPV) at\ndifferent thresholds of the calibrated score.\n\n### Results\n\nAmong 1522 patients were flagged as being high risk, 166 were classified as\nHCM+ and 1356 as HCM\u2212. Being flagged as being at high risk on the initial\nscreening has a PPV of 10.9% for HCM patients. Recalibrated model scores in\nthe flagged sample closely approximated the probability of HCM among those who\nwere initially flagged, lending improved interpretability. Setting a cutoff\nvalue that corresponds to the top 5 and 10 ranked values yields PPVs of 80%\nand 70%, respectively. As the ranks of the model outputs and the recalibrated\nscores are equivalent, PPVs derived from this approach do not depend on which\nscore is used, but the calibrated scores have context-relevant\ninterpretability as the probability of HCM.\n\n### Conclusions\n\nUsing the raw scores from an out-of-the-box\u2013FDA\u2013cleared algorithm for the\nidentification of HCM from ECGs may have low PPV for identifying patients who\ntruly have HCM. Calibration of scores for those flagged can improve the\ninterpretability of the scores in terms of disease probability. (Funded by\nViz.ai.)\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Lampert and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400421",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIoa2400421_Lampert-1745497839993.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Despite optimism for the incorporation of deep-learning (DL) algorithms into clinical practice, there is a lack of prospective data on how U.S. Food and Drug Administration (FDA)\u2013cleared algorithms function out of the box and clinicians\u2019 interpretations of model predictions. This study illustrates how model calibration, which rescales model outputs to represent probabilities of disease among those flagged, enables improved interpretability of the scores by end users.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">Using data from patients flagged as being at high risk for hypertrophic cardiomyopathy (HCM), patients 18 years of age and over were screened from a single health system using a DL electrocardiogram (ECG)\u2013based algorithm for HCM detection. Cardiologists manually reviewed medical records and imaging of patients flagged as being suggestive of HCM to classify individuals as HCM positive (HCM+) or HCM negative (HCM\u2212). Logistic regression was used for calibration to transform model outputs to probabilities in the flagged sample. We compare calibrated probabilities with the estimated probability of disease in the flagged sample and estimated positive predictive value (PPV) at different thresholds of the calibrated score.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">Among 1522 patients were flagged as being high risk, 166 were classified as HCM+ and 1356 as HCM\u2212. Being flagged as being at high risk on the initial screening has a PPV of 10.9% for HCM patients. Recalibrated model scores in the flagged sample closely approximated the probability of HCM among those who were initially flagged, lending improved interpretability. Setting a cutoff value that corresponds to the top 5 and 10 ranked values yields PPVs of 80% and 70%, respectively. As the ranks of the model outputs and the recalibrated scores are equivalent, PPVs derived from this approach do not depend on which score is used, but the calibrated scores have context-relevant interpretability as the probability of HCM.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Using the raw scores from an out-of-the-box\u2013FDA\u2013cleared algorithm for the identification of HCM from ECGs may have low PPV for identifying patients who truly have HCM. Calibration of scores for those flagged can improve the interpretability of the scores in terms of disease probability. (Funded by Viz.ai.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2500297",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2500297",
      "title": "Model Calibration, Interpretability, and Decision-Making with AI-Based Risk Scores",
      "summary": "Unpacking an important application of model calibration for an out-of-the-box artificial intelligence algorithm used to flag patients who are at high risk for hypertrophic cardiomyopathy, this article explains the use of calibrated scores for patient-level decision-making.",
      "content_text": "Unpacking an important application of model calibration for an out-of-the-box artificial intelligence algorithm used to flag patients who are at high risk for hypertrophic cardiomyopathy, this article explains the use of calibrated scores for patient-level decision-making.",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "J.W. Hogan and V.L. Murthy"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2500297",
          "mime_type": "application/pdf"
        }
      ]
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2401008",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2401008",
      "title": "Clinical Deployment of Real-Time Left Ventricular Ejection Fraction Estimation from Coronary Angiography",
      "summary": "This study evaluates the CathEF deep-learning algorithm, which estimates left ventricular ejection fraction directly from routine left coronary angiograms in acute coronary syndrome patients, eliminating the need for additional contrast or catheterization. The study found that CathEF achieved diagnostic performance comparable to ventriculography, although its accuracy was lower in patients with ST-elevation myocardial infarction or right coronary artery involvement.",
      "content_text": "## Abstract\n\nAssessment of left ventricular ejection fraction (LVEF) is crucial for\nprognosis and management in acute coronary syndrome (ACS). While\nechocardiography is the standard assessment method, assessment via left\nventriculography during angiography is often done and carries procedural\nrisks. The CathEF algorithm uses deep learning to estimate LVEF directly from\nroutine left coronary angiograms without additional contrast injection or\nprocedural risks. The CathEF study is a prospective, multicenter,\nobservational study of 207 patients from two Canadian institutions undergoing\ncoronary angiography for ACS. The diagnostic performance of CathEF was\nevaluated using the area under the receiver operating characteristic curve\n(AUROC), sensitivity, and specificity; metrics were compared with\nechocardiography and ventriculography. The AUROC for the CathEF algorithm was\n0.84 (95% confidence interval [CI], 0.79 to 0.89) in identifying LVEF less\nthan or equal to 50%, and 0.90 (95% CI, 0.86 to 0.96) in identifying LVEF less\nthan or equal to 40%. Performance was reduced in patients with ST-elevation\nmyocardial infarction (STEMI) (AUROC, 0.76; 95% CI, 0.66 to 0.88) and those\nwith right coronary artery involvement (AUROC, 0.73; 95% CI, 0.60 to 0.86). In\nsummary, the CathEF algorithm offers a reliable, noninvasive tool for real-\ntime screening for LV systolic dysfunction during coronary angiography in\npatients with ACS. (Funded by the Canadian Institute for Advanced Research\nSolution Network on Integrated AI for Health Imaging and others.)\n\n",
      "date_published": "2025-04-24T00:00:00+00:00",
      "authors": [
        {
          "name": "P. Th\u00e9riault-Lauzier and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2401008",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/f3d616d0-93a2-4322-8e32-fc45c0cf542c/aics2401008_f2.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Assessment of left ventricular ejection fraction (LVEF) is crucial for prognosis and management in acute coronary syndrome (ACS). While echocardiography is the standard assessment method, assessment via left ventriculography during angiography is often done and carries procedural risks. The CathEF algorithm uses deep learning to estimate LVEF directly from routine left coronary angiograms without additional contrast injection or procedural risks. The CathEF study is a prospective, multicenter, observational study of 207 patients from two Canadian institutions undergoing coronary angiography for ACS. The diagnostic performance of CathEF was evaluated using the area under the receiver operating characteristic curve (AUROC), sensitivity, and specificity; metrics were compared with echocardiography and ventriculography. The AUROC for the CathEF algorithm was 0.84 (95% confidence interval [CI], 0.79 to 0.89) in identifying LVEF less than or equal to 50%, and 0.90 (95% CI, 0.86 to 0.96) in identifying LVEF less than or equal to 40%. Performance was reduced in patients with ST-elevation myocardial infarction (STEMI) (AUROC, 0.76; 95% CI, 0.66 to 0.88) and those with right coronary artery involvement (AUROC, 0.73; 95% CI, 0.60 to 0.86). In summary, the CathEF algorithm offers a reliable, noninvasive tool for real-time screening for LV systolic dysfunction during coronary angiography in patients with ACS. (Funded by the Canadian Institute for Advanced Research Solution Network on Integrated AI for Health Imaging and others.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2401164",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2401164",
      "title": "The Use of Artificial Intelligence for Cancer Therapeutic Decision-Making",
      "summary": "This review examines the evolving role of artificial intelligence (AI) in oncology, particularly in radiology and pathology, and treatment selection using large language models. It highlights key challenges \u2014 such as data quality, model validation, and clinical integration \u2014 and proposes strategies to overcome them to support the effective adoption of AI to improve cancer care and patient outcomes.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) has the potential to transform cancer therapeutic\ndecision-making by improving diagnostics and personalizing treatments. This\nreview explores the current and future impact of AI in oncology, focusing on\nits applications in radiology and pathology and the potential of large\nlanguage models in treatment selection. Despite significant advancements, AI\nintegration into clinical workflows is limited due to challenges such as data\nquality, model accuracy, and lack of validation through clinical trials. We\npropose key strategies to address these challenges, including developing\nrobust multicenter datasets, promoting practical AI model development,\nresearching workflow integration and human\u2013AI collaboration, leveraging\nlessons from AI in medical imaging, establishing evaluation guidelines, and\nincentivizing prospective clinical trials. By implementing these strategies,\nAI can significantly enhance cancer care and patient outcomes, paving the way\nfor its effective integration into oncology practice.\n\n",
      "date_published": "2025-04-17T00:00:00+00:00",
      "authors": [
        {
          "name": "O. Elemento, S. Khozin, and C.N. Sternberg"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2401164",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) has the potential to transform cancer therapeutic decision-making by improving diagnostics and personalizing treatments. This review explores the current and future impact of AI in oncology, focusing on its applications in radiology and pathology and the potential of large language models in treatment selection. Despite significant advancements, AI integration into clinical workflows is limited due to challenges such as data quality, model accuracy, and lack of validation through clinical trials. We propose key strategies to address these challenges, including developing robust multicenter datasets, promoting practical AI model development, researching workflow integration and human\u2013AI collaboration, leveraging lessons from AI in medical imaging, establishing evaluation guidelines, and incentivizing prospective clinical trials. By implementing these strategies, AI can significantly enhance cancer care and patient outcomes, paving the way for its effective integration into oncology practice.</div>"
    }
  ]
}