{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2500153",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2500153",
      "title": "Lessons from the Failure of Canada\u2019s Artificial Intelligence and Data Act",
      "summary": "This commentary examines the shortcomings of Canada\u2019s proposed Artificial Intelligence and Data Act, in particular its failure to adequately provide regulatory guidance for health care AI. It argues for a more targeted, sector-specific approach to ensure patient safety, transparency, and responsible innovation in future AI legislation.",
      "content_text": "## Abstract\n\nCanada\u2019s initial attempt at AI governance, the Artificial Intelligence and\nData Act (AIDA), was introduced within Bill C-27, but was ultimately\nterminated with the prorogation of Parliament in January 2025. AIDA sought to\nestablish a risk-based regulatory framework; however, it was criticized for\nits lack of specificity, underinclusiveness, and absence of sector-specific\noversight \u2014 issues that are particularly consequential for health care AI\napplications. The broad and generalized nature of AIDA left regulatory gaps\nconcerning safety, bias, transparency, and patient privacy in AI-driven\nmedical decision-making. In this article, we analyze the shortcomings of AIDA\nin health care AI governance and propose key reforms to guide future\nlegislative efforts. A targeted, sector-specific approach is essential to\nensure AI\u2019s safe and effective integration into health care while fostering\nresponsible innovation. The Canadian experience provides important lessons for\nglobal AI regulation, particularly in balancing technological progress with\npatient safety and ethical considerations.\n\n",
      "date_published": "2025-06-18T00:00:00+00:00",
      "authors": [
        {
          "name": "A.H. Ishaque, A. Aidid, and G. Zadeh"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2500153",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Canada\u2019s initial attempt at AI governance, the Artificial Intelligence and Data Act (AIDA), was introduced within Bill C-27, but was ultimately terminated with the prorogation of Parliament in January 2025. AIDA sought to establish a risk-based regulatory framework; however, it was criticized for its lack of specificity, underinclusiveness, and absence of sector-specific oversight \u2014 issues that are particularly consequential for health care AI applications. The broad and generalized nature of AIDA left regulatory gaps concerning safety, bias, transparency, and patient privacy in AI-driven medical decision-making. In this article, we analyze the shortcomings of AIDA in health care AI governance and propose key reforms to guide future legislative efforts. A targeted, sector-specific approach is essential to ensure AI\u2019s safe and effective integration into health care while fostering responsible innovation. The Canadian experience provides important lessons for global AI regulation, particularly in balancing technological progress with patient safety and ethical considerations.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2500045",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2500045",
      "title": "Improving Mental Health Care Access with Technology: Addressing the Screening-to-Referral Bottleneck",
      "summary": "A perspective on applying technology to alleviate the current mental health workforce shortage by focusing on the initial stages of mental health treatment for both primary care providers \u2014 for whom over 70% of visits involve a mental health component \u2014 and mental health professionals.",
      "content_text": "## Abstract\n\nIn the United States, the economic and social costs of untreated mental health\nconditions continue to escalate, with barriers to access due to workforce\nshortages, long wait times, and high referral rates. Technological\ninnovations, including large language models and generative artificial\nintelligence, can most effectively bridge these gaps if they are focused on\noptimizing the early stages of mental health care \u2014 screening, assessment,\ntriage, and treatment planning. By automating and standardizing initial\npatient assessments, these tools could empower primary care providers.\nCritically, technology-enabled solutions stand to bridge the research-to-\npractice gap when it comes to dimensional models of mental health disorders.\nThese technologies could revolutionize real-world intakes by providing general\naccess to leading evidence-based models of mental health case\nconceptualization, currently leveraged by only a small percentage of\nspecialists. (Funded by the NSF Graduate Research Fellowship Program grant\nnumber, DGE-213989.)\n\n",
      "date_published": "2025-06-18T00:00:00+00:00",
      "authors": [
        {
          "name": "A.J. Gorelik and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2500045",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/017e932c-065b-4ac9-9954-b1bb3b1244c7/aip2500045_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In the United States, the economic and social costs of untreated mental health conditions continue to escalate, with barriers to access due to workforce shortages, long wait times, and high referral rates. Technological innovations, including large language models and generative artificial intelligence, can most effectively bridge these gaps if they are focused on optimizing the early stages of mental health care \u2014 screening, assessment, triage, and treatment planning. By automating and standardizing initial patient assessments, these tools could empower primary care providers. Critically, technology-enabled solutions stand to bridge the research-to-practice gap when it comes to dimensional models of mental health disorders. These technologies could revolutionize real-world intakes by providing general access to leading evidence-based models of mental health case conceptualization, currently leveraged by only a small percentage of specialists. (Funded by the NSF Graduate Research Fellowship Program grant number, DGE-213989.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401234",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401234",
      "title": "The Landscape of Medical AI in China",
      "summary": "This perspective provides data on the landscape of medical AI in China, covering research focus areas, key players from academia and industry, and the main drivers of medical AI development, as well as insights into these trends relative to those seen in the United States.",
      "content_text": "## Abstract\n\nChina has achieved significant progress in the medical AI field in recent\nyears. However, the international community may be unaware of the scope and\nextent of medical AI innovations in China. In this perspective, we provide\ndata on the landscape of medical AI in China, covering research focus areas,\nkey players from academia and industry, and the main drivers of medical AI\ndevelopment. We provide insights into these trends relative to those seen in\nthe United States. The number of medical AI\u2013related publications from China\nhas exceeded that from the United States in the past 2 years. Although both\nChina and the United States prioritize technical development in their\npublications, followed by clinical disciplines, the United States had a more\nbalanced focus between technical and clinical research. The Chinese Academy of\nSciences and major universities affiliated with top hospitals are the\npredominant players. Regarding patents and revenue, several large Chinese\nmedical technology companies are emerging as influential players, but they\ncontinue to lag behind large U.S. technology companies. Rapid increases in\ngovernmental funding, access to computational power, and optimization of an AI\nregulatory framework have become the main drivers of China\u2019s advancements in\nmedical AI. Despite this progress, key challenges remain, including fragmented\ndata sources and limited integration of AI into clinical workflow. Addressing\nthese barriers will allow China to contribute to global medical AI\ndevelopment. (Funded by the National Key Research and Development Program of\nChina, grant number 2022YFC2502800, and others.)\n\n",
      "date_published": "2025-06-18T00:00:00+00:00",
      "authors": [
        {
          "name": "Y. Qiu and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401234",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/2316ba77-137c-42b2-a00c-34ddac97af5e/aip2401234_f2.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">China has achieved significant progress in the medical AI field in recent years. However, the international community may be unaware of the scope and extent of medical AI innovations in China. In this perspective, we provide data on the landscape of medical AI in China, covering research focus areas, key players from academia and industry, and the main drivers of medical AI development. We provide insights into these trends relative to those seen in the United States. The number of medical AI\u2013related publications from China has exceeded that from the United States in the past 2 years. Although both China and the United States prioritize technical development in their publications, followed by clinical disciplines, the United States had a more balanced focus between technical and clinical research. The Chinese Academy of Sciences and major universities affiliated with top hospitals are the predominant players. Regarding patents and revenue, several large Chinese medical technology companies are emerging as influential players, but they continue to lag behind large U.S. technology companies. Rapid increases in governmental funding, access to computational power, and optimization of an AI regulatory framework have become the main drivers of China\u2019s advancements in medical AI. Despite this progress, key challenges remain, including fragmented data sources and limited integration of AI into clinical workflow. Addressing these barriers will allow China to contribute to global medical AI development. (Funded by the National Key Research and Development Program of China, grant number 2022YFC2502800, and others.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIdbp2401120",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIdbp2401120",
      "title": "PadChest-GR: A Bilingual Chest X-Ray Dataset for Grounded Radiology Report Generation",
      "summary": "PadChest Grounded Reporting is a novel dataset derived from PadChest and designed to train and evaluate grounded report generation models from chest x-ray images. PadChest-GR includes comprehensive sentence-level bounding-box annotations for all clinically relevant findings in each image, as judged by expert radiologists.",
      "content_text": "## Abstract\n\n### Background\n\nArtificial intelligence (AI)\u2013powered radiology report generation (RRG) aims to\ncreate free-text radiology reports from clinical imaging. Grounded radiology\nreport generation (GRRG) augments RRG by including the localization of\nindividual findings on the image. Currently, to our knowledge, no manually\nannotated chest x-ray (CXR) datasets exist on which to train GRRG models.\n\n### Methods\n\nIn this article, we present a dataset called PadChest-GR (grounded reporting),\nwhich is derived from the CXR dataset, PadChest, and aimed at training GRRG\nmodels to analyze CXR images. First, we selected a subset of studies from\nPadChest that contained images with frontal projection; studies that were\noriginally labeled as suboptimal and those involving pediatric patients were\nexcluded. Then, using Generative Pretrained Transformer 4 in Microsoft Azure\nOpenAI Service, we processed reports to extract sentences with single\nfindings, translate them from Spanish into English, link them to the existing\nPadChest finding and location labels, and classify the finding progression. A\nteam of 14 radiologists discarded studies with poor image quality or issues\nrelating to the report or findings list and then manually annotated the\nfindings using bounding boxes to surround regions of interest in each image.\n\n### Results\n\nWe curated a public bilingual dataset of 4555 CXR studies with grounded\nreports, of which 3099 were abnormal and 1456 were normal. Each report\ncontains complete lists of sentences describing individual present (positive)\nfindings and absent (negative) findings in English and Spanish. In total,\nPadChest-GR contains 7037 positive-finding sentences and 3422 negative-finding\nsentences. Every positive-finding sentence is associated with up to two\nindependent sets of bounding boxes labeled by different readers and has\ncategorical labels for finding type, locations, and progression.\n\n### Conclusions\n\nPadChest-GR is a manually curated dataset designed to train GRRG models to\nunderstand and interpret radiological images and generated text. By including\ndetailed localization and comprehensive annotations of all clinically relevant\nfindings, PadChest-GR provides a valuable resource for developing and\nevaluating GRRG models from CXR images. (Funded by the Microsoft Corporation.)\n\n",
      "date_published": "2025-06-18T00:00:00+00:00",
      "authors": [
        {
          "name": "D.C. de Castro and Others"
        }
      ],
      "tags": [
        "Datasets, Benchmarks, and Protocols"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIdbp2401120",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/955bdbf3-5fc4-45eb-b7ce-822d56228a4f/aidbp2401120_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Artificial intelligence (AI)\u2013powered radiology report generation (RRG) aims to create free-text radiology reports from clinical imaging. Grounded radiology report generation (GRRG) augments RRG by including the localization of individual findings on the image. Currently, to our knowledge, no manually annotated chest x-ray (CXR) datasets exist on which to train GRRG models.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">In this article, we present a dataset called PadChest-GR (grounded reporting), which is derived from the CXR dataset, PadChest, and aimed at training GRRG models to analyze CXR images. First, we selected a subset of studies from PadChest that contained images with frontal projection; studies that were originally labeled as suboptimal and those involving pediatric patients were excluded. Then, using Generative Pretrained Transformer 4 in Microsoft Azure OpenAI Service, we processed reports to extract sentences with single findings, translate them from Spanish into English, link them to the existing PadChest finding and location labels, and classify the finding progression. A team of 14 radiologists discarded studies with poor image quality or issues relating to the report or findings list and then manually annotated the findings using bounding boxes to surround regions of interest in each image.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">We curated a public bilingual dataset of 4555 CXR studies with grounded reports, of which 3099 were abnormal and 1456 were normal. Each report contains complete lists of sentences describing individual present (positive) findings and absent (negative) findings in English and Spanish. In total, PadChest-GR contains 7037 positive-finding sentences and 3422 negative-finding sentences. Every positive-finding sentence is associated with up to two independent sets of bounding boxes labeled by different readers and has categorical labels for finding type, locations, and progression.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">PadChest-GR is a manually curated dataset designed to train GRRG models to understand and interpret radiological images and generated text. By including detailed localization and comprehensive annotations of all clinically relevant findings, PadChest-GR provides a valuable resource for developing and evaluating GRRG models from CXR images. (Funded by the Microsoft Corporation.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2500061",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2500061",
      "title": "Development and Commercialization Pathways of AI Medical Devices in the United States: Implications for Safety and Regulatory Oversight",
      "summary": "This review analyzes 950 U.S. Food and Drug Administration\u2013regulated artificial intelligence medical devices, revealing stark contrasts between public and private manufacturers in production scale, transparency, and recall rates. It highlights how commercialization strategies and firm characteristics influence device performance and regulatory outcomes, emphasizing areas needing greater oversight.",
      "content_text": "## Abstract\n\nThe landscape of U.S. Food and Drug Administration\u2013regulated artificial\nintelligence\u2013enabled medical devices (AIMDs) has expanded rapidly, with\nclearances and authorizations increasing, on average, from 1.4 to 146 per year\n(1995 through 2014 vs. 2020 through 2024, respectively). In this review of 950\nAIMDs and their associated commercialization trends, private firms comprise\n68.9% of the manufacturers, yet public companies produce more devices per firm\non average (5.1 vs. 2.0), reflecting their larger scale and better resources.\nDeep learning now powers half of all new AIMDs, underscoring its growing role\nin medical AI. Transparency has improved, as the proportion of devices without\nexplicit AI descriptions declined from 62.4% (2015 through 2019) to 8.9% (2020\nthrough 2024). However, public companies exhibit a 30-fold higher recall rate\nthan private firms, raising concerns about patient safety and regulatory\noversight. This review examines disparities in development models, market\npositioning, and postmarket performance, highlighting the distinct\ncommercialization strategies of public and private manufacturers. These\nfindings contextualize how AI medical device manufacturers\u2019 characteristics\nrelate to transparency and recall rates, highlighting commercialization\naspects requiring regulatory and provider attention. (Funded by Johns Hopkins\nUniversity.)\n\n",
      "date_published": "2025-06-02T00:00:00+00:00",
      "authors": [
        {
          "name": "B. Lee and Others"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2500061",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/c8d2da1c-7087-42f2-8c51-02aef7b98d36/aira2500061_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">The landscape of U.S. Food and Drug Administration\u2013regulated artificial intelligence\u2013enabled medical devices (AIMDs) has expanded rapidly, with clearances and authorizations increasing, on average, from 1.4 to 146 per year (1995 through 2014 vs. 2020 through 2024, respectively). In this review of 950 AIMDs and their associated commercialization trends, private firms comprise 68.9% of the manufacturers, yet public companies produce more devices per firm on average (5.1 vs. 2.0), reflecting their larger scale and better resources. Deep learning now powers half of all new AIMDs, underscoring its growing role in medical AI. Transparency has improved, as the proportion of devices without explicit AI descriptions declined from 62.4% (2015 through 2019) to 8.9% (2020 through 2024). However, public companies exhibit a 30-fold higher recall rate than private firms, raising concerns about patient safety and regulatory oversight. This review examines disparities in development models, market positioning, and postmarket performance, highlighting the distinct commercialization strategies of public and private manufacturers. These findings contextualize how AI medical device manufacturers\u2019 characteristics relate to transparency and recall rates, highlighting commercialization aspects requiring regulatory and provider attention. (Funded by Johns Hopkins University.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401257",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401257",
      "title": "Can a Chatbot Be a Medical Surrogate? The Use of Large Language Models in Medical Ethics Decision-Making",
      "summary": "This perspective explores the capabilities of five large language models (ChatGPT-4o mini, Claude 3.5 Sonnet, Copilot for Microsoft 365, Meta AI Llama 3, and Gemini 1.5 Flash) to respond to ethics scenarios that may emerge when AI is used in health care, and finds that though AI can be used as a tool to support ethical decision-making, it is not currently able to provide autonomous ethics consultation regarding the care of human patients.",
      "content_text": "## Abstract\n\nThe use of AI in health care has raised numerous ethical challenges. Issues\nconcerning data privacy, accountability, bias perpetuation, and the\nidentification of appropriate uses and users have prompted scholars and\nscientists to tackle these challenges. The application of AI to address\npractical ethical issues in clinical settings has not been thoroughly\nexplored. We investigated the capacity of five publicly available large\nlanguage models (Chat Generative Pretrained Transformer 4o mini, Claude 3.5\nSonnet, Copilot for Microsoft 365, Meta AI Llama 3, and Gemini 1.5 Flash) to\nrespond to medical ethics scenarios that may arise when AI is implemented in\nhealth care. We assessed and compared these responses with those of a human\nexpert in medical ethics to analyze the extent to which AI can replicate human\nethical decision-making, outline the distinctions between AI and human\ncognition, and evaluate the effectiveness of AI in medical ethical decision-\nmaking. Our findings indicate that while AI systems may assist in identifying\nconsiderations and guidelines for ethical decision-making, they do not\nconsistently demonstrate the flexibility of thought that humans exhibit when\naddressing novel ethical cases. AI can support ethical decision-making, but it\nis not currently capable of showing autonomous ethical reasoning for\nconsultation regarding patient care.\n\n",
      "date_published": "2025-06-02T00:00:00+00:00",
      "authors": [
        {
          "name": "I. Harshe, K.W. Goodman, and G. Agarwal"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401257",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">The use of AI in health care has raised numerous ethical challenges. Issues concerning data privacy, accountability, bias perpetuation, and the identification of appropriate uses and users have prompted scholars and scientists to tackle these challenges. The application of AI to address practical ethical issues in clinical settings has not been thoroughly explored. We investigated the capacity of five publicly available large language models (Chat Generative Pretrained Transformer 4o mini, Claude 3.5 Sonnet, Copilot for Microsoft 365, Meta AI Llama 3, and Gemini 1.5 Flash) to respond to medical ethics scenarios that may arise when AI is implemented in health care. We assessed and compared these responses with those of a human expert in medical ethics to analyze the extent to which AI can replicate human ethical decision-making, outline the distinctions between AI and human cognition, and evaluate the effectiveness of AI in medical ethical decision-making. Our findings indicate that while AI systems may assist in identifying considerations and guidelines for ethical decision-making, they do not consistently demonstrate the flexibility of thought that humans exhibit when addressing novel ethical cases. AI can support ethical decision-making, but it is not currently capable of showing autonomous ethical reasoning for consultation regarding patient care.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2401260",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2401260",
      "title": "Right Care, Right Place, First Time: How AI Is Improving National Virtual Front Doors",
      "summary": "This policy commentary explores the transformative role of artificial intelligence (AI) in virtual front doors, which serve as the initial consumer interface in health systems. It highlights how AI is enabling more personalized, equitable, and efficient care triage in Australia, advancing the goal of delivering the right care, in the right place, the first time.",
      "content_text": "## Abstract\n\nLarge-scale virtual front doors (VFDs) serve as the initial point of consumer\ncontact with a health system. Intended to improve consumer experience, health\nequity, evidence-based care, and utilization of available system capacity,\nVFDs are offered through digital and telephone channels, aiming to provide\nfirst-time resolution at the appropriate level of care.\n\nPreviously, VFDs faced challenges in personalizing triage for particular\nconsumer cohorts and had a tendency to contribute to overtriage. The use of\nartificial intelligence (AI)\u2013enabled VFDs across Australia has been shown to\novercome these limitations. However, ensuring transparency, accountability,\nand safety in AI-enabled VFDs requires clear communication about how AI is\nused, who operates the system, and the values driving its design, alongside\nrobust postimplementation monitoring for bias, shared oversight between\ngovernment and industry, and thoughtful change management to support clinical\nadoption.\n\nAI could become ubiquitous in triaging at scale to achieve the vision of right\ncare, right place, first time. AI-enabled VFDs can solve major health care\nchallenges by improving consumers\u2019 engagement in managing their own care,\nimproving equity of access, reducing unwarranted variability in care pathways,\nand matching demand with available capacity \u2014 all while reducing cost.\n\n",
      "date_published": "2025-05-22T00:00:00+00:00",
      "authors": [
        {
          "name": "B. McMahon and D. McInerney"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2401260",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Large-scale virtual front doors (VFDs) serve as the initial point of consumer contact with a health system. Intended to improve consumer experience, health equity, evidence-based care, and utilization of available system capacity, VFDs are offered through digital and telephone channels, aiming to provide first-time resolution at the appropriate level of care.</div><div role=\"paragraph\">Previously, VFDs faced challenges in personalizing triage for particular consumer cohorts and had a tendency to contribute to overtriage. The use of artificial intelligence (AI)\u2013enabled VFDs across Australia has been shown to overcome these limitations. However, ensuring transparency, accountability, and safety in AI-enabled VFDs requires clear communication about how AI is used, who operates the system, and the values driving its design, alongside robust postimplementation monitoring for bias, shared oversight between government and industry, and thoughtful change management to support clinical adoption.</div><div role=\"paragraph\">AI could become ubiquitous in triaging at scale to achieve the vision of right care, right place, first time. AI-enabled VFDs can solve major health care challenges by improving consumers\u2019 engagement in managing their own care, improving equity of access, reducing unwarranted variability in care pathways, and matching demand with available capacity \u2014 all while reducing cost.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIdbp2400732",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIdbp2400732",
      "title": "Multimodal Image Dataset for AI-Based Skin Cancer (MIDAS) Benchmarking",
      "summary": "This article introduces the Melanoma Research Alliance Multimodal Image Dataset for Artificial Intelligence\u2013Based Skin Cancer (MIDAS), the largest publicly available dataset of biopsy-confirmed skin lesions with paired clinical and dermoscopic images. Using MIDAS, the authors evaluate the diagnostic performance of leading AI models and compare them with dermatologists\u2019 diagnoses, highlighting the need for diverse, real-world data to improve model generalizability and clinical relevance of AI use in dermatologic diagnostics.",
      "content_text": "## Abstract\n\n### Background\n\nWith an estimated 3 billion people lacking dermatologic care globally,\nartificial intelligence (AI) offers potential improvements in access. However,\nhigh-quality, diverse datasets are crucial for developing and testing these\nalgorithms, including both unimodal and multimodal approaches. Most\ndermatology AI models are built on proprietary, siloed data, often from a\nsingle site with a single image type (i.e., clinical or dermoscopic). To\naddress this, we introduce the Melanoma Research Alliance Multimodal Image\nDataset for AI-based Skin Cancer (MIDAS), the largest publicly available,\nprospectively recruited dataset of biopsy-proven skin lesions with paired\ndermoscopic and clinical images.\n\n### Methods\n\nWe evaluated model performance on real-world MIDAS cases using four previously\npublished state-of-the-art (SOTA) models and compared model and clinician\ndiagnostic performance. We also assessed algorithm performance using clinical\nphotography taken at different distances from the lesion to assess its\ninfluence across diagnostic categories.\n\n### Results\n\nWe prospectively enrolled 796 patients through an institutional review\nboard\u2013approved protocol with informed consent, representing 1290 unique\nlesions and 3830 total images (including dermoscopic and clinical images taken\nat 15-cm and 30-cm distances), to build MIDAS. The images represented a\ndiverse range of lesions seen in general dermatology, including malignant,\nbenign, and inflammatory types. Among these were melanocytic nevi (22.4%),\ninvasive cutaneous melanomas (4.4%), and melanomas in situ (4.5%). We observed\nperformance reduction across all the dermatology SOTA models compared with\ntheir previously published metrics. As a baseline, dermatologists achieved 79%\naccuracy in identifying malignant lesions, and dermoscopic images yielded\nhigher sensitivity than clinical ones.\n\n### Conclusions\n\nImproving our understanding of the strengths and weaknesses of AI diagnostic\nalgorithms is critical as these tools advance toward widespread clinical\ndeployment. While many models report high performance, caution is warranted\ndue to a lack of model transportability across different patient populations.\nMIDAS\u2019s robust, multimodal dataset allows researchers to evaluate models on\nreal-world images, better assessing their generalizability and helping to\nbridge the gap between performance and clinical applicability. (Funded by the\nL\u2019Or\u00e9al Dermatological Beauty Brands-Melanoma Research Alliance Team Science\nAward and others.)\n\n",
      "date_published": "2025-05-20T00:00:00+00:00",
      "authors": [
        {
          "name": "A.S. Chiou and Others"
        }
      ],
      "tags": [
        "Datasets, Benchmarks, and Protocols"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIdbp2400732",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/00261c00-0f4b-47a6-b7fa-348553ba822f/aidbp2400732_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">With an estimated 3 billion people lacking dermatologic care globally, artificial intelligence (AI) offers potential improvements in access. However, high-quality, diverse datasets are crucial for developing and testing these algorithms, including both unimodal and multimodal approaches. Most dermatology AI models are built on proprietary, siloed data, often from a single site with a single image type (i.e., clinical or dermoscopic). To address this, we introduce the Melanoma Research Alliance Multimodal Image Dataset for AI-based Skin Cancer (MIDAS), the largest publicly available, prospectively recruited dataset of biopsy-proven skin lesions with paired dermoscopic and clinical images.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We evaluated model performance on real-world MIDAS cases using four previously published state-of-the-art (SOTA) models and compared model and clinician diagnostic performance. We also assessed algorithm performance using clinical photography taken at different distances from the lesion to assess its influence across diagnostic categories.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">We prospectively enrolled 796 patients through an institutional review board\u2013approved protocol with informed consent, representing 1290 unique lesions and 3830 total images (including dermoscopic and clinical images taken at 15-cm and 30-cm distances), to build MIDAS. The images represented a diverse range of lesions seen in general dermatology, including malignant, benign, and inflammatory types. Among these were melanocytic nevi (22.4%), invasive cutaneous melanomas (4.4%), and melanomas in situ (4.5%). We observed performance reduction across all the dermatology SOTA models compared with their previously published metrics. As a baseline, dermatologists achieved 79% accuracy in identifying malignant lesions, and dermoscopic images yielded higher sensitivity than clinical ones.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Improving our understanding of the strengths and weaknesses of AI diagnostic algorithms is critical as these tools advance toward widespread clinical deployment. While many models report high performance, caution is warranted due to a lack of model transportability across different patient populations. MIDAS\u2019s robust, multimodal dataset allows researchers to evaluate models on real-world images, better assessing their generalizability and helping to bridge the gap between performance and clinical applicability. (Funded by the L\u2019Or\u00e9al Dermatological Beauty Brands-Melanoma Research Alliance Team Science Award and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400937",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400937",
      "title": "AI Opportunistic Coronary Calcium Screening at Veterans Affairs Hospitals",
      "summary": "This article presents AI-CAC, a deep learning algorithm developed to automatically quantify coronary artery calcium (CAC) from nongated, noncontrast chest computed tomography scans across the U.S. Veterans Affairs health care system. Validated against clinical electrocardiogram-gated CAC scoring, the model demonstrated high accuracy and prognostic value, enabling opportunistic cardiovascular risk assessment from routine imaging.",
      "content_text": "## Abstract\n\n### Background\n\nCoronary artery calcium (CAC) is highly predictive of cardiovascular events.\nAlthough millions of chest computed tomography (CT) scans are performed\nannually in the United States, CAC is not routinely quantified from scans done\nfor noncardiac purposes.\n\n### Methods\n\nWe developed a deep learning algorithm, AI-CAC, using 446 expert segmentations\nto automatically quantify CAC on noncontrast, nongated CT scans. Our study\ndiffers from prior works by utilizing imaging data from 98 medical centers\nacross the Veterans Affairs national health care system, capturing extensive\nheterogeneity in imaging protocols, scanners, and patients. AI-CAC performance\non nongated scans was compared against clinical standard electrocardiogram\n(ECG)-gated CAC scoring in 795 patients with paired gated scans within 1 year\nof their nongated scan. In addition, the model was tested on 8052 low-dose CTs\n(LDCTs) to simulate opportunistic CAC screening.\n\n### Results\n\nNongated AI-CAC differentiated zero versus nonzero and less than 100 versus\n100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and 87.3%\n(F1 0.89), respectively. Nongated AI-CAC was predictive of 10-year all-cause\nmortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox hazard ratio 3.49;\nP<0.005), and composite first-time stroke, myocardial infarction, or death\n(CAC 0 vs. >400 group: 33.5% vs. 63.8%, Cox hazard ratio 3.00; P<0.005). In\nthe LDCT dataset, 3091 out of 8052 (38.4%) individuals had AI-CAC scores\n>400\\. Four cardiologists qualitatively reviewed a random sample of the >400\nAI-CAC LDCT patients and verified that 527 of the 531 (99.2%) would benefit\nfrom lipid-lowering therapy.\n\n### Conclusions\n\nThis nongated CT CAC algorithm was developed across a national health care\nsystem and shows strong performance in evaluation against paired gated CT\nscans. The model code and weights are available at <https://github.com/Raffi-\nHagopian/AI-CAC/>. (Funded by the Veterans Affairs health care system.)\n\n",
      "date_published": "2025-05-16T00:00:00+00:00",
      "authors": [
        {
          "name": "R. Hagopian and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400937",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIoa2400937_Hagopian-1747337531947.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Coronary artery calcium (CAC) is highly predictive of cardiovascular events. Although millions of chest computed tomography (CT) scans are performed annually in the United States, CAC is not routinely quantified from scans done for noncardiac purposes.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We developed a deep learning algorithm, AI-CAC, using 446 expert segmentations to automatically quantify CAC on noncontrast, nongated CT scans. Our study differs from prior works by utilizing imaging data from 98 medical centers across the Veterans Affairs national health care system, capturing extensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC performance on nongated scans was compared against clinical standard electrocardiogram (ECG)-gated CAC scoring in 795 patients with paired gated scans within 1 year of their nongated scan. In addition, the model was tested on 8052 low-dose CTs (LDCTs) to simulate opportunistic CAC screening.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">Nongated AI-CAC differentiated zero versus nonzero and less than 100 versus 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and 87.3% (F1 0.89), respectively. Nongated AI-CAC was predictive of 10-year all-cause mortality (CAC 0 vs. &gt;400 group: 25.4% vs. 60.2%, Cox hazard ratio 3.49; P&lt;0.005), and composite first-time stroke, myocardial infarction, or death (CAC 0 vs. &gt;400 group: 33.5% vs. 63.8%, Cox hazard ratio 3.00; P&lt;0.005). In the LDCT dataset, 3091 out of 8052 (38.4%) individuals had AI-CAC scores &gt;400. Four cardiologists qualitatively reviewed a random sample of the &gt;400 AI-CAC LDCT patients and verified that 527 of the 531 (99.2%) would benefit from lipid-lowering therapy.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">This nongated CT CAC algorithm was developed across a national health care system and shows strong performance in evaluation against paired gated CT scans. The model code and weights are available at <a href=\"https://github.com/Raffi-Hagopian/AI-CAC/\" target=\"_blank\">https://github.com/Raffi-Hagopian/AI-CAC/</a>. (Funded by the Veterans Affairs health care system.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2500023",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2500023",
      "title": "Essential Strategies for Leveraging AI in the Global HIV Response",
      "summary": "This perspective explores 10 strategies for how artificial intelligence can be thoughtfully integrated into HIV programs amid tightening global health resources. It offers actionable guidance to ensure these tools reinforce \u2014 rather than disrupt \u2014 community priorities, ethical standards, and sustainable care delivery in high-need settings.",
      "content_text": "## Abstract\n\nAs the global HIV response faces mounting financial pressures and shifting\ngeopolitical priorities, there is growing interest in how artificial\nintelligence (AI) can support progress in high-burden, low-resource settings.\nWhile AI offers the potential to enhance efficiency, optimize resource\nallocation, and support client-centered care, it is not a replacement for\nsustained funding, political will, or well-functioning health systems. The\nstrategies for AI implementation in the global HIV response presented here are\norganized around three core themes: data, governance, and regulation; client-\ncentered and community-led approaches; and stakeholder coordination and\ninvestment in a global AI commons. Key priorities include addressing data\ngaps, promoting ethical and inclusive data practices, ensuring local ownership\nand technological readiness, and building digital literacy. AI must complement\n\u2014 not replace \u2014 essential HIV services and be implemented through a rights-\nbased, community-driven lens to avoid deepening existing disparities. When\ndeployed responsibly, AI can contribute meaningfully to HIV response efforts\nand catalyze broader innovations in global health.\n\n",
      "date_published": "2025-05-09T00:00:00+00:00",
      "authors": [
        {
          "name": "M.J. Reid and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2500023",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">As the global HIV response faces mounting financial pressures and shifting geopolitical priorities, there is growing interest in how artificial intelligence (AI) can support progress in high-burden, low-resource settings. While AI offers the potential to enhance efficiency, optimize resource allocation, and support client-centered care, it is not a replacement for sustained funding, political will, or well-functioning health systems. The strategies for AI implementation in the global HIV response presented here are organized around three core themes: data, governance, and regulation; client-centered and community-led approaches; and stakeholder coordination and investment in a global AI commons. Key priorities include addressing data gaps, promoting ethical and inclusive data practices, ensuring local ownership and technological readiness, and building digital literacy. AI must complement \u2014 not replace \u2014 essential HIV services and be implemented through a rights-based, community-driven lens to avoid deepening existing disparities. When deployed responsibly, AI can contribute meaningfully to HIV response efforts and catalyze broader innovations in global health.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401167",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401167",
      "title": "A Call for Disclosure When Using AI for Patient Communications",
      "summary": "The UC San Diego Health system examines the ethical and practical implications of using generative artificial intelligence (AI) to assist in drafting messages to patients through an approach that prioritizes transparency by disclosing AI involvement in clinical communication and calls for national guidelines to ensure consistent and trustworthy use of AI in patient care.",
      "content_text": "## Abstract\n\nIn the spring of 2023, the UC San Diego Health system began generating draft\nreplies to patient messages with next-generation artificial intelligence (AI)\ntools as an early adopter. The rollout presented our AI governance committee\nwith novel challenges, particularly regarding transparency in the use of AI\nwith our patients, which we chose to make explicit with an automated\ndisclosure. Since our launch, hundreds of other institutions have adopted this\nsame tool without clear guidelines on transparency regarding the disclosure of\nAI use to patients. We share the subsequent experience and call for\nprofessional organizations to help create disclosure guidelines that can be\nused nationally.\n\n",
      "date_published": "2025-05-09T00:00:00+00:00",
      "authors": [
        {
          "name": "M. Millen, M. Tai-Seale, and C.A. Longhurst"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401167",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/2433527d-7ee6-4c6d-8c0f-aa22716392f6/aip2401167_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In the spring of 2023, the UC San Diego Health system began generating draft replies to patient messages with next-generation artificial intelligence (AI) tools as an early adopter. The rollout presented our AI governance committee with novel challenges, particularly regarding transparency in the use of AI with our patients, which we chose to make explicit with an automated disclosure. Since our launch, hundreds of other institutions have adopted this same tool without clear guidelines on transparency regarding the disclosure of AI use to patients. We share the subsequent experience and call for professional organizations to help create disclosure guidelines that can be used nationally.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401106",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401106",
      "title": "Development, Evaluation, and Assessment of Large Language Models (DEAL) Checklist: A Technical Report",
      "summary": "The Development, Evaluation, and Assessment of Large Language Models (DEAL) checklist offers two pathways \u2014 DEAL-A for advanced model development and DEAL-B for applied research \u2014 to ensure comprehensive and consistent reporting of LLM studies, fostering reliable scientific communication and evaluation.",
      "content_text": "## Abstract\n\nLarge language models (LLMs) have advanced artificial intelligence research in\nmedicine, especially in natural language processing tasks. However, the\nnascent evolution of LLM practices presents challenges to the transparency,\nreproducibility, and rigor of research methods. Standardized reporting in\nresearch is critical to ensure reliable scientific communication and\nevaluation. This article introduces the Development, Evaluation, and\nAssessment of Large Language Models (DEAL) checklist, designed to guide\nauthors and reviewers in reporting LLM studies. The checklist comprises two\npathways: DEAL-A, tailored for advanced model development and fine-tuning, and\nDEAL-B, suited to applied research using pretrained models with minimal\nmodifications. Each pathway addresses critical elements such as model\nspecifications, data-handling practices, training procedures, evaluation\nmetrics, and transparency standards. The DEAL checklist provides a\ncomprehensive structure for documenting LLM research with the aim of making it\naccessible and reproducible. This structured approach aims to set a standard\nfor future research, facilitating peer review and encouraging best practices.\nThe DEAL checklist will serve as a valuable tool for enhancing the quality and\nreproducibility of LLM research. By offering clear guidelines on critical\nreporting elements, the DEAL checklist promotes robust and transparent\nscientific reporting, ultimately supporting the reliable advancement of LLM\ntechnologies.\n\n",
      "date_published": "2025-05-09T00:00:00+00:00",
      "authors": [
        {
          "name": "S. Tripathi and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401106",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/9b27612d-d694-43e4-844d-afb927bee5c9/aip2401106_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Large language models (LLMs) have advanced artificial intelligence research in medicine, especially in natural language processing tasks. However, the nascent evolution of LLM practices presents challenges to the transparency, reproducibility, and rigor of research methods. Standardized reporting in research is critical to ensure reliable scientific communication and evaluation. This article introduces the Development, Evaluation, and Assessment of Large Language Models (DEAL) checklist, designed to guide authors and reviewers in reporting LLM studies. The checklist comprises two pathways: DEAL-A, tailored for advanced model development and fine-tuning, and DEAL-B, suited to applied research using pretrained models with minimal modifications. Each pathway addresses critical elements such as model specifications, data-handling practices, training procedures, evaluation metrics, and transparency standards. The DEAL checklist provides a comprehensive structure for documenting LLM research with the aim of making it accessible and reproducible. This structured approach aims to set a standard for future research, facilitating peer review and encouraging best practices. The DEAL checklist will serve as a valuable tool for enhancing the quality and reproducibility of LLM research. By offering clear guidelines on critical reporting elements, the DEAL checklist promotes robust and transparent scientific reporting, ultimately supporting the reliable advancement of LLM technologies.</div>"
    }
  ]
}