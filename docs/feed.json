{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401103",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401103",
      "title": "Cultivating Health Care\u2019s AI Future",
      "summary": "This Perspective article summarizes an NEJM AI Grand Rounds conversation with Dr. Vijay Pande, General Partner at Andreessen Horowitz. We discuss the intersection of artificial intelligence, venture capital, and health care, as well as the potential for AI to democratize access to high-quality health care.",
      "content_text": "## Abstract\n\nDr. Vijay Pande, General Partner at Andreessen Horowitz (a16z), discusses the\nintersection of artificial intelligence, venture capital, and health care on a\nrecent episode of _NEJM AI_ Grand Rounds.1 He emphasizes the potential for AI\nto democratize access to high-quality health care, analogous to how consumer\ntechnology has equalized access to information and entertainment. Dr. Pande\nargues that AI\u2019s impact on health care will be most significant in preventive\ncare and early intervention rather than in treating advanced disease states.\nHe also provides insights into a16z\u2019s investment thesis for health care\ncompanies, and highlights the need for individuals with expertise in both\nclinical medicine and AI to drive innovation in this field. Looking ahead,\nwhile current AI systems excel at tasks based on existing knowledge, the next\nmajor breakthrough may require developing AIs capable of exploration,\ncreation, and discovery akin to human scientific inquiry.\n\n",
      "date_published": "2025-01-21T00:00:00+00:00",
      "authors": [
        {
          "name": "V. Pande, A.K. Manrai, and A.L. Beam"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401103",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Dr. Vijay Pande, General Partner at Andreessen Horowitz (a16z), discusses the intersection of artificial intelligence, venture capital, and health care on a recent episode of <i>NEJM AI</i> Grand Rounds.<sup>1</sup> He emphasizes the potential for AI to democratize access to high-quality health care, analogous to how consumer technology has equalized access to information and entertainment. Dr. Pande argues that AI\u2019s impact on health care will be most significant in preventive care and early intervention rather than in treating advanced disease states. He also provides insights into a16z\u2019s investment thesis for health care companies, and highlights the need for individuals with expertise in both clinical medicine and AI to drive innovation in this field. Looking ahead, while current AI systems excel at tasks based on existing knowledge, the next major breakthrough may require developing AIs capable of exploration, creation, and discovery akin to human scientific inquiry.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400590",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400590",
      "title": "Developing ICU Clinical Behavioral Atlas Using Ambient Intelligence and Computer Vision",
      "summary": "The Clinical Behavioral Atlas, a pioneering computer vision system tailored for the intensive care unit setting, is capable of automatically detecting a broad range of clinically relevant activities, objects, and personnel categories through RGB video data, making strides in ICU observational research.",
      "content_text": "## Abstract\n\n### Background\n\nWhile computer vision has gained traction in medical applications, models\nspecifically engineered for intensive care unit (ICU) activities are limited.\n\n### Methods\n\nWe present Clinical Behavioral Atlas (CBA), a computer vision system that can\nidentify 40 clinically relevant activity categories and 55 object categories\nsolely through RGB video data. The system was developed using a dataset\ncomprising over 140,000 hours of continuous video and over 350,000 densely\nannotated frames, collected from 16 sensors in 8 ICU rooms at an academic\nmedical center.\n\n### Results\n\nThe model demonstrated strong performance in entity and activity detection,\nwith sensitivities of 0.75\u223c0.81 and average precisions of 0.64\u223c0.73,\nrespectively. Permutation tests yielded P values of less than 0.05 for most\nactivity categories. We observed a positive correlation between the\nperformance and both the number and size of entities. The model excelled at\nidentifying common and large objects, even with limited samples, but struggled\nwith small items like oral swabs. Activity detection performance correlated\nlinearly with video duration. The model showed robust performance (>0.85\naverage precision) for most clinical activities, but activities of daily\nliving exhibited greater variation and lower average precision (0.23\u20130.95),\nindicating potential for further refinement due to their complexity and\nrelative scarcity in the dataset. Experiments against other popular activity\nrecognition models reveal that our method substantially outperforms all\nbaselines, with improvements of 0.30 and 0.45 in average precision over the\nnext best method.\n\n### Conclusions\n\nCBA expands automated identification of clinically important bedside clinical\nactions such as ICU preventive bundle elements. While we have demonstrated the\nfeasibility of computer vision as a tool to assist in clinical care in high-\nintensity settings such as the ICU, the development of a full clinical-level\nperformance CBA model will require larger datasets, ideally from multiple\nlocations. (Funded by Schmidt Futures and others.)\n\n",
      "date_published": "2025-01-21T00:00:00+00:00",
      "authors": [
        {
          "name": "W. Dai and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400590",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/6aa05beb-e69e-4062-b748-9a791acbd42b/aioa2400590_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">While computer vision has gained traction in medical applications, models specifically engineered for intensive care unit (ICU) activities are limited.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We present Clinical Behavioral Atlas (CBA), a computer vision system that can identify 40 clinically relevant activity categories and 55 object categories solely through RGB video data. The system was developed using a dataset comprising over 140,000 hours of continuous video and over 350,000 densely annotated frames, collected from 16 sensors in 8 ICU rooms at an academic medical center.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">The model demonstrated strong performance in entity and activity detection, with sensitivities of 0.75\u223c0.81 and average precisions of 0.64\u223c0.73, respectively. Permutation tests yielded P values of less than 0.05 for most activity categories. We observed a positive correlation between the performance and both the number and size of entities. The model excelled at identifying common and large objects, even with limited samples, but struggled with small items like oral swabs. Activity detection performance correlated linearly with video duration. The model showed robust performance (&gt;0.85 average precision) for most clinical activities, but activities of daily living exhibited greater variation and lower average precision (0.23\u20130.95), indicating potential for further refinement due to their complexity and relative scarcity in the dataset. Experiments against other popular activity recognition models reveal that our method substantially outperforms all baselines, with improvements of 0.30 and 0.45 in average precision over the next best method.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">CBA expands automated identification of clinically important bedside clinical actions such as ICU preventive bundle elements. While we have demonstrated the feasibility of computer vision as a tool to assist in clinical care in high-intensity settings such as the ICU, the development of a full clinical-level performance CBA model will require larger datasets, ideally from multiple locations. (Funded by Schmidt Futures and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400979",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400979",
      "title": "The Burden of Reviewing LLM-Generated Content",
      "summary": "We propose further study on the use of LLMs to generate clinical documentation and communication, looking in particular at the net benefit on efficiency and cognitive burden when the physician\u2019s efforts are shifted from generating new content to reviewing content created by LLMs.",
      "content_text": "## Abstract\n\nLarge language models (LLMs) in health care aim to reduce the cognitive and\nadministrative burden for health care professionals. Yet the accuracy and\ncompleteness of LLM-generated material is still highly variable. Reviewing\noutput for errors is critical to ensure documentation is accurate and\ncomplete. Unfortunately, humans tend to follow the path of least cognitive\neffort and will quickly become overly reliant on automation aids. Developers\nargue that LLM applications in health care are not automatic solutions as they\nrequire a human-in-the-loop approach, and that the risk of errors is\nadequately mitigated by relying on physician review. However, the perception\nof LLMs as outperforming humans may lead to overestimating the capabilities of\nthe models. When humans begin to rely more heavily on LLM output, the\nautomation argument is no longer valid. While the physician has the final\ndetermination in practice, this approach is grounded in two principles: the\nassumption that every physician will thoroughly review all output, every time,\nfor every patient; and the reliance on disclaimers to cover liability and\nshift accountability from developers to physicians. This creates a new and\ntedious burden on physicians \u2014 proofreading content that was neither written\nnor dictated by the user is difficult to do well. Minimal research has been\npublished to understand the net benefit on efficiency and cognitive burden\nwhen physicians\u2019 efforts are shifted from generating new content to reviewing\ncontent generated by LLMs. Furthermore, the use of LLMs increases physicians\u2019\naccountability for something they did not write. The liability of missing a\nhigh-risk error in LLM output is a novel factor for consideration and not well\nunderstood. Further study on the use of LLMs to generate clinical\ndocumentation or clinical communication should consider the cognitive impact\nof proofreading and the increased accountability of physicians. We recommend\nintegrating implementation science, user experience research, and\ncollaborative input from policy makers, regulators, and professional medical\nassociations to establish a shared accountability structure. This approach\naims to protect patients while empowering physicians to perform their duties\nconfidently, ultimately enhancing both patient safety and physician\nsatisfaction.\n\n",
      "date_published": "2025-01-13T00:00:00+00:00",
      "authors": [
        {
          "name": "J.W. Ohde, L.M. Rost, and J.D. Overgaard"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400979",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Large language models (LLMs) in health care aim to reduce the cognitive and administrative burden for health care professionals. Yet the accuracy and completeness of LLM-generated material is still highly variable. Reviewing output for errors is critical to ensure documentation is accurate and complete. Unfortunately, humans tend to follow the path of least cognitive effort and will quickly become overly reliant on automation aids. Developers argue that LLM applications in health care are not automatic solutions as they require a human-in-the-loop approach, and that the risk of errors is adequately mitigated by relying on physician review. However, the perception of LLMs as outperforming humans may lead to overestimating the capabilities of the models. When humans begin to rely more heavily on LLM output, the automation argument is no longer valid. While the physician has the final determination in practice, this approach is grounded in two principles: the assumption that every physician will thoroughly review all output, every time, for every patient; and the reliance on disclaimers to cover liability and shift accountability from developers to physicians. This creates a new and tedious burden on physicians \u2014 proofreading content that was neither written nor dictated by the user is difficult to do well. Minimal research has been published to understand the net benefit on efficiency and cognitive burden when physicians\u2019 efforts are shifted from generating new content to reviewing content generated by LLMs. Furthermore, the use of LLMs increases physicians\u2019 accountability for something they did not write. The liability of missing a high-risk error in LLM output is a novel factor for consideration and not well understood. Further study on the use of LLMs to generate clinical documentation or clinical communication should consider the cognitive impact of proofreading and the increased accountability of physicians. We recommend integrating implementation science, user experience research, and collaborative input from policy makers, regulators, and professional medical associations to establish a shared accountability structure. This approach aims to protect patients while empowering physicians to perform their duties confidently, ultimately enhancing both patient safety and physician satisfaction.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400889",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400889",
      "title": "Using Large Language Models to Promote Health Equity",
      "summary": "We emphasize the emerging potential of LLMs to promote health equity by highlighting three promising use cases: detecting human biases (e.g., from clinical notes); creating structured equity-relevant databases from unstructured text; and improving equity of access to health information.",
      "content_text": "## Abstract\n\nWhile the discussion about the effects of large language models (LLMs) on\nhealth equity has been largely cautionary, LLMs also present significant\nopportunities for improving health equity. We highlight three such\nopportunities: improving the detection of human bias; creating structured\ndatasets relevant to health equity; and improving equity of access to health\ninformation.\n\n",
      "date_published": "2025-01-13T00:00:00+00:00",
      "authors": [
        {
          "name": "E. Pierson and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400889",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/8b997667-e09c-4dbb-a6ba-7c21b50554f2/aip2400889_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">While the discussion about the effects of large language models (LLMs) on health equity has been largely cautionary, LLMs also present significant opportunities for improving health equity. We highlight three such opportunities: improving the detection of human bias; creating structured datasets relevant to health equity; and improving equity of access to health information.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "title": "RAG in Health Care: A Novel Framework for Improving Communication and Decision-Making by Addressing LLM Limitations",
      "summary": "This review article explores and evaluates the opportunities in using retrieval augmented generation (RAG) to overcome existing LLM limitations when used within the healthcare and pharmaceutical domain. It provides a framework of how RAG works and describes its strengths and weaknesses when applied to the field of medicine.",
      "content_text": "## Abstract\n\nWithin the field of artificial intelligence (AI), large language models (LLMs)\nhave the potential to transform the delivery of medical information. LLMs, as\na subset of generative AI, have demonstrated value in content creation, idea\ngeneration, and interactive communication. However, their inherent\nlimitations, such as the need for up-to-date information, hallucinations of\nincorrect facts, and a reliance on public-domain data, restrict the full\npotential of generative AI within the health care setting. To address these\nlimitations, retrieval-augmented generation (RAG) offers a novel framework by\nconnecting LLMs with external knowledge, enabling them to access information\nbeyond their training data. Within the health care domain, additional datasets\ncould include peer-reviewed studies, gated medical compendiums, and the\ninternal policies of health care organizations such as hospitals or\npharmaceutical companies. By leveraging RAG, existing generative AI tools gain\nthe capability to consider both public and private information, expanding\ntheir application and enhancing accuracy and relevance within the health care\nsetting. The utility of RAG in the health care setting has yet to be fully\nexplored, but it has the potential to revolutionize the industry. This article\nseeks to outline present and future use cases of RAG for health care\ninformation exchange within both clinical and industrial settings.\n\n",
      "date_published": "2024-12-23T00:00:00+00:00",
      "authors": [
        {
          "name": "K.K.Y. Ng, I. Matsuba, and P.C. Zhang"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2400380",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIra2400380_Zhang-1734963181703.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Within the field of artificial intelligence (AI), large language models (LLMs) have the potential to transform the delivery of medical information. LLMs, as a subset of generative AI, have demonstrated value in content creation, idea generation, and interactive communication. However, their inherent limitations, such as the need for up-to-date information, hallucinations of incorrect facts, and a reliance on public-domain data, restrict the full potential of generative AI within the health care setting. To address these limitations, retrieval-augmented generation (RAG) offers a novel framework by connecting LLMs with external knowledge, enabling them to access information beyond their training data. Within the health care domain, additional datasets could include peer-reviewed studies, gated medical compendiums, and the internal policies of health care organizations such as hospitals or pharmaceutical companies. By leveraging RAG, existing generative AI tools gain the capability to consider both public and private information, expanding their application and enhancing accuracy and relevance within the health care setting. The utility of RAG in the health care setting has yet to be fully explored, but it has the potential to revolutionize the industry. This article seeks to outline present and future use cases of RAG for health care information exchange within both clinical and industrial settings.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2400927",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2400927",
      "title": "Managing Patient Use of Generative Health AI",
      "summary": "In reviewing the benefits and risks of patients\u2019 use of generative artificial intelligence and policy approaches to increasing trust and containing risks associated with this use, the authors conclude that while government will play a role in helping patients use generative AI effectively, major responsibility will fall to the private sector.",
      "content_text": "## Abstract\n\nPatients\u2019 use of artificial intelligence, or patient AI (PAI), is now\nwidespread, promising both benefits and risks. There is an urgent need to\nassist patients with using these new technologies as safely and effectively as\npossible. Focusing on patients\u2019 use of large language models (LLMs) for health\ncare purposes, this article explores critical issues in the management of\ngenerative PAI in the United States, including constitutional limits on\ngovernment\u2019s regulatory authority, and identifies opportunities for public and\nprivate actors to help patients take advantage of generative PAI safely. With\nrespect to the public sector, there is a critical need for federal action to\nfund research on the benefits and risks of PAI and on the development of valid\nmetrics and performance standards for PAI. The federal government also needs\nto better protect the privacy and security of patient data shared with PAI,\nand to require transparency with respect to how LLMs used for health care\npurposes are funded, how they are developed, and how they perform. (Supported\nby the Commonwealth Fund.)\n\n",
      "date_published": "2024-12-20T00:00:00+00:00",
      "authors": [
        {
          "name": "D. Blumenthal and C. Goldberg"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2400927",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Patients\u2019 use of artificial intelligence, or patient AI (PAI), is now widespread, promising both benefits and risks. There is an urgent need to assist patients with using these new technologies as safely and effectively as possible. Focusing on patients\u2019 use of large language models (LLMs) for health care purposes, this article explores critical issues in the management of generative PAI in the United States, including constitutional limits on government\u2019s regulatory authority, and identifies opportunities for public and private actors to help patients take advantage of generative PAI safely. With respect to the public sector, there is a critical need for federal action to fund research on the benefits and risks of PAI and on the development of valid metrics and performance standards for PAI. The federal government also needs to better protect the privacy and security of patient data shared with PAI, and to require transparency with respect to how LLMs used for health care purposes are funded, how they are developed, and how they perform. (Supported by the Commonwealth Fund.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "title": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations",
      "summary": "We set out recommendations enabling those creating and/or using health care datasets to identify and transparently acknowledge underlying biases.",
      "content_text": "## Abstract\n\nWithout careful dissection of the ways in which biases can be encoded into\nartificial intelligence (AI) health technologies, there is a risk of\nperpetuating existing health inequalities at scale. One major source of bias\nis the data that underpins such technologies. The STANDING Together\nrecommendations aim to encourage transparency regarding limitations of health\ndatasets and proactive evaluation of their effect across population groups.\nDraft recommendation items were informed by a systematic review and\nstakeholder survey. The recommendations were developed using a Delphi\napproach, supplemented by a public consultation and international interview\nstudy. Overall, more than 350 representatives from 58 countries provided input\ninto this initiative. 194 Delphi participants from 25 countries voted and\nprovided comments on 32 candidate items across three electronic survey rounds\nand one in-person consensus meeting. The 29 STANDING Together consensus\nrecommendations are presented here in two parts. Recommendations for\nDocumentation of Health Datasets provide guidance for dataset curators to\nenable transparency around data composition and limitations. Recommendations\nfor Use of Health Datasets aim to enable identification and mitigation of\nalgorithmic biases that might exacerbate health inequalities. These\nrecommendations are intended to prompt proactive inquiry rather than acting as\na checklist. We hope to raise awareness that no dataset is free of\nlimitations, so transparent communication of data limitations should be\nperceived as valuable, and absence of this information as a limitation. We\nhope that adoption of the STANDING Together recommendations by stakeholders\nacross the AI health technology lifecycle will enable everyone in society to\nbenefit from technologies which are safe and effective. (Funded by The NHS AI\nLab and The Health Foundation, and supported by the National Institute for\nHealth and Care Research [NIHR].)\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "J.E. Alderman and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401088",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective. (Funded by The NHS AI Lab and The Health Foundation, and supported by the National Institute for Health and Care Research [NIHR].)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "title": "How Generalizable Are Foundation Models When Applied to Different Demographic Groups and Settings?",
      "summary": "An evaluation of the foundational model RETFound on an Asian-specific dataset indicates the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need to include more diverse data and collaborate globally on research.",
      "content_text": "## Abstract\n\nRETFound is a retinal image\u2013based foundational artificial intelligence (AI)\nmodel that can be fine-tuned to downstream tasks. However, its\ngeneralizability to Asian populations remains unclear. In this study, we fine-\ntuned RETFound on an Asian-specific dataset. We then evaluated the performance\nof RETFound versus a conventional Vision Transformer model (pretrained on\nImageNet) in diagnosing glaucoma and coronary heart disease and predicting the\n3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d\ndataset, RETFound showed no significant improvement compared with a\nconventional Vision Transformer model (area under the curves [AUCs] of 0.863,\n0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2).\nFurthermore, in scenarios with limited training data (fine-tuned on \u226425% of\nthe full dataset), RETFound showed a slight advantage (up to a maximum AUC\nincrease of 0.03). However, these improvements were not statistically\nsignificant (all P\u22650.2). These findings indicate the challenges foundational\nAI models face in adapting to diverse demographics, emphasizing the need for\nmore diverse data in current foundation models and the importance of global\ncollaboration on foundation model research.\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "Z. Xiong and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400497",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/925aa4d2-c256-45bf-ade8-81f317c70045/aics2400497_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">RETFound is a retinal image\u2013based foundational artificial intelligence (AI) model that can be fine-tuned to downstream tasks. However, its generalizability to Asian populations remains unclear. In this study, we fine-tuned RETFound on an Asian-specific dataset. We then evaluated the performance of RETFound versus a conventional Vision Transformer model (pretrained on ImageNet) in diagnosing glaucoma and coronary heart disease and predicting the 3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d dataset, RETFound showed no significant improvement compared with a conventional Vision Transformer model (area under the curves [AUCs] of 0.863, 0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2). Furthermore, in scenarios with limited training data (fine-tuned on \u226425% of the full dataset), RETFound showed a slight advantage (up to a maximum AUC increase of 0.03). However, these improvements were not statistically significant (all P\u22650.2). These findings indicate the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need for more diverse data in current foundation models and the importance of global collaboration on foundation model research.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401073",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401073",
      "title": "The Promise and Perils of Autonomous AI in Science",
      "summary": "We recommend the development of clearer guidelines and ethical standards for the use of artificial intelligence (AI) in research, fostering human-AI collaboration to enhance research quality while preserving human oversight and integrating the innovative \u201cdata-chaining\u201d transparency mechanisms more broadly to support reproducibility and traceability.",
      "content_text": "## Abstract\n\nIn this edition of _NEJM AI_ , Ifargan and colleagues present data-to-paper,\nan autonomous platform designed to mimic human scientific practice by guiding\na large language model through a stepwise research process to produce complete\nresearch papers. While the platform shows promise, significant errors\nfrequently occur with complex datasets, requiring human intervention for\ncorrection. Here, we explore the risks of over-reliance on such automated\ntools, echoing potential threats to scientific integrity and a surge in low-\nquality publications, while also considering their potential role in the\nreproduction and verification of scientific findings. To address these\nchallenges, we recommend the development of clearer guidelines and ethical\nstandards for the use of artificial intelligence (AI) in research, fostering\nhuman\u2013AI collaboration to enhance research quality while preserving human\noversight, and integrating the innovative \u201cdata-chaining\u201d transparency\nmechanisms more broadly to support reproducibility and traceability.\n\n",
      "date_published": "2024-12-16T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Gao and E.M. Harrison"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401073",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In this edition of <i>NEJM AI</i>, Ifargan and colleagues present data-to-paper, an autonomous platform designed to mimic human scientific practice by guiding a large language model through a stepwise research process to produce complete research papers. While the platform shows promise, significant errors frequently occur with complex datasets, requiring human intervention for correction. Here, we explore the risks of over-reliance on such automated tools, echoing potential threats to scientific integrity and a surge in low-quality publications, while also considering their potential role in the reproduction and verification of scientific findings. To address these challenges, we recommend the development of clearer guidelines and ethical standards for the use of artificial intelligence (AI) in research, fostering human\u2013AI collaboration to enhance research quality while preserving human oversight, and integrating the innovative \u201cdata-chaining\u201d transparency mechanisms more broadly to support reproducibility and traceability.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "title": "Ethical Use of Artificial Intelligence in Medical Diagnostics Demands a Focus on Accuracy, Not Fairness",
      "summary": "We argue that in developing and evaluating AI tools for medical diagnostics, rather than focusing on fairness criteria that quantify disparities between protected subpopulations, our first and most important objective should be to track and maximize diagnostic accuracy for all subpopulations.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) promises to be a transformative technology for\nmedicine and health care. As such, there is an increasing interest in ensuring\nits ethical use. In this perspective, we consider the employment of AI for\nmedical diagnostics, where the goal is the detection and classification of an\nunderlying pathology, based on data such as patient information, clinical\npresentation, tests, and imaging. We argue that instead of prioritizing\nfairness criteria that measure disparities between protected groups, the\nprimary goal should be to assess and enhance diagnostic accuracy within each\nsubpopulation. This approach shifts the focus from optimizing overall\npopulation accuracy to ensuring maximal accuracy in each subpopulation. Our\nperspective implies that we should be using all available information,\nincluding protected group identity, in our methods. We decouple the goal of\naccurate diagnosis from fairness considerations in screening and postdiagnosis\nclinical decisions, which often require the allocation of finite resources.\nFurthermore, we underscore the importance of collecting high-quality and\nrepresentative datasets for each subpopulation, including ensuring that the\nground truth labels we train and with which we evaluate are unbiased.\n\n",
      "date_published": "2024-12-13T00:00:00+00:00",
      "authors": [
        {
          "name": "M.R. Sabuncu, A.Q. Wang, and M. Nguyen"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400672",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) promises to be a transformative technology for medicine and health care. As such, there is an increasing interest in ensuring its ethical use. In this perspective, we consider the employment of AI for medical diagnostics, where the goal is the detection and classification of an underlying pathology, based on data such as patient information, clinical presentation, tests, and imaging. We argue that instead of prioritizing fairness criteria that measure disparities between protected groups, the primary goal should be to assess and enhance diagnostic accuracy within each subpopulation. This approach shifts the focus from optimizing overall population accuracy to ensuring maximal accuracy in each subpopulation. Our perspective implies that we should be using all available information, including protected group identity, in our methods. We decouple the goal of accurate diagnosis from fairness considerations in screening and postdiagnosis clinical decisions, which often require the allocation of finite resources. Furthermore, we underscore the importance of collecting high-quality and representative datasets for each subpopulation, including ensuring that the ground truth labels we train and with which we evaluate are unbiased.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "title": "Autonomous LLM-Driven Research \u2014 from Data to Human-Verifiable Research Papers",
      "summary": "Our study demonstrates a potential for AI-driven acceleration of scientific discovery in biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.",
      "content_text": "## Abstract\n\n### Background\n\nArtificial intelligence (AI) promises to accelerate scientific discovery, but\nit remains unclear whether AI systems can perform fully autonomous research,\nand whether they can do so while adhering to key scientific values, such as\ntransparency, traceability, and verifiability. The aim of this study was to\ndevelop and evaluate an AI-automation platform that performs transparent,\ntraceable, and human-verifiable scientific research.\n\n### Methods\n\nTo mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation\nplatform that guides interacting large language model (LLM) agents through a\ncomplete stepwise research process that starts with annotated data and results\nin comprehensive research papers, while programmatically backtracing\ninformation flow and allowing human oversight and interactions. The platform\ncan run fully autonomously (in autopilot mode) or with human intervention (in\ncopilot mode).\n\n### Results\n\nIn autopilot mode, provided only with annotated data, data-to-paper raised\nhypotheses; designed research plans; wrote and debugged analysis codes;\ngenerated and interpreted results; and created complete, information-traceable\nresearch papers. Even though the research novelty of manuscripts created by\ndata-to-paper was relatively limited, the process demonstrated the autonomous\ngeneration of de novo quantitative insights from data, such as unraveling\nassociations between health indicators and clinical outcomes. For simple\nresearch goals and datasets, a fully autonomous cycle can create manuscripts\nthat independently recapitulate the findings of peer-reviewed biomedical\npublications without major errors in about 80 to 90% of cases. Yet, as goal or\ndata complexity increases, human copiloting becomes critical for ensuring\naccuracy and overall quality. By tracking information flow through the steps,\nthe platform creates \u201cdata-chained\u201d manuscripts, in which downstream results\nare programmatically linked to upstream code and data, thus setting a new\nstandard for the verifiability of scientific outputs.\n\n### Conclusions\n\nOur work demonstrates the potential for AI-driven acceleration of scientific\ndiscovery in data-driven biomedical research and beyond, while enhancing,\nrather than jeopardizing, traceability, transparency, and verifiability.\n\n",
      "date_published": "2024-12-03T00:00:00+00:00",
      "authors": [
        {
          "name": "T. Ifargan and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400555",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/06a1651e-79ce-4d95-887b-9fbf5ed07916/aioa2400555_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Artificial intelligence (AI) promises to accelerate scientific discovery, but it remains unclear whether AI systems can perform fully autonomous research, and whether they can do so while adhering to key scientific values, such as transparency, traceability, and verifiability. The aim of this study was to develop and evaluate an AI-automation platform that performs transparent, traceable, and human-verifiable scientific research.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">To mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation platform that guides interacting large language model (LLM) agents through a complete stepwise research process that starts with annotated data and results in comprehensive research papers, while programmatically backtracing information flow and allowing human oversight and interactions. The platform can run fully autonomously (in autopilot mode) or with human intervention (in copilot mode).</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">In autopilot mode, provided only with annotated data, data-to-paper raised hypotheses; designed research plans; wrote and debugged analysis codes; generated and interpreted results; and created complete, information-traceable research papers. Even though the research novelty of manuscripts created by data-to-paper was relatively limited, the process demonstrated the autonomous generation of de novo quantitative insights from data, such as unraveling associations between health indicators and clinical outcomes. For simple research goals and datasets, a fully autonomous cycle can create manuscripts that independently recapitulate the findings of peer-reviewed biomedical publications without major errors in about 80 to 90% of cases. Yet, as goal or data complexity increases, human copiloting becomes critical for ensuring accuracy and overall quality. By tracking information flow through the steps, the platform creates \u201cdata-chained\u201d manuscripts, in which downstream results are programmatically linked to upstream code and data, thus setting a new standard for the verifiability of scientific outputs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Our work demonstrates the potential for AI-driven acceleration of scientific discovery in data-driven biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2400464",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2400464",
      "title": "Disclosure, Humanizing, and Contextual Vulnerability of Generative AI Chatbots",
      "summary": "One key decision for regulatory bodies is whether to require app makers to disclose the use of generative AI-powered chatbots in their products. We suggest that some generative AI-based chatbots lead consumers to use chatbots in unintended ways that make consumers contextually vulnerable, and we discuss how managers and regulators can proactively address this challenge.",
      "content_text": "## Abstract\n\nIn the wake of recent advancements in generative artificial intelligence (AI),\nregulatory bodies are trying to keep pace. One key decision is whether to\nrequire app makers to disclose the use of generative AI-powered chatbots in\ntheir products. We suggest that some generative AI-based chatbots lead\nconsumers to use chatbots in unintended ways that create mental health risks,\nmaking consumers contextually vulnerable \u2014 defined as a temporary state of\nsusceptibility to harm or other adverse mental health effects arising from the\ninterplay between a user\u2019s interactions with a particular system and the\nsystem\u2019s response. We argue that for health apps, including medical devices\nand wellness apps, disclosure should be mandated. We also show that even when\nchatbots are disclosed in these instances, they may still carry risks due to\nthe tendency of app makers to humanize their chatbots. The current regulatory\nstructure does not fully address these challenges. We discuss how app makers\nand regulators should proactively address this challenge by considering where\napps fall along the continuum of perceived humanness. For health-related apps,\nthis evaluation should lead to a mandate or strong recommendation that neutral\n(nonhumanized) chatbots be the default, with any deviations from this standard\nrequiring clear justification. (Funded in part by a Novo Nordisk Foundation\nGrant; NNF23SA0087056.)\n\n",
      "date_published": "2024-01-21T00:00:00+00:00",
      "authors": [
        {
          "name": "J. De Freitas and I.G. Cohen"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2400464",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/6cb77c22-220e-44fe-8cae-bdef14a73e73/aipc2400464_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In the wake of recent advancements in generative artificial intelligence (AI), regulatory bodies are trying to keep pace. One key decision is whether to require app makers to disclose the use of generative AI-powered chatbots in their products. We suggest that some generative AI-based chatbots lead consumers to use chatbots in unintended ways that create mental health risks, making consumers contextually vulnerable \u2014 defined as a temporary state of susceptibility to harm or other adverse mental health effects arising from the interplay between a user\u2019s interactions with a particular system and the system\u2019s response. We argue that for health apps, including medical devices and wellness apps, disclosure should be mandated. We also show that even when chatbots are disclosed in these instances, they may still carry risks due to the tendency of app makers to humanize their chatbots. The current regulatory structure does not fully address these challenges. We discuss how app makers and regulators should proactively address this challenge by considering where apps fall along the continuum of perceived humanness. For health-related apps, this evaluation should lead to a mandate or strong recommendation that neutral (nonhumanized) chatbots be the default, with any deviations from this standard requiring clear justification. (Funded in part by a Novo Nordisk Foundation Grant; NNF23SA0087056.)</div>"
    }
  ]
}