{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "title": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations",
      "summary": "We set out recommendations enabling those creating and/or using health care datasets to identify and transparently acknowledge underlying biases.",
      "content_text": "## Abstract\n\nWithout careful dissection of the ways in which biases can be encoded into\nartificial intelligence (AI) health technologies, there is a risk of\nperpetuating existing health inequalities at scale. One major source of bias\nis the data that underpins such technologies. The STANDING Together\nrecommendations aim to encourage transparency regarding limitations of health\ndatasets and proactive evaluation of their effect across population groups.\nDraft recommendation items were informed by a systematic review and\nstakeholder survey. The recommendations were developed using a Delphi\napproach, supplemented by a public consultation and international interview\nstudy. Overall, more than 350 representatives from 58 countries provided input\ninto this initiative. 194 Delphi participants from 25 countries voted and\nprovided comments on 32 candidate items across three electronic survey rounds\nand one in-person consensus meeting. The 29 STANDING Together consensus\nrecommendations are presented here in two parts. Recommendations for\nDocumentation of Health Datasets provide guidance for dataset curators to\nenable transparency around data composition and limitations. Recommendations\nfor Use of Health Datasets aim to enable identification and mitigation of\nalgorithmic biases that might exacerbate health inequalities. These\nrecommendations are intended to prompt proactive inquiry rather than acting as\na checklist. We hope to raise awareness that no dataset is free of\nlimitations, so transparent communication of data limitations should be\nperceived as valuable, and absence of this information as a limitation. We\nhope that adoption of the STANDING Together recommendations by stakeholders\nacross the AI health technology lifecycle will enable everyone in society to\nbenefit from technologies which are safe and effective. (Funded by The NHS AI\nLab and The Health Foundation, and supported by the National Institute for\nHealth and Care Research [NIHR].)\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "J.E. Alderman and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401088",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective. (Funded by The NHS AI Lab and The Health Foundation, and supported by the National Institute for Health and Care Research [NIHR].)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "title": "How Generalizable Are Foundation Models When Applied to Different Demographic Groups and Settings?",
      "summary": "An evaluation of the foundational model RETFound on an Asian-specific dataset indicates the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need to include more diverse data and collaborate globally on research.",
      "content_text": "## Abstract\n\nRETFound is a retinal image\u2013based foundational artificial intelligence (AI)\nmodel that can be fine-tuned to downstream tasks. However, its\ngeneralizability to Asian populations remains unclear. In this study, we fine-\ntuned RETFound on an Asian-specific dataset. We then evaluated the performance\nof RETFound versus a conventional Vision Transformer model (pretrained on\nImageNet) in diagnosing glaucoma and coronary heart disease and predicting the\n3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d\ndataset, RETFound showed no significant improvement compared with a\nconventional Vision Transformer model (area under the curves [AUCs] of 0.863,\n0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2).\nFurthermore, in scenarios with limited training data (fine-tuned on \u226425% of\nthe full dataset), RETFound showed a slight advantage (up to a maximum AUC\nincrease of 0.03). However, these improvements were not statistically\nsignificant (all P\u22650.2). These findings indicate the challenges foundational\nAI models face in adapting to diverse demographics, emphasizing the need for\nmore diverse data in current foundation models and the importance of global\ncollaboration on foundation model research.\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "Z. Xiong and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400497",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/a7df80bc-2fdc-4805-abca-a9270bb157ec/aics2400497_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">RETFound is a retinal image\u2013based foundational artificial intelligence (AI) model that can be fine-tuned to downstream tasks. However, its generalizability to Asian populations remains unclear. In this study, we fine-tuned RETFound on an Asian-specific dataset. We then evaluated the performance of RETFound versus a conventional Vision Transformer model (pretrained on ImageNet) in diagnosing glaucoma and coronary heart disease and predicting the 3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d dataset, RETFound showed no significant improvement compared with a conventional Vision Transformer model (area under the curves [AUCs] of 0.863, 0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2). Furthermore, in scenarios with limited training data (fine-tuned on \u226425% of the full dataset), RETFound showed a slight advantage (up to a maximum AUC increase of 0.03). However, these improvements were not statistically significant (all P\u22650.2). These findings indicate the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need for more diverse data in current foundation models and the importance of global collaboration on foundation model research.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "title": "Ethical Use of Artificial Intelligence in Medical Diagnostics Demands a Focus on Accuracy, Not Fairness",
      "summary": "We argue that in developing and evaluating AI tools for medical diagnostics, rather than focusing on fairness criteria that quantify disparities between protected subpopulations, our first and most important objective should be to track and maximize diagnostic accuracy for all subpopulations.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) promises to be a transformative technology for\nmedicine and health care. As such, there is an increasing interest in ensuring\nits ethical use. In this perspective, we consider the employment of AI for\nmedical diagnostics, where the goal is the detection and classification of an\nunderlying pathology, based on data such as patient information, clinical\npresentation, tests, and imaging. We argue that instead of prioritizing\nfairness criteria that measure disparities between protected groups, the\nprimary goal should be to assess and enhance diagnostic accuracy within each\nsubpopulation. This approach shifts the focus from optimizing overall\npopulation accuracy to ensuring maximal accuracy in each subpopulation. Our\nperspective implies that we should be using all available information,\nincluding protected group identity, in our methods. We decouple the goal of\naccurate diagnosis from fairness considerations in screening and postdiagnosis\nclinical decisions, which often require the allocation of finite resources.\nFurthermore, we underscore the importance of collecting high-quality and\nrepresentative datasets for each subpopulation, including ensuring that the\nground truth labels we train and with which we evaluate are unbiased.\n\n",
      "date_published": "2024-12-13T00:00:00+00:00",
      "authors": [
        {
          "name": "M.R. Sabuncu, A.Q. Wang, and M. Nguyen"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400672",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) promises to be a transformative technology for medicine and health care. As such, there is an increasing interest in ensuring its ethical use. In this perspective, we consider the employment of AI for medical diagnostics, where the goal is the detection and classification of an underlying pathology, based on data such as patient information, clinical presentation, tests, and imaging. We argue that instead of prioritizing fairness criteria that measure disparities between protected groups, the primary goal should be to assess and enhance diagnostic accuracy within each subpopulation. This approach shifts the focus from optimizing overall population accuracy to ensuring maximal accuracy in each subpopulation. Our perspective implies that we should be using all available information, including protected group identity, in our methods. We decouple the goal of accurate diagnosis from fairness considerations in screening and postdiagnosis clinical decisions, which often require the allocation of finite resources. Furthermore, we underscore the importance of collecting high-quality and representative datasets for each subpopulation, including ensuring that the ground truth labels we train and with which we evaluate are unbiased.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "title": "Autonomous LLM-Driven Research \u2014 from Data to Human-Verifiable Research Papers",
      "summary": "Our study demonstrates a potential for AI-driven acceleration of scientific discovery in biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.",
      "content_text": "## Abstract\n\n### Background\n\nArtificial intelligence (AI) promises to accelerate scientific discovery, but\nit remains unclear whether AI systems can perform fully autonomous research,\nand whether they can do so while adhering to key scientific values, such as\ntransparency, traceability, and verifiability. The aim of this study was to\ndevelop and evaluate an AI-automation platform that performs transparent,\ntraceable, and human-verifiable scientific research.\n\n### Methods\n\nTo mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation\nplatform that guides interacting large language model (LLM) agents through a\ncomplete stepwise research process that starts with annotated data and results\nin comprehensive research papers, while programmatically backtracing\ninformation flow and allowing human oversight and interactions. The platform\ncan run fully autonomously (in autopilot mode) or with human intervention (in\ncopilot mode).\n\n### Results\n\nIn autopilot mode, provided only with annotated data, data-to-paper raised\nhypotheses; designed research plans; wrote and debugged analysis codes;\ngenerated and interpreted results; and created complete, information-traceable\nresearch papers. Even though the research novelty of manuscripts created by\ndata-to-paper was relatively limited, the process demonstrated the autonomous\ngeneration of de novo quantitative insights from data, such as unraveling\nassociations between health indicators and clinical outcomes. For simple\nresearch goals and datasets, a fully autonomous cycle can create manuscripts\nthat independently recapitulate the findings of peer-reviewed biomedical\npublications without major errors in about 80 to 90% of cases. Yet, as goal or\ndata complexity increases, human copiloting becomes critical for ensuring\naccuracy and overall quality. By tracking information flow through the steps,\nthe platform creates \u201cdata-chained\u201d manuscripts, in which downstream results\nare programmatically linked to upstream code and data, thus setting a new\nstandard for the verifiability of scientific outputs.\n\n### Conclusions\n\nOur work demonstrates the potential for AI-driven acceleration of scientific\ndiscovery in data-driven biomedical research and beyond, while enhancing,\nrather than jeopardizing, traceability, transparency, and verifiability.\n\n",
      "date_published": "2024-12-03T00:00:00+00:00",
      "authors": [
        {
          "name": "T. Ifargan and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400555",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/590c8332-e34e-4d7d-860f-17fb5d45e5cf/aioa2400555_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Artificial intelligence (AI) promises to accelerate scientific discovery, but it remains unclear whether AI systems can perform fully autonomous research, and whether they can do so while adhering to key scientific values, such as transparency, traceability, and verifiability. The aim of this study was to develop and evaluate an AI-automation platform that performs transparent, traceable, and human-verifiable scientific research.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">To mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation platform that guides interacting large language model (LLM) agents through a complete stepwise research process that starts with annotated data and results in comprehensive research papers, while programmatically backtracing information flow and allowing human oversight and interactions. The platform can run fully autonomously (in autopilot mode) or with human intervention (in copilot mode).</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">In autopilot mode, provided only with annotated data, data-to-paper raised hypotheses; designed research plans; wrote and debugged analysis codes; generated and interpreted results; and created complete, information-traceable research papers. Even though the research novelty of manuscripts created by data-to-paper was relatively limited, the process demonstrated the autonomous generation of de novo quantitative insights from data, such as unraveling associations between health indicators and clinical outcomes. For simple research goals and datasets, a fully autonomous cycle can create manuscripts that independently recapitulate the findings of peer-reviewed biomedical publications without major errors in about 80 to 90% of cases. Yet, as goal or data complexity increases, human copiloting becomes critical for ensuring accuracy and overall quality. By tracking information flow through the steps, the platform creates \u201cdata-chained\u201d manuscripts, in which downstream results are programmatically linked to upstream code and data, thus setting a new standard for the verifiability of scientific outputs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Our work demonstrates the potential for AI-driven acceleration of scientific discovery in data-driven biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401000",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401000",
      "title": "Reclaiming Voice with AI",
      "summary": "While AI is often dual use and concerns about voice cloning often center on potential misuse, we argue that suppressing this technology may inflict tangible harm on patients by denying them the chance to reclaim their voice.",
      "content_text": "## Abstract\n\nVoice impairments affect millions of Americans, with personalized text-to-\nspeech technology offering limited solutions due to the need for extensive\nvoice banking. Here, we report the case of Alexis Bogan, a 20-year-old patient\nwho acutely lost her voice after surgery to resect her brain stem\nhemangioblastoma. In a world-first application, OpenAI\u2019s Voice Engine was used\nto clone Ms. Bogan\u2019s voice from just 15 seconds of preexisting audio, sourced\nfrom a school project she had filmed a few years prior. This enabled her to\nuse a personalized text-to-speech app for daily communication while\nrehabilitating her speech. This case was highlighted on a recent episode of\nthe _NEJM AI_ Grand Rounds podcast,1 framing a broader discussion on voice\ncloning technology. While AI is often dual use and concerns about voice\ncloning often center on potential misuse, such as \u201cdeepfakes\u201d and\nmisinformation, we argue that suppressing this technology may inflict tangible\nharm on patients by denying them the chance to reclaim their voice. Inspired\nby Ms. Bogan\u2019s journey, we urge researchers, clinicians, ethicists,\npolicymakers, and tech companies to collaborate swiftly yet responsibly in\nadvancing AI voice cloning in health care. By doing so, we can empower\npatients to recover not just their voice, but also a fundamental aspect of\ntheir identity and quality of life.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "F.N. Mirza and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401000",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIp2401000_Ali-1732707614033.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Voice impairments affect millions of Americans, with personalized text-to-speech technology offering limited solutions due to the need for extensive voice banking. Here, we report the case of Alexis Bogan, a 20-year-old patient who acutely lost her voice after surgery to resect her brain stem hemangioblastoma. In a world-first application, OpenAI\u2019s Voice Engine was used to clone Ms. Bogan\u2019s voice from just 15 seconds of preexisting audio, sourced from a school project she had filmed a few years prior. This enabled her to use a personalized text-to-speech app for daily communication while rehabilitating her speech. This case was highlighted on a recent episode of the <i>NEJM AI</i> Grand Rounds podcast,<sup>1</sup> framing a broader discussion on voice cloning technology. While AI is often dual use and concerns about voice cloning often center on potential misuse, such as \u201cdeepfakes\u201d and misinformation, we argue that suppressing this technology may inflict tangible harm on patients by denying them the chance to reclaim their voice. Inspired by Ms. Bogan\u2019s journey, we urge researchers, clinicians, ethicists, policymakers, and tech companies to collaborate swiftly yet responsibly in advancing AI voice cloning in health care. By doing so, we can empower patients to recover not just their voice, but also a fundamental aspect of their identity and quality of life.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400867",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400867",
      "title": "FDA-Authorized AI/ML Tool for Sepsis Prediction: Development and Validation",
      "summary": "Sepsis is a life-threatening acute condition that requires accurate and rapid identification to guide proper treatment. This study outlines the development and validation of the first U.S. Food and Drug Administration\u2013authorized artificial intelligence\u2013based software to identify patients at risk of having sepsis.",
      "content_text": "## Abstract\n\n### Background\n\nSepsis is a life-threatening condition that demands prompt treatment for\nimproved patient outcomes. Its heterogenous presentation makes early detection\nchallenging, highlighting the need for effective risk assessment tools.\nArtificial intelligence (AI) models could potentially identify patients with\nsepsis, but none have previously been authorized by the U.S. Food and Drug\nAdministration (FDA) for commercial use. This study outlines the development\nand validation of the Sepsis ImmunoScore, the first FDA-authorized AI-based\nsoftware designed to identify patients at risk of sepsis.\n\n### Methods\n\nIn this prospective study, we enrolled adult patients (18+ years of age)\nsuspected of infection, as indicated by a blood culture order, from five U.S.\ninstitutions between April 2017 and July 2022. The participants were divided\ninto an algorithm development cohort (n=2366), an internal validation cohort\n(n=393), and an external validation cohort (n=698). The primary end point was\nsepsis presence (as defined by Sepsis-3) within 24 hours of test initiation.\nSecondary end points included length of hospital stay, intensive care unit\n(ICU) admission within 24 hours, mechanical ventilation use within 24 hours,\nvasopressor use within 24 hours, and in-hospital mortality.\n\n### Results\n\nThe Sepsis ImmunoScore demonstrated high diagnostic accuracy, with an area\nunder the curve of 0.85 (0.83 to 0.87) in the derivation cohort, 0.80 (0.74 to\n0.86) in internal validation, and 0.81 (0.77 to 0.86) in external validation.\nThe scores were categorized into four sepsis risk levels with corresponding\nlikelihood ratios: low (0.1), medium (0.5), high (2.1), and very high (8.3).\nThese risk categories also predicted in-hospital mortality: low (0.0%), medium\n(1.9%), high (8.7%), and very high (18.2%) in the external validation cohort.\nSimilar trends were observed for other metrics, such as length of hospital\nstay, ICU utilization, mechanical ventilation, and vasopressor use.\n\n### Conclusions\n\nThe Sepsis ImmunoScore demonstrated high accuracy for the identification and\nprediction of sepsis and critical illness metrics that could enable prompt\nidentification of patients at high risk of sepsis and adverse outcomes,\npotentially improving clinical decision-making and patient outcomes. (Funded\nby the Defense Threat Reduction Agency and others.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "A. Bhargava and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400867",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/23154c4d-4969-464d-962f-389b497979ee/aioa2400867_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Sepsis is a life-threatening condition that demands prompt treatment for improved patient outcomes. Its heterogenous presentation makes early detection challenging, highlighting the need for effective risk assessment tools. Artificial intelligence (AI) models could potentially identify patients with sepsis, but none have previously been authorized by the U.S. Food and Drug Administration (FDA) for commercial use. This study outlines the development and validation of the Sepsis ImmunoScore, the first FDA-authorized AI-based software designed to identify patients at risk of sepsis.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">In this prospective study, we enrolled adult patients (18+ years of age) suspected of infection, as indicated by a blood culture order, from five U.S. institutions between April 2017 and July 2022. The participants were divided into an algorithm development cohort (n=2366), an internal validation cohort (n=393), and an external validation cohort (n=698). The primary end point was sepsis presence (as defined by Sepsis-3) within 24 hours of test initiation. Secondary end points included length of hospital stay, intensive care unit (ICU) admission within 24 hours, mechanical ventilation use within 24 hours, vasopressor use within 24 hours, and in-hospital mortality.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">The Sepsis ImmunoScore demonstrated high diagnostic accuracy, with an area under the curve of 0.85 (0.83 to 0.87) in the derivation cohort, 0.80 (0.74 to 0.86) in internal validation, and 0.81 (0.77 to 0.86) in external validation. The scores were categorized into four sepsis risk levels with corresponding likelihood ratios: low (0.1), medium (0.5), high (2.1), and very high (8.3). These risk categories also predicted in-hospital mortality: low (0.0%), medium (1.9%), high (8.7%), and very high (18.2%) in the external validation cohort. Similar trends were observed for other metrics, such as length of hospital stay, ICU utilization, mechanical ventilation, and vasopressor use.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">The Sepsis ImmunoScore demonstrated high accuracy for the identification and prediction of sepsis and critical illness metrics that could enable prompt identification of patients at high risk of sepsis and adverse outcomes, potentially improving clinical decision-making and patient outcomes. (Funded by the Defense Threat Reduction Agency and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2300221",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2300221",
      "title": "Development and Validation of a Multimodal Multitask Vision\n                    Foundation Model for Generalist Ophthalmic Artificial\n                    Intelligence",
      "summary": "A proposal for an AI foundation model for ophthalmology that can process eight ophthalmic imaging modalities and adapt to a multitude of ophthalmic scenarios and applications.",
      "content_text": "## Abstract\n\n### Background\n\nSpecialized single-use, single-modality models often have limited or no\ngeneralization to new diseases, modalities, and clinical tasks. Foundation\nmodels are built for multipurpose use, enabling them to perform tasks even\nwhen not specifically pretrained for them, and to adapt to different clinical\napplications.\n\n### Methods\n\nWe present VisionFM, an artificial intelligence foundation model for\nophthalmology pretrained on 3.4 million images from over 500,000 individuals,\ncovering diverse ophthalmic diseases, imaging modalities and devices, and\nclinical scenarios. Pretrained based on eight modalities, VisionFM was tested\nfor multiple applications, including disease screening and detection,\nprognosis and prediction, and segmentation of lesions and anatomical\nstructures, on an ophthalmic database comprising 53 public and 12 private\ndatasets. We compared the model against ophthalmologists with varying\nexperience in ophthalmic and systemic disease diagnoses.\n\n### Results\n\nVisionFM outperformed baseline deep learning approaches in diagnosing ocular\ndiseases, achieving an average area under the receiver operating\ncharacteristic curve (AUROC) of 0.950 (95% confidence interval [CI], 0.941 to\n0.959) across eight disease categories and five imaging modalities in internal\nvalidation. In external validation, VisionFM achieved an AUROC of 0.945 (95%\nCI, 0.934 to 0.956) in fundus-based diabetic retinopathy recognition, and an\nAUROC of 0.974 (95% CI, 0.966 to 0.983) in optical coherence tomography\u2013based\nage-related macular degeneration recognition. In a comparative study of\ndiagnostic accuracy of 12 ocular diseases from fundus photographs, VisionFM\nshows diagnostic accuracy close to that of intermediate-level\nophthalmologists. Its generalizability extends to new imaging modalities and\ndevices, effectively handling dataset shifts. For example, VisionFM accurately\ngraded diabetic retinopathy with an AUROC of 0.935 (95% CI, 0.902 to 0.964)\nusing an imaging modality it was never exposed to during pretraining.\nFurthermore, VisionFM is able to predict both glaucoma progression and the\npresence of intracranial tumors directly from fundus photographs.\n\n### Conclusions\n\nVisionFM provides an efficient platform for diagnosis or prediction of\nmultiple diseases using multiple imaging modalities and is scalable to\nincorporate additional data, modalities, and applications via its open-sourced\nmodel weights and codebase. (Funded by the Research Grants Council (RGC) of\nHong Kong SAR and others.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Qiu and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2300221",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/4171a5db-da90-45e7-9ec3-4dd11dc5f849/aioa2300221_f3.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Specialized single-use, single-modality models often have limited or no generalization to new diseases, modalities, and clinical tasks. Foundation models are built for multipurpose use, enabling them to perform tasks even when not specifically pretrained for them, and to adapt to different clinical applications.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We present VisionFM, an artificial intelligence foundation model for ophthalmology pretrained on 3.4 million images from over 500,000 individuals, covering diverse ophthalmic diseases, imaging modalities and devices, and clinical scenarios. Pretrained based on eight modalities, VisionFM was tested for multiple applications, including disease screening and detection, prognosis and prediction, and segmentation of lesions and anatomical structures, on an ophthalmic database comprising 53 public and 12 private datasets. We compared the model against ophthalmologists with varying experience in ophthalmic and systemic disease diagnoses.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">VisionFM outperformed baseline deep learning approaches in diagnosing ocular diseases, achieving an average area under the receiver operating characteristic curve (AUROC) of 0.950 (95% confidence interval [CI], 0.941 to 0.959) across eight disease categories and five imaging modalities in internal validation. In external validation, VisionFM achieved an AUROC of 0.945 (95% CI, 0.934 to 0.956) in fundus-based diabetic retinopathy recognition, and an AUROC of 0.974 (95% CI, 0.966 to 0.983) in optical coherence tomography\u2013based age-related macular degeneration recognition. In a comparative study of diagnostic accuracy of 12 ocular diseases from fundus photographs, VisionFM shows diagnostic accuracy close to that of intermediate-level ophthalmologists. Its generalizability extends to new imaging modalities and devices, effectively handling dataset shifts. For example, VisionFM accurately graded diabetic retinopathy with an AUROC of 0.935 (95% CI, 0.902 to 0.964) using an imaging modality it was never exposed to during pretraining. Furthermore, VisionFM is able to predict both glaucoma progression and the presence of intracranial tumors directly from fundus photographs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">VisionFM provides an efficient platform for diagnosis or prediction of multiple diseases using multiple imaging modalities and is scalable to incorporate additional data, modalities, and applications via its open-sourced model weights and codebase. (Funded by the Research Grants Council (RGC) of Hong Kong SAR and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401024",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401024",
      "title": "A New Foundation Model for Multimodal Ophthalmic Images: Advancing Disease Detection and Prediction",
      "summary": "A new foundation model for ophthalmic images demonstrates important progress, particularly through its flexible approach to multimodal training and its application to image segmentation tasks.",
      "content_text": "## Abstract\n\nFoundation models are a powerful tool in ophthalmology for building\ngeneralizable systems that can be efficiently applied to a range of ocular and\nsystemic health tasks. A new foundation model for ophthalmic images\ndemonstrates important progress, particularly through its flexible approach to\nmultimodal training and its application to image segmentation tasks.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "M.A. Chia, Y. Zhou, and P.A. Keane"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401024",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Foundation models are a powerful tool in ophthalmology for building generalizable systems that can be efficiently applied to a range of ocular and systemic health tasks. A new foundation model for ophthalmic images demonstrates important progress, particularly through its flexible approach to multimodal training and its application to image segmentation tasks.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2400961",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2400961",
      "title": "Cognitive Bias in Large Language Models: Implications for Research and Practice",
      "summary": "This commentary discusses the presence of cognitive biases in the outputs of large language models and the implications for clinical decision support and human\u2013computer interaction.",
      "content_text": "## Abstract\n\nThe use of large language models (LLMs) such as ChatGPT in clinical settings\nis growing, but concerns about their susceptibility to cognitive biases\npersist. Wang and Redelmeier\u2019s study reveals that LLMs are prone to biases,\nraising important questions about their role in medical decision-making. To\nprevent errors in decision-making with LLMs, it is recommended that clinicians\naim to critically engage with LLMs (e.g. refuting their hypotheses rather than\nlooking for confirmation) researchers should focus on identifying and\nevaluating collaborative strategies between AI and human decision-making.\nFurthermore, research on context-specific implementation is important. We need\nto ensure that AI complements, rather than replicates, human cognitive\nprocesses. (Funded by the Netherlands Organisation for Health Research and\nDevelopment.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "L. Zwaan"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2400961",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">The use of large language models (LLMs) such as ChatGPT in clinical settings is growing, but concerns about their susceptibility to cognitive biases persist. Wang and Redelmeier\u2019s study reveals that LLMs are prone to biases, raising important questions about their role in medical decision-making. To prevent errors in decision-making with LLMs, it is recommended that clinicians aim to critically engage with LLMs (e.g. refuting their hypotheses rather than looking for confirmation) researchers should focus on identifying and evaluating collaborative strategies between AI and human decision-making. Furthermore, research on context-specific implementation is important. We need to ensure that AI complements, rather than replicates, human cognitive processes. (Funded by the Netherlands Organisation for Health Research and Development.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400639",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400639",
      "title": "Cognitive Biases and Artificial Intelligence",
      "summary": "Artificial intelligence models sometimes propagate racial and gender bias. This study shows that human-like cognitive biases are prevalent in generative artificial intelligence and sometimes much more so than among practicing clinicians.",
      "content_text": "## Abstract\n\nGenerative artificial intelligence (AI) models are increasingly utilized for\nmedical applications. We tested whether such models are prone to human-like\ncognitive biases when offering medical recommendations. We explored the\nperformance of OpenAI generative pretrained transformer (GPT)-4 and Google\nGemini-1.0-Pro with clinical cases that involved 10 cognitive biases and\nsystem prompts that created synthetic clinician respondents. Medical\nrecommendations from generative AI were compared with strict axioms of\nrationality and prior results from clinicians. We found that significant\ndiscrepancies were apparent for most biases. For example, surgery was\nrecommended more frequently for lung cancer when framed in survival rather\nthan mortality statistics (framing effect: 75% vs. 12%; P<0.001). Similarly,\npulmonary embolism was more likely to be listed in the differential diagnoses\nif the opening sentence mentioned hemoptysis rather than chronic obstructive\npulmonary disease (primacy effect: 100% vs. 26%; P<0.001). In addition, the\nsame emergency department treatment was more likely to be rated as\ninappropriate if the patient subsequently died rather than recovered\n(hindsight bias: 85% vs. 0%; P<0.001). One exception was base-rate neglect\nthat showed no bias when interpreting a positive viral screening test\n(correction for false positives: 94% vs. 93%; P=0.431). The extent of these\nbiases varied minimally with the characteristics of synthetic respondents, was\ngenerally larger than observed in prior research with practicing clinicians,\nand differed between generative AI models. We suggest that generative AI\nmodels display human-like cognitive biases and that the magnitude of bias can\nbe larger than observed in practicing clinicians.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Wang and D.A. Redelmeier"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400639",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/571fe939-b7d3-458d-a92f-1d3e1a00b195/aics2400639_f3.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Generative artificial intelligence (AI) models are increasingly utilized for medical applications. We tested whether such models are prone to human-like cognitive biases when offering medical recommendations. We explored the performance of OpenAI generative pretrained transformer (GPT)-4 and Google Gemini-1.0-Pro with clinical cases that involved 10 cognitive biases and system prompts that created synthetic clinician respondents. Medical recommendations from generative AI were compared with strict axioms of rationality and prior results from clinicians. We found that significant discrepancies were apparent for most biases. For example, surgery was recommended more frequently for lung cancer when framed in survival rather than mortality statistics (framing effect: 75% vs. 12%; P&lt;0.001). Similarly, pulmonary embolism was more likely to be listed in the differential diagnoses if the opening sentence mentioned hemoptysis rather than chronic obstructive pulmonary disease (primacy effect: 100% vs. 26%; P&lt;0.001). In addition, the same emergency department treatment was more likely to be rated as inappropriate if the patient subsequently died rather than recovered (hindsight bias: 85% vs. 0%; P&lt;0.001). One exception was base-rate neglect that showed no bias when interpreting a positive viral screening test (correction for false positives: 94% vs. 93%; P=0.431). The extent of these biases varied minimally with the characteristics of synthetic respondents, was generally larger than observed in prior research with practicing clinicians, and differed between generative AI models. We suggest that generative AI models display human-like cognitive biases and that the magnitude of bias can be larger than observed in practicing clinicians.</div>"
    }
  ]
}