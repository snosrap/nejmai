{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "title": "Ethical Use of Artificial Intelligence in Medical Diagnostics Demands a Focus on Accuracy, Not Fairness",
      "summary": "We argue that in developing and evaluating AI tools for medical diagnostics, rather than focusing on fairness criteria that quantify disparities between protected subpopulations, our first and most important objective should be to track and maximize diagnostic accuracy for all subpopulations.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) promises to be a transformative technology for\nmedicine and health care. As such, there is an increasing interest in ensuring\nits ethical use. In this perspective, we consider the employment of AI for\nmedical diagnostics, where the goal is the detection and classification of an\nunderlying pathology, based on data such as patient information, clinical\npresentation, tests, and imaging. We argue that instead of prioritizing\nfairness criteria that measure disparities between protected groups, the\nprimary goal should be to assess and enhance diagnostic accuracy within each\nsubpopulation. This approach shifts the focus from optimizing overall\npopulation accuracy to ensuring maximal accuracy in each subpopulation. Our\nperspective implies that we should be using all available information,\nincluding protected group identity, in our methods. We decouple the goal of\naccurate diagnosis from fairness considerations in screening and postdiagnosis\nclinical decisions, which often require the allocation of finite resources.\nFurthermore, we underscore the importance of collecting high-quality and\nrepresentative datasets for each subpopulation, including ensuring that the\nground truth labels we train and with which we evaluate are unbiased.\n\n",
      "date_published": "2024-12-13T00:00:00+00:00",
      "authors": [
        {
          "name": "M.R. Sabuncu, A.Q. Wang, and M. Nguyen"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400672",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) promises to be a transformative technology for medicine and health care. As such, there is an increasing interest in ensuring its ethical use. In this perspective, we consider the employment of AI for medical diagnostics, where the goal is the detection and classification of an underlying pathology, based on data such as patient information, clinical presentation, tests, and imaging. We argue that instead of prioritizing fairness criteria that measure disparities between protected groups, the primary goal should be to assess and enhance diagnostic accuracy within each subpopulation. This approach shifts the focus from optimizing overall population accuracy to ensuring maximal accuracy in each subpopulation. Our perspective implies that we should be using all available information, including protected group identity, in our methods. We decouple the goal of accurate diagnosis from fairness considerations in screening and postdiagnosis clinical decisions, which often require the allocation of finite resources. Furthermore, we underscore the importance of collecting high-quality and representative datasets for each subpopulation, including ensuring that the ground truth labels we train and with which we evaluate are unbiased.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "title": "Autonomous LLM-Driven Research \u2014 from Data to Human-Verifiable Research Papers",
      "summary": "Our study demonstrates a potential for AI-driven acceleration of scientific discovery in biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.",
      "content_text": "## Abstract\n\n### Background\n\nArtificial intelligence (AI) promises to accelerate scientific discovery, but\nit remains unclear whether AI systems can perform fully autonomous research,\nand whether they can do so while adhering to key scientific values, such as\ntransparency, traceability, and verifiability. The aim of this study was to\ndevelop and evaluate an AI-automation platform that performs transparent,\ntraceable, and human-verifiable scientific research.\n\n### Methods\n\nTo mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation\nplatform that guides interacting large language model (LLM) agents through a\ncomplete stepwise research process that starts with annotated data and results\nin comprehensive research papers, while programmatically backtracing\ninformation flow and allowing human oversight and interactions. The platform\ncan run fully autonomously (in autopilot mode) or with human intervention (in\ncopilot mode).\n\n### Results\n\nIn autopilot mode, provided only with annotated data, data-to-paper raised\nhypotheses; designed research plans; wrote and debugged analysis codes;\ngenerated and interpreted results; and created complete, information-traceable\nresearch papers. Even though the research novelty of manuscripts created by\ndata-to-paper was relatively limited, the process demonstrated the autonomous\ngeneration of de novo quantitative insights from data, such as unraveling\nassociations between health indicators and clinical outcomes. For simple\nresearch goals and datasets, a fully autonomous cycle can create manuscripts\nthat independently recapitulate the findings of peer-reviewed biomedical\npublications without major errors in about 80 to 90% of cases. Yet, as goal or\ndata complexity increases, human copiloting becomes critical for ensuring\naccuracy and overall quality. By tracking information flow through the steps,\nthe platform creates \u201cdata-chained\u201d manuscripts, in which downstream results\nare programmatically linked to upstream code and data, thus setting a new\nstandard for the verifiability of scientific outputs.\n\n### Conclusions\n\nOur work demonstrates the potential for AI-driven acceleration of scientific\ndiscovery in data-driven biomedical research and beyond, while enhancing,\nrather than jeopardizing, traceability, transparency, and verifiability.\n\n",
      "date_published": "2024-12-03T00:00:00+00:00",
      "authors": [
        {
          "name": "T. Ifargan and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400555",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/590c8332-e34e-4d7d-860f-17fb5d45e5cf/aioa2400555_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Artificial intelligence (AI) promises to accelerate scientific discovery, but it remains unclear whether AI systems can perform fully autonomous research, and whether they can do so while adhering to key scientific values, such as transparency, traceability, and verifiability. The aim of this study was to develop and evaluate an AI-automation platform that performs transparent, traceable, and human-verifiable scientific research.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">To mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation platform that guides interacting large language model (LLM) agents through a complete stepwise research process that starts with annotated data and results in comprehensive research papers, while programmatically backtracing information flow and allowing human oversight and interactions. The platform can run fully autonomously (in autopilot mode) or with human intervention (in copilot mode).</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">In autopilot mode, provided only with annotated data, data-to-paper raised hypotheses; designed research plans; wrote and debugged analysis codes; generated and interpreted results; and created complete, information-traceable research papers. Even though the research novelty of manuscripts created by data-to-paper was relatively limited, the process demonstrated the autonomous generation of de novo quantitative insights from data, such as unraveling associations between health indicators and clinical outcomes. For simple research goals and datasets, a fully autonomous cycle can create manuscripts that independently recapitulate the findings of peer-reviewed biomedical publications without major errors in about 80 to 90% of cases. Yet, as goal or data complexity increases, human copiloting becomes critical for ensuring accuracy and overall quality. By tracking information flow through the steps, the platform creates \u201cdata-chained\u201d manuscripts, in which downstream results are programmatically linked to upstream code and data, thus setting a new standard for the verifiability of scientific outputs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Our work demonstrates the potential for AI-driven acceleration of scientific discovery in data-driven biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401000",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401000",
      "title": "Reclaiming Voice with AI",
      "summary": "While AI is often dual use and concerns about voice cloning often center on potential misuse, we argue that suppressing this technology may inflict tangible harm on patients by denying them the chance to reclaim their voice.",
      "content_text": "## Abstract\n\nVoice impairments affect millions of Americans, with personalized text-to-\nspeech technology offering limited solutions due to the need for extensive\nvoice banking. Here, we report the case of Alexis Bogan, a 20-year-old patient\nwho acutely lost her voice after surgery to resect her brain stem\nhemangioblastoma. In a world-first application, OpenAI\u2019s Voice Engine was used\nto clone Ms. Bogan\u2019s voice from just 15 seconds of preexisting audio, sourced\nfrom a school project she had filmed a few years prior. This enabled her to\nuse a personalized text-to-speech app for daily communication while\nrehabilitating her speech. This case was highlighted on a recent episode of\nthe _NEJM AI_ Grand Rounds podcast,1 framing a broader discussion on voice\ncloning technology. While AI is often dual use and concerns about voice\ncloning often center on potential misuse, such as \u201cdeepfakes\u201d and\nmisinformation, we argue that suppressing this technology may inflict tangible\nharm on patients by denying them the chance to reclaim their voice. Inspired\nby Ms. Bogan\u2019s journey, we urge researchers, clinicians, ethicists,\npolicymakers, and tech companies to collaborate swiftly yet responsibly in\nadvancing AI voice cloning in health care. By doing so, we can empower\npatients to recover not just their voice, but also a fundamental aspect of\ntheir identity and quality of life.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "F.N. Mirza and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401000",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIp2401000_Ali-1732707614033.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Voice impairments affect millions of Americans, with personalized text-to-speech technology offering limited solutions due to the need for extensive voice banking. Here, we report the case of Alexis Bogan, a 20-year-old patient who acutely lost her voice after surgery to resect her brain stem hemangioblastoma. In a world-first application, OpenAI\u2019s Voice Engine was used to clone Ms. Bogan\u2019s voice from just 15 seconds of preexisting audio, sourced from a school project she had filmed a few years prior. This enabled her to use a personalized text-to-speech app for daily communication while rehabilitating her speech. This case was highlighted on a recent episode of the <i>NEJM AI</i> Grand Rounds podcast,<sup>1</sup> framing a broader discussion on voice cloning technology. While AI is often dual use and concerns about voice cloning often center on potential misuse, such as \u201cdeepfakes\u201d and misinformation, we argue that suppressing this technology may inflict tangible harm on patients by denying them the chance to reclaim their voice. Inspired by Ms. Bogan\u2019s journey, we urge researchers, clinicians, ethicists, policymakers, and tech companies to collaborate swiftly yet responsibly in advancing AI voice cloning in health care. By doing so, we can empower patients to recover not just their voice, but also a fundamental aspect of their identity and quality of life.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400867",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400867",
      "title": "FDA-Authorized AI/ML Tool for Sepsis Prediction: Development and Validation",
      "summary": "Sepsis is a life-threatening acute condition that requires accurate and rapid identification to guide proper treatment. This study outlines the development and validation of the first U.S. Food and Drug Administration\u2013authorized artificial intelligence\u2013based software to identify patients at risk of having sepsis.",
      "content_text": "## Abstract\n\n### Background\n\nSepsis is a life-threatening condition that demands prompt treatment for\nimproved patient outcomes. Its heterogenous presentation makes early detection\nchallenging, highlighting the need for effective risk assessment tools.\nArtificial intelligence (AI) models could potentially identify patients with\nsepsis, but none have previously been authorized by the U.S. Food and Drug\nAdministration (FDA) for commercial use. This study outlines the development\nand validation of the Sepsis ImmunoScore, the first FDA-authorized AI-based\nsoftware designed to identify patients at risk of sepsis.\n\n### Methods\n\nIn this prospective study, we enrolled adult patients (18+ years of age)\nsuspected of infection, as indicated by a blood culture order, from five U.S.\ninstitutions between April 2017 and July 2022. The participants were divided\ninto an algorithm development cohort (n=2366), an internal validation cohort\n(n=393), and an external validation cohort (n=698). The primary end point was\nsepsis presence (as defined by Sepsis-3) within 24 hours of test initiation.\nSecondary end points included length of hospital stay, intensive care unit\n(ICU) admission within 24 hours, mechanical ventilation use within 24 hours,\nvasopressor use within 24 hours, and in-hospital mortality.\n\n### Results\n\nThe Sepsis ImmunoScore demonstrated high diagnostic accuracy, with an area\nunder the curve of 0.85 (0.83 to 0.87) in the derivation cohort, 0.80 (0.74 to\n0.86) in internal validation, and 0.81 (0.77 to 0.86) in external validation.\nThe scores were categorized into four sepsis risk levels with corresponding\nlikelihood ratios: low (0.1), medium (0.5), high (2.1), and very high (8.3).\nThese risk categories also predicted in-hospital mortality: low (0.0%), medium\n(1.9%), high (8.7%), and very high (18.2%) in the external validation cohort.\nSimilar trends were observed for other metrics, such as length of hospital\nstay, ICU utilization, mechanical ventilation, and vasopressor use.\n\n### Conclusions\n\nThe Sepsis ImmunoScore demonstrated high accuracy for the identification and\nprediction of sepsis and critical illness metrics that could enable prompt\nidentification of patients at high risk of sepsis and adverse outcomes,\npotentially improving clinical decision-making and patient outcomes. (Funded\nby the Defense Threat Reduction Agency and others.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "A. Bhargava and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400867",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/23154c4d-4969-464d-962f-389b497979ee/aioa2400867_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Sepsis is a life-threatening condition that demands prompt treatment for improved patient outcomes. Its heterogenous presentation makes early detection challenging, highlighting the need for effective risk assessment tools. Artificial intelligence (AI) models could potentially identify patients with sepsis, but none have previously been authorized by the U.S. Food and Drug Administration (FDA) for commercial use. This study outlines the development and validation of the Sepsis ImmunoScore, the first FDA-authorized AI-based software designed to identify patients at risk of sepsis.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">In this prospective study, we enrolled adult patients (18+ years of age) suspected of infection, as indicated by a blood culture order, from five U.S. institutions between April 2017 and July 2022. The participants were divided into an algorithm development cohort (n=2366), an internal validation cohort (n=393), and an external validation cohort (n=698). The primary end point was sepsis presence (as defined by Sepsis-3) within 24 hours of test initiation. Secondary end points included length of hospital stay, intensive care unit (ICU) admission within 24 hours, mechanical ventilation use within 24 hours, vasopressor use within 24 hours, and in-hospital mortality.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">The Sepsis ImmunoScore demonstrated high diagnostic accuracy, with an area under the curve of 0.85 (0.83 to 0.87) in the derivation cohort, 0.80 (0.74 to 0.86) in internal validation, and 0.81 (0.77 to 0.86) in external validation. The scores were categorized into four sepsis risk levels with corresponding likelihood ratios: low (0.1), medium (0.5), high (2.1), and very high (8.3). These risk categories also predicted in-hospital mortality: low (0.0%), medium (1.9%), high (8.7%), and very high (18.2%) in the external validation cohort. Similar trends were observed for other metrics, such as length of hospital stay, ICU utilization, mechanical ventilation, and vasopressor use.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">The Sepsis ImmunoScore demonstrated high accuracy for the identification and prediction of sepsis and critical illness metrics that could enable prompt identification of patients at high risk of sepsis and adverse outcomes, potentially improving clinical decision-making and patient outcomes. (Funded by the Defense Threat Reduction Agency and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2300221",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2300221",
      "title": "Development and Validation of a Multimodal Multitask Vision\n                    Foundation Model for Generalist Ophthalmic Artificial\n                    Intelligence",
      "summary": "A proposal for an AI foundation model for ophthalmology that can process eight ophthalmic imaging modalities and adapt to a multitude of ophthalmic scenarios and applications.",
      "content_text": "## Abstract\n\n### Background\n\nSpecialized single-use, single-modality models often have limited or no\ngeneralization to new diseases, modalities, and clinical tasks. Foundation\nmodels are built for multipurpose use, enabling them to perform tasks even\nwhen not specifically pretrained for them, and to adapt to different clinical\napplications.\n\n### Methods\n\nWe present VisionFM, an artificial intelligence foundation model for\nophthalmology pretrained on 3.4 million images from over 500,000 individuals,\ncovering diverse ophthalmic diseases, imaging modalities and devices, and\nclinical scenarios. Pretrained based on eight modalities, VisionFM was tested\nfor multiple applications, including disease screening and detection,\nprognosis and prediction, and segmentation of lesions and anatomical\nstructures, on an ophthalmic database comprising 53 public and 12 private\ndatasets. We compared the model against ophthalmologists with varying\nexperience in ophthalmic and systemic disease diagnoses.\n\n### Results\n\nVisionFM outperformed baseline deep learning approaches in diagnosing ocular\ndiseases, achieving an average area under the receiver operating\ncharacteristic curve (AUROC) of 0.950 (95% confidence interval [CI], 0.941 to\n0.959) across eight disease categories and five imaging modalities in internal\nvalidation. In external validation, VisionFM achieved an AUROC of 0.945 (95%\nCI, 0.934 to 0.956) in fundus-based diabetic retinopathy recognition, and an\nAUROC of 0.974 (95% CI, 0.966 to 0.983) in optical coherence tomography\u2013based\nage-related macular degeneration recognition. In a comparative study of\ndiagnostic accuracy of 12 ocular diseases from fundus photographs, VisionFM\nshows diagnostic accuracy close to that of intermediate-level\nophthalmologists. Its generalizability extends to new imaging modalities and\ndevices, effectively handling dataset shifts. For example, VisionFM accurately\ngraded diabetic retinopathy with an AUROC of 0.935 (95% CI, 0.902 to 0.964)\nusing an imaging modality it was never exposed to during pretraining.\nFurthermore, VisionFM is able to predict both glaucoma progression and the\npresence of intracranial tumors directly from fundus photographs.\n\n### Conclusions\n\nVisionFM provides an efficient platform for diagnosis or prediction of\nmultiple diseases using multiple imaging modalities and is scalable to\nincorporate additional data, modalities, and applications via its open-sourced\nmodel weights and codebase. (Funded by the Research Grants Council (RGC) of\nHong Kong SAR and others.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Qiu and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2300221",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/4171a5db-da90-45e7-9ec3-4dd11dc5f849/aioa2300221_f3.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Specialized single-use, single-modality models often have limited or no generalization to new diseases, modalities, and clinical tasks. Foundation models are built for multipurpose use, enabling them to perform tasks even when not specifically pretrained for them, and to adapt to different clinical applications.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We present VisionFM, an artificial intelligence foundation model for ophthalmology pretrained on 3.4 million images from over 500,000 individuals, covering diverse ophthalmic diseases, imaging modalities and devices, and clinical scenarios. Pretrained based on eight modalities, VisionFM was tested for multiple applications, including disease screening and detection, prognosis and prediction, and segmentation of lesions and anatomical structures, on an ophthalmic database comprising 53 public and 12 private datasets. We compared the model against ophthalmologists with varying experience in ophthalmic and systemic disease diagnoses.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">VisionFM outperformed baseline deep learning approaches in diagnosing ocular diseases, achieving an average area under the receiver operating characteristic curve (AUROC) of 0.950 (95% confidence interval [CI], 0.941 to 0.959) across eight disease categories and five imaging modalities in internal validation. In external validation, VisionFM achieved an AUROC of 0.945 (95% CI, 0.934 to 0.956) in fundus-based diabetic retinopathy recognition, and an AUROC of 0.974 (95% CI, 0.966 to 0.983) in optical coherence tomography\u2013based age-related macular degeneration recognition. In a comparative study of diagnostic accuracy of 12 ocular diseases from fundus photographs, VisionFM shows diagnostic accuracy close to that of intermediate-level ophthalmologists. Its generalizability extends to new imaging modalities and devices, effectively handling dataset shifts. For example, VisionFM accurately graded diabetic retinopathy with an AUROC of 0.935 (95% CI, 0.902 to 0.964) using an imaging modality it was never exposed to during pretraining. Furthermore, VisionFM is able to predict both glaucoma progression and the presence of intracranial tumors directly from fundus photographs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">VisionFM provides an efficient platform for diagnosis or prediction of multiple diseases using multiple imaging modalities and is scalable to incorporate additional data, modalities, and applications via its open-sourced model weights and codebase. (Funded by the Research Grants Council (RGC) of Hong Kong SAR and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401024",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401024",
      "title": "A New Foundation Model for Multimodal Ophthalmic Images: Advancing Disease Detection and Prediction",
      "summary": "A new foundation model for ophthalmic images demonstrates important progress, particularly through its flexible approach to multimodal training and its application to image segmentation tasks.",
      "content_text": "## Abstract\n\nFoundation models are a powerful tool in ophthalmology for building\ngeneralizable systems that can be efficiently applied to a range of ocular and\nsystemic health tasks. A new foundation model for ophthalmic images\ndemonstrates important progress, particularly through its flexible approach to\nmultimodal training and its application to image segmentation tasks.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "M.A. Chia, Y. Zhou, and P.A. Keane"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401024",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Foundation models are a powerful tool in ophthalmology for building generalizable systems that can be efficiently applied to a range of ocular and systemic health tasks. A new foundation model for ophthalmic images demonstrates important progress, particularly through its flexible approach to multimodal training and its application to image segmentation tasks.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2400961",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2400961",
      "title": "Cognitive Bias in Large Language Models: Implications for Research and Practice",
      "summary": "This commentary discusses the presence of cognitive biases in the outputs of large language models and the implications for clinical decision support and human\u2013computer interaction.",
      "content_text": "## Abstract\n\nThe use of large language models (LLMs) such as ChatGPT in clinical settings\nis growing, but concerns about their susceptibility to cognitive biases\npersist. Wang and Redelmeier\u2019s study reveals that LLMs are prone to biases,\nraising important questions about their role in medical decision-making. To\nprevent errors in decision-making with LLMs, it is recommended that clinicians\naim to critically engage with LLMs (e.g. refuting their hypotheses rather than\nlooking for confirmation) researchers should focus on identifying and\nevaluating collaborative strategies between AI and human decision-making.\nFurthermore, research on context-specific implementation is important. We need\nto ensure that AI complements, rather than replicates, human cognitive\nprocesses. (Funded by the Netherlands Organisation for Health Research and\nDevelopment.)\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "L. Zwaan"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2400961",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">The use of large language models (LLMs) such as ChatGPT in clinical settings is growing, but concerns about their susceptibility to cognitive biases persist. Wang and Redelmeier\u2019s study reveals that LLMs are prone to biases, raising important questions about their role in medical decision-making. To prevent errors in decision-making with LLMs, it is recommended that clinicians aim to critically engage with LLMs (e.g. refuting their hypotheses rather than looking for confirmation) researchers should focus on identifying and evaluating collaborative strategies between AI and human decision-making. Furthermore, research on context-specific implementation is important. We need to ensure that AI complements, rather than replicates, human cognitive processes. (Funded by the Netherlands Organisation for Health Research and Development.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400639",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400639",
      "title": "Cognitive Biases and Artificial Intelligence",
      "summary": "Artificial intelligence models sometimes propagate racial and gender bias. This study shows that human-like cognitive biases are prevalent in generative artificial intelligence and sometimes much more so than among practicing clinicians.",
      "content_text": "## Abstract\n\nGenerative artificial intelligence (AI) models are increasingly utilized for\nmedical applications. We tested whether such models are prone to human-like\ncognitive biases when offering medical recommendations. We explored the\nperformance of OpenAI generative pretrained transformer (GPT)-4 and Google\nGemini-1.0-Pro with clinical cases that involved 10 cognitive biases and\nsystem prompts that created synthetic clinician respondents. Medical\nrecommendations from generative AI were compared with strict axioms of\nrationality and prior results from clinicians. We found that significant\ndiscrepancies were apparent for most biases. For example, surgery was\nrecommended more frequently for lung cancer when framed in survival rather\nthan mortality statistics (framing effect: 75% vs. 12%; P<0.001). Similarly,\npulmonary embolism was more likely to be listed in the differential diagnoses\nif the opening sentence mentioned hemoptysis rather than chronic obstructive\npulmonary disease (primacy effect: 100% vs. 26%; P<0.001). In addition, the\nsame emergency department treatment was more likely to be rated as\ninappropriate if the patient subsequently died rather than recovered\n(hindsight bias: 85% vs. 0%; P<0.001). One exception was base-rate neglect\nthat showed no bias when interpreting a positive viral screening test\n(correction for false positives: 94% vs. 93%; P=0.431). The extent of these\nbiases varied minimally with the characteristics of synthetic respondents, was\ngenerally larger than observed in prior research with practicing clinicians,\nand differed between generative AI models. We suggest that generative AI\nmodels display human-like cognitive biases and that the magnitude of bias can\nbe larger than observed in practicing clinicians.\n\n",
      "date_published": "2024-11-27T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Wang and D.A. Redelmeier"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400639",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/571fe939-b7d3-458d-a92f-1d3e1a00b195/aics2400639_f3.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Generative artificial intelligence (AI) models are increasingly utilized for medical applications. We tested whether such models are prone to human-like cognitive biases when offering medical recommendations. We explored the performance of OpenAI generative pretrained transformer (GPT)-4 and Google Gemini-1.0-Pro with clinical cases that involved 10 cognitive biases and system prompts that created synthetic clinician respondents. Medical recommendations from generative AI were compared with strict axioms of rationality and prior results from clinicians. We found that significant discrepancies were apparent for most biases. For example, surgery was recommended more frequently for lung cancer when framed in survival rather than mortality statistics (framing effect: 75% vs. 12%; P&lt;0.001). Similarly, pulmonary embolism was more likely to be listed in the differential diagnoses if the opening sentence mentioned hemoptysis rather than chronic obstructive pulmonary disease (primacy effect: 100% vs. 26%; P&lt;0.001). In addition, the same emergency department treatment was more likely to be rated as inappropriate if the patient subsequently died rather than recovered (hindsight bias: 85% vs. 0%; P&lt;0.001). One exception was base-rate neglect that showed no bias when interpreting a positive viral screening test (correction for false positives: 94% vs. 93%; P=0.431). The extent of these biases varied minimally with the characteristics of synthetic respondents, was generally larger than observed in prior research with practicing clinicians, and differed between generative AI models. We suggest that generative AI models display human-like cognitive biases and that the magnitude of bias can be larger than observed in practicing clinicians.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400631",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400631",
      "title": "Rubrics to Prompts: Assessing Medical Student Post-Encounter Notes with AI",
      "summary": "UT Southwestern Medical Center piloted the prospective deployment of an AI-based automated grading system for medical student post-encounter learner notes during administration of the fall 2023 Objective Structured Clinical Examinations. Using established rubrics, zero-shot GPT-4 with minimal prompt engineering was employed as a first-pass grader. GPT-4 achieved 90% agreement with expert human assessors while dramatically accelerating turnaround time and reducing manual grading effort.",
      "content_text": "## Abstract\n\nThis case study, conducted at UT Southwestern Medical Center\u2019s Simulation\nCenter, describes the first successful prospective deployment of a generative\nartificial intelligence (AI)\u2013based automated grading system for medical\nstudent post-encounter Objective Structured Clinical Examination (OSCE) notes.\nThe OSCE is a standard approach to measuring the competence of medical\nstudents by their participation in live-action, simulated patient encounters\nwith human actors. The post-encounter learner note is a vital element of the\nOSCE, and accurate assessment of student performance requires specially\ntrained manual evaluators, which imposes significant labor and time\ninvestments. The Simulation Center at UT Southwestern provides a compelling\nplatform for observing the benefits and challenges of AI-based enhancements in\nmedical education at scale. To that end, we prospectively activated a first-\npass AI grading system at the center for 245 (preclerkship) medical students\nparticipating in a 10-station fall 2023 OSCE session. Our inaugural deployment\nof the AI notes grading system reduced human effort by an estimated 91% (as\nmeasured by gradable items) and dramatically reduced turnaround time (from\nweeks to days). Conceived as a zero-shot large language model architecture\nwith minimal prompt engineering, the system requires no prior domain-specific\ntraining data and can be readily adapted for new evaluation rubrics, opening\nthe door to scaling this approach to other institutions. Confidence in our\nzero-shot Generative Pretrained Transformer 4 (GPT-4) framework was\nestablished by pre-deployment of retrospective evaluations. With the OSCE in\nprior years, the system achieved up to 89.7% agreement with human expert\ngraders at the rubric item level (Cohen\u2019s kappa, 0.79) and a Spearman\u2019s\ncorrelation of 0.86 with the total examination score. We also demonstrate that\nlocal, smaller, open-source models (such as Llama-2-7B) can be fine-tuned via\nknowledge distillation from frontier models like GPT-4 to achieve similar\nperformance, thereby indicating important operational implications for\nscalability, data privacy, security, and model control. These achievements\nwere the result of a strategic, multiyear effort to pivot toward AI that was\nbegun prior to ChatGPT\u2019s release. In addition to highlighting the model\u2019s\nperformance and capabilities (including a retrospective analysis of 1124\nstudents, 10,175 post-encounter notes, and 156,978 scored items), we share\nobservations on the development and sign-off prior to the launch of an AI\ndeployment protocol for our program. (Funded by UT Southwestern institutional\nfunds and others.)\n\n",
      "date_published": "2024-11-25T00:00:00+00:00",
      "authors": [
        {
          "name": "A.R. Jamieson and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400631",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/4c3a63dc-ba9b-49b0-9082-dcc67c8bb2bb/aics2400631_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">This case study, conducted at UT Southwestern Medical Center\u2019s Simulation Center, describes the first successful prospective deployment of a generative artificial intelligence (AI)\u2013based automated grading system for medical student post-encounter Objective Structured Clinical Examination (OSCE) notes. The OSCE is a standard approach to measuring the competence of medical students by their participation in live-action, simulated patient encounters with human actors. The post-encounter learner note is a vital element of the OSCE, and accurate assessment of student performance requires specially trained manual evaluators, which imposes significant labor and time investments. The Simulation Center at UT Southwestern provides a compelling platform for observing the benefits and challenges of AI-based enhancements in medical education at scale. To that end, we prospectively activated a first-pass AI grading system at the center for 245 (preclerkship) medical students participating in a 10-station fall 2023 OSCE session. Our inaugural deployment of the AI notes grading system reduced human effort by an estimated 91% (as measured by gradable items) and dramatically reduced turnaround time (from weeks to days). Conceived as a zero-shot large language model architecture with minimal prompt engineering, the system requires no prior domain-specific training data and can be readily adapted for new evaluation rubrics, opening the door to scaling this approach to other institutions. Confidence in our zero-shot Generative Pretrained Transformer 4 (GPT-4) framework was established by pre-deployment of retrospective evaluations. With the OSCE in prior years, the system achieved up to 89.7% agreement with human expert graders at the rubric item level (Cohen\u2019s kappa, 0.79) and a Spearman\u2019s correlation of 0.86 with the total examination score. We also demonstrate that local, smaller, open-source models (such as Llama-2-7B) can be fine-tuned via knowledge distillation from frontier models like GPT-4 to achieve similar performance, thereby indicating important operational implications for scalability, data privacy, security, and model control. These achievements were the result of a strategic, multiyear effort to pivot toward AI that was begun prior to ChatGPT\u2019s release. In addition to highlighting the model\u2019s performance and capabilities (including a retrospective analysis of 1124 students, 10,175 post-encounter notes, and 156,978 scored items), we share observations on the development and sign-off prior to the launch of an AI deployment protocol for our program. (Funded by UT Southwestern institutional funds and others.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400727",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400727",
      "title": "Artificial Intelligence\u2013Based Copilots to Generate Causal Evidence",
      "summary": "A structured approach for leveraging advances in generative artificial intelligence to improve the quality of causal analyses using real-world health data.",
      "content_text": "## Abstract\n\nWhile there is growing consensus that real-world data should play a larger\nrole in generating causal evidence for health care, it is less clear whether\nand how AI can help. Current approaches to AI-driven analysis of health data\nare ill-equipped to account for the many threats to causal validity. However,\nthe current human-reliant pipeline for causal analysis also falls short:\nanalyses are complex, require multidisciplinary expertise, and are slow,\nlabor-intensive and error-prone. Here, we speculate how a \u201chuman-in-the-loop\u201d\nAI-based system could help relieve bottlenecks to high-quality causal\nanalyses. We describe how an AI-based causal copilot, leveraging the formal\ninferential structure of the causal road map, could guide and support\nresearchers through a structured process of translating a causal question into\na hypothetical experiment; translating contextual knowledge into transparent\nand well-justified assumptions; designing, testing, and benchmarking a\ncorresponding statistical analysis plan and code (including integration of\nmachine learning on multimodal data); and supporting causal interpretation of\nresults. Such a system could augment the speed and quality with which\nresearchers conduct causal analyses with real-world data, improve transparency\nand verification of analyses and assumptions, and ultimately serve as a basis\nfor point-of-care personalized decision support.\n\n",
      "date_published": "2024-11-22T00:00:00+00:00",
      "authors": [
        {
          "name": "M. Petersen and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400727",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/0c3dc115-46f7-4b46-bbec-8393aff38ebf/aip2400727_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">While there is growing consensus that real-world data should play a larger role in generating causal evidence for health care, it is less clear whether and how AI can help. Current approaches to AI-driven analysis of health data are ill-equipped to account for the many threats to causal validity. However, the current human-reliant pipeline for causal analysis also falls short: analyses are complex, require multidisciplinary expertise, and are slow, labor-intensive and error-prone. Here, we speculate how a \u201chuman-in-the-loop\u201d AI-based system could help relieve bottlenecks to high-quality causal analyses. We describe how an AI-based causal copilot, leveraging the formal inferential structure of the causal road map, could guide and support researchers through a structured process of translating a causal question into a hypothetical experiment; translating contextual knowledge into transparent and well-justified assumptions; designing, testing, and benchmarking a corresponding statistical analysis plan and code (including integration of machine learning on multimodal data); and supporting causal interpretation of results. Such a system could augment the speed and quality with which researchers conduct causal analyses with real-world data, improve transparency and verification of analyses and assumptions, and ultimately serve as a basis for point-of-care personalized decision support.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400659",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400659",
      "title": "Does AI-Powered Clinical Documentation Enhance Clinician Efficiency? A Longitudinal Study",
      "summary": "Using DAX Copilot did not make clinicians more efficient in EHR use or financial impact, except for time spent in documentation for high DAX users.",
      "content_text": "## Abstract\n\n### Background\n\nNuance\u2019s Dragon Ambient eXperience (DAX) Copilot is an artificial intelligence\n(AI)\u2013driven ambient clinical documentation software platform. Atrium Health, a\nlarge multisite academic learning health system, was the first to use DAX\nCopilot. This study evaluates outcomes for participating clinicians after DAX\nimplementation.\n\n### Methods\n\nIn this longitudinal study, 112 primary care clinicians using DAX were\nrecruited between June and August 2023 along with a control group of 103\nclinicians from similar practices not using DAX. Primary outcomes of\nelectronic health record (EHR) use and financial impact were assessed over 180\ndays using linear mixed models. Within the DAX group were two subgroups:\nactive users (who transferred \u226525% of DAX notes) and high users (who\ntransferred \u226560% of DAX notes). We performed exploratory analyses to compare\nthe control group with DAX subgroups, in addition to subgroup analyses\nstratified by patient volume and clinician specialty.\n\n### Results\n\nAfter controlling for length of intervention, age, gender, provider type,\nyears of practice, and baseline outcome, we did not find statistical\nsignificance in the primary analyses of EHR and financial metrics. Exploratory\nanalyses suggested that small decreases in documentation hours could result\nfrom high DAX usage (means ratio [MR] 0.93, 95% confidence interval [CI] 0.88\nto 0.98) and from implementing DAX with low-volume clinicians (MR 0.91, 95% CI\n0.83 to 0.99) and with family medicine clinicians (MR 0.91, 95% CI 0.85 to\n0.98).\n\n### Conclusions\n\nAI-powered ambient clinical documentation software has been promoted as a\npromising strategy to alleviate the documentation burden faced by outpatient\nclinicians. However, our findings suggest that the tool did not make\nclinicians as a group more efficient. Future studies can further investigate\nthe utility of DAX for clinician subgroups and alternative implementations\nwith improved clinical adoption. (Funded by Wake Forest University Health\nSciences; ClinicalTrials.gov number,\n[NCT06329427](http://clinicaltrials.gov/show/NCT06329427).)\n\n",
      "date_published": "2024-11-22T00:00:00+00:00",
      "authors": [
        {
          "name": "T.-L. Liu and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400659",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/1204c663-315b-4597-b1c5-14e1c31e2cb4/aioa2400659_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Nuance\u2019s Dragon Ambient eXperience (DAX) Copilot is an artificial intelligence (AI)\u2013driven ambient clinical documentation software platform. Atrium Health, a large multisite academic learning health system, was the first to use DAX Copilot. This study evaluates outcomes for participating clinicians after DAX implementation.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">In this longitudinal study, 112 primary care clinicians using DAX were recruited between June and August 2023 along with a control group of 103 clinicians from similar practices not using DAX. Primary outcomes of electronic health record (EHR) use and financial impact were assessed over 180 days using linear mixed models. Within the DAX group were two subgroups: active users (who transferred \u226525% of DAX notes) and high users (who transferred \u226560% of DAX notes). We performed exploratory analyses to compare the control group with DAX subgroups, in addition to subgroup analyses stratified by patient volume and clinician specialty.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">After controlling for length of intervention, age, gender, provider type, years of practice, and baseline outcome, we did not find statistical significance in the primary analyses of EHR and financial metrics. Exploratory analyses suggested that small decreases in documentation hours could result from high DAX usage (means ratio [MR] 0.93, 95% confidence interval [CI] 0.88 to 0.98) and from implementing DAX with low-volume clinicians (MR 0.91, 95% CI 0.83 to 0.99) and with family medicine clinicians (MR 0.91, 95% CI 0.85 to 0.98).</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">AI-powered ambient clinical documentation software has been promoted as a promising strategy to alleviate the documentation burden faced by outpatient clinicians. However, our findings suggest that the tool did not make clinicians as a group more efficient. Future studies can further investigate the utility of DAX for clinician subgroups and alternative implementations with improved clinical adoption. (Funded by Wake Forest University Health Sciences; ClinicalTrials.gov number, <a href=\"http://clinicaltrials.gov/show/NCT06329427\" target=\"_blank\">NCT06329427</a>.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400404",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400404",
      "title": "National Use of Artificial Intelligence for Eye Screening in Singapore",
      "summary": "Drawing on the experience of the first-ever artificial intelligence software-as-a-medical-device solution successfully approved by regulators and implemented at a country-wide health care system level, this case study discusses a sequential roadmap of lessons learnt in overcoming practical challenges to facilitate the planning of the successful development, adoption, and diffusion of medical AI solutions in health care settings.",
      "content_text": "## Abstract\n\nDiabetes is a major health care challenge, affecting 10% of the global\npopulation. One third of patients with diabetes have an ocular complication\nknown as diabetic retinopathy (DR). DR progression to manifestations such as\nvision-threatening diabetic retinopathy (VTDR) remains the leading cause of\nblindness in working-aged adults. Yearly DR screening is a universally\nrecommended practice in primary care settings for patients with diabetes, but\nit is often difficult to implement due to a lack of staffing and screening\ncapacity in primary care. This case study highlights our experience with\ndeveloping a medical artificial intelligence (AI) software-as-a-medical-device\n(SaMD) solution for DR screening and implementing it at a national level to\nprovide the capacity needed for DR screening in Singapore. Our approach\ninvolved two broad phases. First, we established a national telemedicine\nscreening program, Singapore Integrated Diabetic Retinopathy Program (SiDRP),\nfor population screening of DR in primary care run by trained, nonclinician\nhuman graders. Second, we deployed a deep learning\u2013based AI solution,\nSingapore Eye Lesion Analyzer (SELENA+), into the SiDRP to scale-up the DR\nscreening process by the human graders. We demonstrated the cost-effectiveness\nof this solution, and obtained medical device regulatory approval for clinical\nuse in health care settings. We report the prospective evaluation of SELENA+\nin SiDRP using real-world pilot data from the first 1712 patients\nconsecutively recruited. Sensitivity and specificity of SELENA+ in detection\nof referable DR cases were 94.7% (95% confidence interval [CI] 88.0% to 98.3%)\nand 82.2% (95% CI 80.8% to 83.5%), respectively. In comparison, sensitivity\nand specificity of human graders were 98.9% (95% CI 94.0% to 99.9%) and 97.2%\n(95% CI 96.6\u201397.8%), respectively. For patients with VTDR, SELENA+\ndemonstrated a substantial advantage of higher sensitivity compared with human\nperformance, reflecting the benefit of the fine-tuning of SELENA+ that we\nperformed to enhance the AI solution\u2019s ability to detect VTDR. We outline the\nclinical, technical, operational, regulatory, and governance challenges\nencountered as well as the lessons learnt in this AI algorithm implementation\njourney. We also present a conceptual framework with considerations and\nstrategies for the broader adoption of medical AI SaMD solutions in the field\nof ophthalmology and beyond.\n\n",
      "date_published": "2024-11-19T00:00:00+00:00",
      "authors": [
        {
          "name": "D.V. Gunasekeran and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400404",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/f7dbec2d-e3d5-4e63-bb5e-b1120a2a0e51/aics2400404_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Diabetes is a major health care challenge, affecting 10% of the global population. One third of patients with diabetes have an ocular complication known as diabetic retinopathy (DR). DR progression to manifestations such as vision-threatening diabetic retinopathy (VTDR) remains the leading cause of blindness in working-aged adults. Yearly DR screening is a universally recommended practice in primary care settings for patients with diabetes, but it is often difficult to implement due to a lack of staffing and screening capacity in primary care. This case study highlights our experience with developing a medical artificial intelligence (AI) software-as-a-medical-device (SaMD) solution for DR screening and implementing it at a national level to provide the capacity needed for DR screening in Singapore. Our approach involved two broad phases. First, we established a national telemedicine screening program, Singapore Integrated Diabetic Retinopathy Program (SiDRP), for population screening of DR in primary care run by trained, nonclinician human graders. Second, we deployed a deep learning\u2013based AI solution, Singapore Eye Lesion Analyzer (SELENA+), into the SiDRP to scale-up the DR screening process by the human graders. We demonstrated the cost-effectiveness of this solution, and obtained medical device regulatory approval for clinical use in health care settings. We report the prospective evaluation of SELENA+ in SiDRP using real-world pilot data from the first 1712 patients consecutively recruited. Sensitivity and specificity of SELENA+ in detection of referable DR cases were 94.7% (95% confidence interval [CI] 88.0% to 98.3%) and 82.2% (95% CI 80.8% to 83.5%), respectively. In comparison, sensitivity and specificity of human graders were 98.9% (95% CI 94.0% to 99.9%) and 97.2% (95% CI 96.6\u201397.8%), respectively. For patients with VTDR, SELENA+ demonstrated a substantial advantage of higher sensitivity compared with human performance, reflecting the benefit of the fine-tuning of SELENA+ that we performed to enhance the AI solution\u2019s ability to detect VTDR. We outline the clinical, technical, operational, regulatory, and governance challenges encountered as well as the lessons learnt in this AI algorithm implementation journey. We also present a conceptual framework with considerations and strategies for the broader adoption of medical AI SaMD solutions in the field of ophthalmology and beyond.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400567",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400567",
      "title": "Mary Steps Out: Capturing Patient Experience through Qualitative and AI Methods",
      "summary": "An examination of the inherent experiential gap between researchers and their participants by viewing the problem through the lens of a popular thought experiment involving the perception of color.",
      "content_text": "## Abstract\n\nFrank Jackson\u2019s 1982 thought experiment, \u201cMary\u2019s Room,\u201d illustrates the\nphilosophical divide between propositional and experiential knowledge. We\npresent a compelling case for the incorporation of lived experience into\nbiomedical research and advocate the integration of AI \u2014 particularly large\nlanguage models (LLMs) such as GPT-4 \u2014 to bridge this epistemological gap.\nWhen paired with sophisticated natural language processing techniques, LLMs\ncould systematically analyze qualitative data from disconnected electronic\nhealth record data. We explore methodologic use cases \u2014 including grounded\ntheory and thematic analysis \u2014 while addressing the challenges of analytical\nfidelity and bias reduction with continuous human oversight. We suggest that\nAI-augmented qualitative research can uncover hidden insights from a multitude\nof disparate datasets, revealing patient experiences that would otherwise\nremain inaccessible. This integrated approach could enrich the understanding\nof health and disease, while ensuring it is as inclusive and reflective of\nhuman complexity as the lives it seeks to understand and improve.\n\n",
      "date_published": "2024-11-06T00:00:00+00:00",
      "authors": [
        {
          "name": "V. Renard and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400567",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Frank Jackson\u2019s 1982 thought experiment, \u201cMary\u2019s Room,\u201d illustrates the philosophical divide between propositional and experiential knowledge. We present a compelling case for the incorporation of lived experience into biomedical research and advocate the integration of AI \u2014 particularly large language models (LLMs) such as GPT-4 \u2014 to bridge this epistemological gap. When paired with sophisticated natural language processing techniques, LLMs could systematically analyze qualitative data from disconnected electronic health record data. We explore methodologic use cases \u2014 including grounded theory and thematic analysis \u2014 while addressing the challenges of analytical fidelity and bias reduction with continuous human oversight. We suggest that AI-augmented qualitative research can uncover hidden insights from a multitude of disparate datasets, revealing patient experiences that would otherwise remain inaccessible. This integrated approach could enrich the understanding of health and disease, while ensuring it is as inclusive and reflective of human complexity as the lives it seeks to understand and improve.</div>"
    }
  ]
}