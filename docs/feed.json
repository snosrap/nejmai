{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400875",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400875",
      "title": "The Utility of Generative AI in Advancing Global Health",
      "summary": "This perspective not only addresses the transformative potential of generative artificial intelligence in advancing global health, particularly in low- and middle-income countries, but proposes actionable solutions to overcome the associated challenges and ethical dilemmas.",
      "content_text": "## Abstract\n\nAchieving the United Nations\u2019 Sustainable Development Goals remains\nchallenging for many low- and middle-income countries (LMICs). Generative\nartificial intelligence (AI) offers the potential to improve health care\ndelivery access and quality in LMICs. Generative AI applications in health\ncare include individualizing decision support systems for personalized\ndiagnosis and treatment, predicting and managing epidemics, improving medical\nimage interpretation, enhancing telemedicine, and accelerating drug discovery.\nIn addition, generative AI can translate medical information into local\nlanguages, boosting health literacy and treatment adherence. However,\npotential challenges and barriers exist in integrating generative AI into the\nhealth systems of LMICs. Ethical dilemmas include the possible negative\nimpacts on climate, limited human expertise and technological infrastructure,\nundermining the local health workforce, introducing potential biases,\ncompromising data privacy, and increasing liability for faults or inaccurate\ndecisions. The barriers and challenges that impede the increased uptake of\ngenerative AI in LMICs need urgent global attention, careful thought, and\ndeliberate action to address them. AI centers of excellence based in LMICs can\nserve as focal points, providing guidance and stewardship on how to address\nthe challenges while harvesting the benefits of generative AI.\n\n",
      "date_published": "2025-02-10T00:00:00+00:00",
      "authors": [
        {
          "name": "H. Akbarialiabad and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400875",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/115f3599-b07b-4c27-95a5-edf1033970fa/aip2400875_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Achieving the United Nations\u2019 Sustainable Development Goals remains challenging for many low- and middle-income countries (LMICs). Generative artificial intelligence (AI) offers the potential to improve health care delivery access and quality in LMICs. Generative AI applications in health care include individualizing decision support systems for personalized diagnosis and treatment, predicting and managing epidemics, improving medical image interpretation, enhancing telemedicine, and accelerating drug discovery. In addition, generative AI can translate medical information into local languages, boosting health literacy and treatment adherence. However, potential challenges and barriers exist in integrating generative AI into the health systems of LMICs. Ethical dilemmas include the possible negative impacts on climate, limited human expertise and technological infrastructure, undermining the local health workforce, introducing potential biases, compromising data privacy, and increasing liability for faults or inaccurate decisions. The barriers and challenges that impede the increased uptake of generative AI in LMICs need urgent global attention, careful thought, and deliberate action to address them. AI centers of excellence based in LMICs can serve as focal points, providing guidance and stewardship on how to address the challenges while harvesting the benefits of generative AI.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400922",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400922",
      "title": "Unseen Commercial Forces Could Undermine Artificial Intelligence Decision Support",
      "summary": "An examination of how commercial influences in health care AI \u2014 such as decision support manipulation, market concentration, and exploitative practices \u2014 reveals risks of misaligning care with patient and societal needs. This perspective advocates for comprehensive oversight and, where appropriate and necessary, regulatory frameworks to protect patient-centered care while fostering innovation and competition.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) is poised to transform health care, yet without\nrobust safeguards, unseen commercial interests could distort care by\nprioritizing profit over patient well-being. The phenomenon of \u201cbiomarkup\u201d\nunderscores how subtle shifts in biomarker thresholds can fuel unwarranted\ntesting, consultations, and treatments, ultimately undercutting cost-control\nefforts. It is easy to imagine a new generation of AI-based revenue cycle\nmanagement model tools that achieve higher reimbursements by nudging\nclinicians toward more lucrative care pathways. AI-based decision support\ninterventions are vulnerable across their entire development life cycle and\ncould be manipulated to favor specific products or services. Furthermore, the\ngrowing consolidation of electronic health records and AI ecosystems restricts\ndata access and risks stifling competition. Addressing these challenges\nrequires a combination of technical and systemic solutions. Standardized\napplication programming interfaces can foster electronic health record\ninteroperability and mitigate vendor lock-in, while equitable and respectful\ndata sharing and governance broadens participation in AI development. Robust\noversight \u2014 including transparency mandates; rigorous auditing; addressing\nfraudulent, coercive, and monopolistic practices; and possibly new regulation\n\u2014 should curb exploitative practices and preserve AI\u2019s transformative\npotential for patient-centered care, cost containment, and societal benefit.\n(Funded by ARPA-H Biomedical Data Fabric Toolbox and others.)\n\n",
      "date_published": "2025-02-06T00:00:00+00:00",
      "authors": [
        {
          "name": "K.D. Mandl"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400922",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) is poised to transform health care, yet without robust safeguards, unseen commercial interests could distort care by prioritizing profit over patient well-being. The phenomenon of \u201cbiomarkup\u201d underscores how subtle shifts in biomarker thresholds can fuel unwarranted testing, consultations, and treatments, ultimately undercutting cost-control efforts. It is easy to imagine a new generation of AI-based revenue cycle management model tools that achieve higher reimbursements by nudging clinicians toward more lucrative care pathways. AI-based decision support interventions are vulnerable across their entire development life cycle and could be manipulated to favor specific products or services. Furthermore, the growing consolidation of electronic health records and AI ecosystems restricts data access and risks stifling competition. Addressing these challenges requires a combination of technical and systemic solutions. Standardized application programming interfaces can foster electronic health record interoperability and mitigate vendor lock-in, while equitable and respectful data sharing and governance broadens participation in AI development. Robust oversight \u2014 including transparency mandates; rigorous auditing; addressing fraudulent, coercive, and monopolistic practices; and possibly new regulation \u2014 should curb exploitative practices and preserve AI\u2019s transformative potential for patient-centered care, cost containment, and societal benefit. (Funded by ARPA-H Biomedical Data Fabric Toolbox and others.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2400657",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2400657",
      "title": "Not All Clinical AI Monitoring Systems Are Created Equal: Review and Recommendations",
      "summary": "This article presents a critical review of the design and performance of AI monitoring systems in clinical settings, highlighting the lack of consensus and transparency in current practices. It introduces a structured road map to guide the development of effective and practical monitoring systems.",
      "content_text": "## Abstract\n\nWhile practices for the initial evaluation of clinical artificial intelligence\n(AI) algorithms are well established, there is little consensus on how to\ndesign effective monitoring systems for the post-deployment setting. In real-\nworld case studies, design choices are often driven by practical constraints\nrather than their impact on the performance of the monitoring system. This\nnarrative review critically examines the key decisions that shape the\nperformance of AI monitoring systems, including the selection of monitoring\ncriteria, the choice of data sources, and the statistical procedures employed.\nOur findings reveal significant variation in the designed systems, often with\nlittle transparency regarding how their design choices affect monitoring\nperformance. To provide a more structured approach to designing monitoring\nsystems that are both effective and practical, we introduce a road map for\nnavigating the many options available. We bootstrap efforts in clinical AI\nmonitoring by highlighting tools from three related fields that face similarly\ncomplex challenges: quality management, clinical trials, and real-world\nevidence generation.\n\n",
      "date_published": "2025-01-23T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Feng and Others"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2400657",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/ce1f7fbb-9a7d-46f0-9aed-85758c8735be/aira2400657_f2.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">While practices for the initial evaluation of clinical artificial intelligence (AI) algorithms are well established, there is little consensus on how to design effective monitoring systems for the post-deployment setting. In real-world case studies, design choices are often driven by practical constraints rather than their impact on the performance of the monitoring system. This narrative review critically examines the key decisions that shape the performance of AI monitoring systems, including the selection of monitoring criteria, the choice of data sources, and the statistical procedures employed. Our findings reveal significant variation in the designed systems, often with little transparency regarding how their design choices affect monitoring performance. To provide a more structured approach to designing monitoring systems that are both effective and practical, we introduce a road map for navigating the many options available. We bootstrap efforts in clinical AI monitoring by highlighting tools from three related fields that face similarly complex challenges: quality management, clinical trials, and real-world evidence generation.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400741",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400741",
      "title": "Multicenter Double-Blind Study Evaluating AI-Driven Detection of Proximal Deep Vein Thrombosis",
      "summary": "This article details a robust, diagnostic test accuracy study, comparing an AI-guided ultrasound tool (AutoDVT) with standard compression ultrasound in 414 patients at 11 centers in the UK, using a double-blind study design. Our initial development data were published in Nature Digital in 2021.",
      "content_text": "## Abstract\n\n### Background\n\nUltrasound is one of the most widely requested forms of diagnostic imaging.\nThe costs for diagnosing deep vein thrombosis (DVT) in the UK are \u00a3175\nmillion, annually. In at least 80% of cases, DVT is excluded. As health care\nprovision becomes increasingly stretched, resource utilization needs to be\noptimized. This prospective, double-blind, test accuracy study was designed to\ntest whether an artificial intelligence (AI)\u2013guided software device (AutoDVT)\ncould support nonradiology specialists to diagnose proximal DVT.\n\n### Methods\n\nEleven regional hospital DVT diagnostic clinics enrolled adult patients, 18\nyears of age or older, who were referred for investigation of symptoms\nsuggestive of DVT, including a compression ultrasound. Prior to the clinical\ncompression ultrasound, an AutoDVT scan was completed. This was a two-point\nAI-guided compression ultrasound scan. We found that the main primary outcome\nwas the sensitivity of AutoDVT within a diagnostic algorithm for the detection\nof proximal DVT by nonradiology-trained staff. Other outcomes included\nspecificity and positive/negative predictive value of AutoDVT.\n\n### Results\n\nA total of 414 participants were enrolled. Proximal DVT was detected in 10.5%\nof those analyzed. AutoDVT resulted in 68% sensitivity (95% confidence\ninterval [CI], 49 to 83%) and 80% specificity (95% CI, 74 to 85%) for the\ndetection of proximal DVT. The negative predictive value for AutoDVT was 95%\n(95% CI, 92 to 98%), with a positive predictive value of 28% (95% CI, 19 to\n40%). Overall, 63 out of 294 results (21%; 95% CI, 17 to 27%) were discrepant\ncompared with compression ultrasound.\n\n### Conclusions\n\nThough AI-guided ultrasound use can detect proximal DVT, test accuracy was not\nsufficient for this device to be used safely. Further optimization of the\nsoftware is required prior to use in clinical practice by nonradiology-trained\nhealth care professionals. (Funded by the Wellcome Trust [Wellcome Innovator\nAward 220505/Z/20/Z]. The trial was registered as [ISRCTN\n11069056](https://doi.org/10.1186/ISRCTN11069056).)\n\n",
      "date_published": "2025-01-23T00:00:00+00:00",
      "authors": [
        {
          "name": "N. Curry and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400741",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIoa2400741_Curry.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Ultrasound is one of the most widely requested forms of diagnostic imaging. The costs for diagnosing deep vein thrombosis (DVT) in the UK are \u00a3175 million, annually. In at least 80% of cases, DVT is excluded. As health care provision becomes increasingly stretched, resource utilization needs to be optimized. This prospective, double-blind, test accuracy study was designed to test whether an artificial intelligence (AI)\u2013guided software device (AutoDVT) could support nonradiology specialists to diagnose proximal DVT.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">Eleven regional hospital DVT diagnostic clinics enrolled adult patients, 18 years of age or older, who were referred for investigation of symptoms suggestive of DVT, including a compression ultrasound. Prior to the clinical compression ultrasound, an AutoDVT scan was completed. This was a two-point AI-guided compression ultrasound scan. We found that the main primary outcome was the sensitivity of AutoDVT within a diagnostic algorithm for the detection of proximal DVT by nonradiology-trained staff. Other outcomes included specificity and positive/negative predictive value of AutoDVT.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">A total of 414 participants were enrolled. Proximal DVT was detected in 10.5% of those analyzed. AutoDVT resulted in 68% sensitivity (95% confidence interval [CI], 49 to 83%) and 80% specificity (95% CI, 74 to 85%) for the detection of proximal DVT. The negative predictive value for AutoDVT was 95% (95% CI, 92 to 98%), with a positive predictive value of 28% (95% CI, 19 to 40%). Overall, 63 out of 294 results (21%; 95% CI, 17 to 27%) were discrepant compared with compression ultrasound.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Though AI-guided ultrasound use can detect proximal DVT, test accuracy was not sufficient for this device to be used safely. Further optimization of the software is required prior to use in clinical practice by nonradiology-trained health care professionals. (Funded by the Wellcome Trust [Wellcome Innovator Award 220505/Z/20/Z]. The trial was registered as <a href=\"https://doi.org/10.1186/ISRCTN11069056\" target=\"_blank\">ISRCTN 11069056</a>.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400402",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400402",
      "title": "A Cross-Sectional Study of GPT-4\u2013Based Plain Language Translation of Clinical Notes to Improve Patient Comprehension of Disease Course and Management",
      "summary": "In a novel clinical application of GPT-4\u2013based translation as a tool for improving confidence and accuracy of patient comprehension of hospital admission events and next steps in disease management, both perceived and objectively measured comprehension scores are higher when patients read plain language discharge summary notes (DSNs) translated by GPT-4 compared with untranslated DSNs. This effect is greater in marginalized populations with historically low rates of health literacy.",
      "content_text": "## Abstract\n\n### Background\n\nThe 21st Century Cures Act provides patients with access to their clinical\nnotes, but most patients, particularly those with limited health literacy,\nhave difficulty understanding and utilizing them for health decisions and care\nmanagement. Thus, there is a critical need to improve the readability of\nclinical notes to ensure that patient access to medical information\nfacilitates equitable health outcomes. This study introduces a novel clinical\napplication of Generative Pretrained Transformer 4 (GPT-4), assessing the\neffect of GPT-4\u2013based plain language translation of discharge summary notes\n(DSNs) on subjective and objective comprehension, self-reported confidence,\nand time spent reading each DSN.\n\n### Methods\n\nWe enrolled 553 patients from December 2023 to February 2024. Participants\nwere at least 18 years of age, able to read English, had no diagnosis of\ncognitive impairment, and had an appointment scheduled within 2 months of\nenrollment at the Duke University Health System, a multicenter academic health\nsystem of three inpatient hospitals and multiple outpatient clinics. Patients\nread four DSNs related to common reasons for admission to an inpatient general\nmedical service: congestive heart failure, community-acquired pneumonia,\ndiabetic ketoacidosis, and acute ischemic stroke. Two DSNs (selected at\nrandom) were replaced by GPT-4\u2013based plain language translations. After\nreading each DSN, patients answered questionnaire items assessing subjective\nand objective comprehension. The effects of translation on comprehension\noutcomes were analyzed with linear mixed models.\n\n### Results\n\nAcross all four DSNs, GPT-4\u2013based translation improved objective comprehension\nby 1.2 of 4 points (CI, 1.09 to 1.26; P<0.001), subjective comprehension by\n2.4 of 16 points (CI, 2.15 to 2.71; P<0.001), and self-reported confidence by\n2.0 of 8 points (CI, 1.82 to 2.13; P<0.001). Improvements were greater among\nBlack and Hispanic patients, older patients, male patients, and those who\nreported limited health knowledge. In particular, objective comprehension\nimproved an additional 0.35 and 0.81 out of 4 points among Black and Hispanic\npatients, respectively.\n\n### Conclusions\n\nGPT-4\u2013based translation substantially improved patient comprehension of DSNs,\nespecially in populations that historically have low health literacy. Further\nresearch is needed to validate results in additional demographic groups and\nstudy downstream effects on health decision-making and outcomes.\n\n",
      "date_published": "2025-01-23T00:00:00+00:00",
      "authors": [
        {
          "name": "A. Kumar and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400402",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/a25a4fbb-e9d8-4393-bf4a-1ced0ee70408/aioa2400402_f2.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">The 21st Century Cures Act provides patients with access to their clinical notes, but most patients, particularly those with limited health literacy, have difficulty understanding and utilizing them for health decisions and care management. Thus, there is a critical need to improve the readability of clinical notes to ensure that patient access to medical information facilitates equitable health outcomes. This study introduces a novel clinical application of Generative Pretrained Transformer 4 (GPT-4), assessing the effect of GPT-4\u2013based plain language translation of discharge summary notes (DSNs) on subjective and objective comprehension, self-reported confidence, and time spent reading each DSN.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We enrolled 553 patients from December 2023 to February 2024. Participants were at least 18 years of age, able to read English, had no diagnosis of cognitive impairment, and had an appointment scheduled within 2 months of enrollment at the Duke University Health System, a multicenter academic health system of three inpatient hospitals and multiple outpatient clinics. Patients read four DSNs related to common reasons for admission to an inpatient general medical service: congestive heart failure, community-acquired pneumonia, diabetic ketoacidosis, and acute ischemic stroke. Two DSNs (selected at random) were replaced by GPT-4\u2013based plain language translations. After reading each DSN, patients answered questionnaire items assessing subjective and objective comprehension. The effects of translation on comprehension outcomes were analyzed with linear mixed models.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">Across all four DSNs, GPT-4\u2013based translation improved objective comprehension by 1.2 of 4 points (CI, 1.09 to 1.26; P&lt;0.001), subjective comprehension by 2.4 of 16 points (CI, 2.15 to 2.71; P&lt;0.001), and self-reported confidence by 2.0 of 8 points (CI, 1.82 to 2.13; P&lt;0.001). Improvements were greater among Black and Hispanic patients, older patients, male patients, and those who reported limited health knowledge. In particular, objective comprehension improved an additional 0.35 and 0.81 out of 4 points among Black and Hispanic patients, respectively.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">GPT-4\u2013based translation substantially improved patient comprehension of DSNs, especially in populations that historically have low health literacy. Further research is needed to validate results in additional demographic groups and study downstream effects on health decision-making and outcomes.</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401235",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401235",
      "title": "It\u2019s Time to Bench the Medical Exam Benchmark",
      "summary": "To move forward as a field, we need to design benchmarks that better align with the tasks these models are expected to perform upon deployment",
      "content_text": "## Abstract\n\nMedical licensing examinations, such as the United States Medical Licensing\nExamination, have become the default benchmarks for evaluating large language\nmodels (LLMs) in health care. Performance on these benchmarks is frequently\ncited as evidence of progress and used to justify the deployment of LLMs into\nclinical settings. However, we argue that these benchmarks are fundamentally\nlimited as signals for assessing true clinical utility.\n\n",
      "date_published": "2025-01-23T00:00:00+00:00",
      "authors": [
        {
          "name": "I.D. Raji, R. Daneshjou, and E. Alsentzer"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401235",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Medical licensing examinations, such as the United States Medical Licensing Examination, have become the default benchmarks for evaluating large language models (LLMs) in health care. Performance on these benchmarks is frequently cited as evidence of progress and used to justify the deployment of LLMs into clinical settings. However, we argue that these benchmarks are fundamentally limited as signals for assessing true clinical utility.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401103",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401103",
      "title": "Cultivating Health Care\u2019s AI Future",
      "summary": "This Perspective article summarizes an NEJM AI Grand Rounds conversation with Dr. Vijay Pande, General Partner at Andreessen Horowitz. We discuss the intersection of artificial intelligence, venture capital, and health care, as well as the potential for AI to democratize access to high-quality health care.",
      "content_text": "## Abstract\n\nDr. Vijay Pande, General Partner at Andreessen Horowitz (a16z), discusses the\nintersection of artificial intelligence, venture capital, and health care on a\nrecent episode of _NEJM AI_ Grand Rounds.1 He emphasizes the potential for AI\nto democratize access to high-quality health care, analogous to how consumer\ntechnology has equalized access to information and entertainment. Dr. Pande\nargues that AI\u2019s impact on health care will be most significant in preventive\ncare and early intervention rather than in treating advanced disease states.\nHe also provides insights into a16z\u2019s investment thesis for health care\ncompanies, and highlights the need for individuals with expertise in both\nclinical medicine and AI to drive innovation in this field. Looking ahead,\nwhile current AI systems excel at tasks based on existing knowledge, the next\nmajor breakthrough may require developing AIs capable of exploration,\ncreation, and discovery akin to human scientific inquiry.\n\n",
      "date_published": "2025-01-21T00:00:00+00:00",
      "authors": [
        {
          "name": "V. Pande, A.K. Manrai, and A.L. Beam"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401103",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Dr. Vijay Pande, General Partner at Andreessen Horowitz (a16z), discusses the intersection of artificial intelligence, venture capital, and health care on a recent episode of <i>NEJM AI</i> Grand Rounds.<sup>1</sup> He emphasizes the potential for AI to democratize access to high-quality health care, analogous to how consumer technology has equalized access to information and entertainment. Dr. Pande argues that AI\u2019s impact on health care will be most significant in preventive care and early intervention rather than in treating advanced disease states. He also provides insights into a16z\u2019s investment thesis for health care companies, and highlights the need for individuals with expertise in both clinical medicine and AI to drive innovation in this field. Looking ahead, while current AI systems excel at tasks based on existing knowledge, the next major breakthrough may require developing AIs capable of exploration, creation, and discovery akin to human scientific inquiry.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400590",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400590",
      "title": "Developing ICU Clinical Behavioral Atlas Using Ambient Intelligence and Computer Vision",
      "summary": "The Clinical Behavioral Atlas, a pioneering computer vision system tailored for the intensive care unit setting, is capable of automatically detecting a broad range of clinically relevant activities, objects, and personnel categories through RGB video data, making strides in ICU observational research.",
      "content_text": "## Abstract\n\n### Background\n\nWhile computer vision has gained traction in medical applications, models\nspecifically engineered for intensive care unit (ICU) activities are limited.\n\n### Methods\n\nWe present Clinical Behavioral Atlas (CBA), a computer vision system that can\nidentify 40 clinically relevant activity categories and 55 object categories\nsolely through RGB video data. The system was developed using a dataset\ncomprising over 140,000 hours of continuous video and over 350,000 densely\nannotated frames, collected from 16 sensors in 8 ICU rooms at an academic\nmedical center.\n\n### Results\n\nThe model demonstrated strong performance in entity and activity detection,\nwith sensitivities of 0.75\u223c0.81 and average precisions of 0.64\u223c0.73,\nrespectively. Permutation tests yielded P values of less than 0.05 for most\nactivity categories. We observed a positive correlation between the\nperformance and both the number and size of entities. The model excelled at\nidentifying common and large objects, even with limited samples, but struggled\nwith small items like oral swabs. Activity detection performance correlated\nlinearly with video duration. The model showed robust performance (>0.85\naverage precision) for most clinical activities, but activities of daily\nliving exhibited greater variation and lower average precision (0.23\u20130.95),\nindicating potential for further refinement due to their complexity and\nrelative scarcity in the dataset. Experiments against other popular activity\nrecognition models reveal that our method substantially outperforms all\nbaselines, with improvements of 0.30 and 0.45 in average precision over the\nnext best method.\n\n### Conclusions\n\nCBA expands automated identification of clinically important bedside clinical\nactions such as ICU preventive bundle elements. While we have demonstrated the\nfeasibility of computer vision as a tool to assist in clinical care in high-\nintensity settings such as the ICU, the development of a full clinical-level\nperformance CBA model will require larger datasets, ideally from multiple\nlocations. (Funded by Schmidt Futures and others.)\n\n",
      "date_published": "2025-01-21T00:00:00+00:00",
      "authors": [
        {
          "name": "W. Dai and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400590",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/d68b8a1a-2303-4791-a855-e2d0ba0b6e01/aioa2400590_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">While computer vision has gained traction in medical applications, models specifically engineered for intensive care unit (ICU) activities are limited.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">We present Clinical Behavioral Atlas (CBA), a computer vision system that can identify 40 clinically relevant activity categories and 55 object categories solely through RGB video data. The system was developed using a dataset comprising over 140,000 hours of continuous video and over 350,000 densely annotated frames, collected from 16 sensors in 8 ICU rooms at an academic medical center.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">The model demonstrated strong performance in entity and activity detection, with sensitivities of 0.75\u223c0.81 and average precisions of 0.64\u223c0.73, respectively. Permutation tests yielded P values of less than 0.05 for most activity categories. We observed a positive correlation between the performance and both the number and size of entities. The model excelled at identifying common and large objects, even with limited samples, but struggled with small items like oral swabs. Activity detection performance correlated linearly with video duration. The model showed robust performance (&gt;0.85 average precision) for most clinical activities, but activities of daily living exhibited greater variation and lower average precision (0.23\u20130.95), indicating potential for further refinement due to their complexity and relative scarcity in the dataset. Experiments against other popular activity recognition models reveal that our method substantially outperforms all baselines, with improvements of 0.30 and 0.45 in average precision over the next best method.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">CBA expands automated identification of clinically important bedside clinical actions such as ICU preventive bundle elements. While we have demonstrated the feasibility of computer vision as a tool to assist in clinical care in high-intensity settings such as the ICU, the development of a full clinical-level performance CBA model will require larger datasets, ideally from multiple locations. (Funded by Schmidt Futures and others.)</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400979",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400979",
      "title": "The Burden of Reviewing LLM-Generated Content",
      "summary": "We propose further study on the use of LLMs to generate clinical documentation and communication, looking in particular at the net benefit on efficiency and cognitive burden when the physician\u2019s efforts are shifted from generating new content to reviewing content created by LLMs.",
      "content_text": "## Abstract\n\nLarge language models (LLMs) in health care aim to reduce the cognitive and\nadministrative burden for health care professionals. Yet the accuracy and\ncompleteness of LLM-generated material is still highly variable. Reviewing\noutput for errors is critical to ensure documentation is accurate and\ncomplete. Unfortunately, humans tend to follow the path of least cognitive\neffort and will quickly become overly reliant on automation aids. Developers\nargue that LLM applications in health care are not automatic solutions as they\nrequire a human-in-the-loop approach, and that the risk of errors is\nadequately mitigated by relying on physician review. However, the perception\nof LLMs as outperforming humans may lead to overestimating the capabilities of\nthe models. When humans begin to rely more heavily on LLM output, the\nautomation argument is no longer valid. While the physician has the final\ndetermination in practice, this approach is grounded in two principles: the\nassumption that every physician will thoroughly review all output, every time,\nfor every patient; and the reliance on disclaimers to cover liability and\nshift accountability from developers to physicians. This creates a new and\ntedious burden on physicians \u2014 proofreading content that was neither written\nnor dictated by the user is difficult to do well. Minimal research has been\npublished to understand the net benefit on efficiency and cognitive burden\nwhen physicians\u2019 efforts are shifted from generating new content to reviewing\ncontent generated by LLMs. Furthermore, the use of LLMs increases physicians\u2019\naccountability for something they did not write. The liability of missing a\nhigh-risk error in LLM output is a novel factor for consideration and not well\nunderstood. Further study on the use of LLMs to generate clinical\ndocumentation or clinical communication should consider the cognitive impact\nof proofreading and the increased accountability of physicians. We recommend\nintegrating implementation science, user experience research, and\ncollaborative input from policy makers, regulators, and professional medical\nassociations to establish a shared accountability structure. This approach\naims to protect patients while empowering physicians to perform their duties\nconfidently, ultimately enhancing both patient safety and physician\nsatisfaction.\n\n",
      "date_published": "2025-01-13T00:00:00+00:00",
      "authors": [
        {
          "name": "J.W. Ohde, L.M. Rost, and J.D. Overgaard"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400979",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Large language models (LLMs) in health care aim to reduce the cognitive and administrative burden for health care professionals. Yet the accuracy and completeness of LLM-generated material is still highly variable. Reviewing output for errors is critical to ensure documentation is accurate and complete. Unfortunately, humans tend to follow the path of least cognitive effort and will quickly become overly reliant on automation aids. Developers argue that LLM applications in health care are not automatic solutions as they require a human-in-the-loop approach, and that the risk of errors is adequately mitigated by relying on physician review. However, the perception of LLMs as outperforming humans may lead to overestimating the capabilities of the models. When humans begin to rely more heavily on LLM output, the automation argument is no longer valid. While the physician has the final determination in practice, this approach is grounded in two principles: the assumption that every physician will thoroughly review all output, every time, for every patient; and the reliance on disclaimers to cover liability and shift accountability from developers to physicians. This creates a new and tedious burden on physicians \u2014 proofreading content that was neither written nor dictated by the user is difficult to do well. Minimal research has been published to understand the net benefit on efficiency and cognitive burden when physicians\u2019 efforts are shifted from generating new content to reviewing content generated by LLMs. Furthermore, the use of LLMs increases physicians\u2019 accountability for something they did not write. The liability of missing a high-risk error in LLM output is a novel factor for consideration and not well understood. Further study on the use of LLMs to generate clinical documentation or clinical communication should consider the cognitive impact of proofreading and the increased accountability of physicians. We recommend integrating implementation science, user experience research, and collaborative input from policy makers, regulators, and professional medical associations to establish a shared accountability structure. This approach aims to protect patients while empowering physicians to perform their duties confidently, ultimately enhancing both patient safety and physician satisfaction.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400889",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400889",
      "title": "Using Large Language Models to Promote Health Equity",
      "summary": "We emphasize the emerging potential of LLMs to promote health equity by highlighting three promising use cases: detecting human biases (e.g., from clinical notes); creating structured equity-relevant databases from unstructured text; and improving equity of access to health information.",
      "content_text": "## Abstract\n\nWhile the discussion about the effects of large language models (LLMs) on\nhealth equity has been largely cautionary, LLMs also present significant\nopportunities for improving health equity. We highlight three such\nopportunities: improving the detection of human bias; creating structured\ndatasets relevant to health equity; and improving equity of access to health\ninformation.\n\n",
      "date_published": "2025-01-13T00:00:00+00:00",
      "authors": [
        {
          "name": "E. Pierson and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400889",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/e1a05382-d266-4603-bb27-fd0057405d79/aip2400889_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">While the discussion about the effects of large language models (LLMs) on health equity has been largely cautionary, LLMs also present significant opportunities for improving health equity. We highlight three such opportunities: improving the detection of human bias; creating structured datasets relevant to health equity; and improving equity of access to health information.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2400464",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2400464",
      "title": "Disclosure, Humanizing, and Contextual Vulnerability of Generative AI Chatbots",
      "summary": "One key decision for regulatory bodies is whether to require app makers to disclose the use of generative AI-powered chatbots in their products. We suggest that some generative AI-based chatbots lead consumers to use chatbots in unintended ways that make consumers contextually vulnerable, and we discuss how managers and regulators can proactively address this challenge.",
      "content_text": "## Abstract\n\nIn the wake of recent advancements in generative artificial intelligence (AI),\nregulatory bodies are trying to keep pace. One key decision is whether to\nrequire app makers to disclose the use of generative AI-powered chatbots in\ntheir products. We suggest that some generative AI-based chatbots lead\nconsumers to use chatbots in unintended ways that create mental health risks,\nmaking consumers contextually vulnerable \u2014 defined as a temporary state of\nsusceptibility to harm or other adverse mental health effects arising from the\ninterplay between a user\u2019s interactions with a particular system and the\nsystem\u2019s response. We argue that for health apps, including medical devices\nand wellness apps, disclosure should be mandated. We also show that even when\nchatbots are disclosed in these instances, they may still carry risks due to\nthe tendency of app makers to humanize their chatbots. The current regulatory\nstructure does not fully address these challenges. We discuss how app makers\nand regulators should proactively address this challenge by considering where\napps fall along the continuum of perceived humanness. For health-related apps,\nthis evaluation should lead to a mandate or strong recommendation that neutral\n(nonhumanized) chatbots be the default, with any deviations from this standard\nrequiring clear justification. (Funded in part by a Novo Nordisk Foundation\nGrant; NNF23SA0087056.)\n\n",
      "date_published": "2024-01-21T00:00:00+00:00",
      "authors": [
        {
          "name": "J. De Freitas and I.G. Cohen"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2400464",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/d68c4083-26e2-44d3-b542-5773fc0ce917/aipc2400464_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In the wake of recent advancements in generative artificial intelligence (AI), regulatory bodies are trying to keep pace. One key decision is whether to require app makers to disclose the use of generative AI-powered chatbots in their products. We suggest that some generative AI-based chatbots lead consumers to use chatbots in unintended ways that create mental health risks, making consumers contextually vulnerable \u2014 defined as a temporary state of susceptibility to harm or other adverse mental health effects arising from the interplay between a user\u2019s interactions with a particular system and the system\u2019s response. We argue that for health apps, including medical devices and wellness apps, disclosure should be mandated. We also show that even when chatbots are disclosed in these instances, they may still carry risks due to the tendency of app makers to humanize their chatbots. The current regulatory structure does not fully address these challenges. We discuss how app makers and regulators should proactively address this challenge by considering where apps fall along the continuum of perceived humanness. For health-related apps, this evaluation should lead to a mandate or strong recommendation that neutral (nonhumanized) chatbots be the default, with any deviations from this standard requiring clear justification. (Funded in part by a Novo Nordisk Foundation Grant; NNF23SA0087056.)</div>"
    }
  ]
}