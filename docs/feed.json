{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400390",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400390",
      "title": "Fine-Tuning LLMs with Medical Data: Can Safety Be Ensured?",
      "summary": "This study examines the vulnerability of large-scale language models (LLMs) tuned to medical data to adversarial attacks. The study shows that such models can expose sensitive patient information. The results demonstrate the effectiveness of prompt-based attacks in compromising privacy, and underscore the critical need for enhanced defenses in the training of health care\u2013focused LLMs.",
      "content_text": "## Abstract\n\nDeveloping large-scale language models (LLMs) for health care requires fine-\ntuning with health care domain data suitable for downstream tasks. However,\nfine-tuning LLMs with medical data can expose the training data used during\nlearning to adversarial attacks. This issue is particularly important as\nmedical data contain sensitive and identifiable patient data. The prompt-based\nadversarial attack approach was employed to assess the potential for medical\nprivacy breaches in LLMs. The success rate of the attack was evaluated by\ncategorizing 71 medical questions into three key metrics. To confirm the\nexposure of LLMs training data, each case was compared with the original\nelectronic medical record. The security of the model was confirmed to be\ncompromised by the prompt attack method, resulting in a jailbreak (i.e.,\nsecurity breach). The American Standard Code for Information Interchange code\nencoding method had a success rate of up to 80.8% in disabling the guardrail.\nThe success rate of attacks that caused the model to expose part of the\ntraining data was up to 21.8%. These findings underscore the critical need for\nrobust defense strategies to protect patient privacy and maintain the\nintegrity of medical information. Addressing these vulnerabilities is crucial\nfor integrating LLMs into clinical workflows safely, balancing the benefits of\nadvanced artificial intelligence technologies with the need to protect\nsensitive patient data. (Funded by the Korea Health Industry Development\nInstitute and the Ministry of Health & Welfare, Republic of Korea.)\n\n",
      "date_published": "2024-12-24T00:00:00+00:00",
      "authors": [
        {
          "name": "M. Kim and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400390",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/1d15cfb3-4fd8-4d5d-961d-c0e76f8d15db/aics2400390_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Developing large-scale language models (LLMs) for health care requires fine-tuning with health care domain data suitable for downstream tasks. However, fine-tuning LLMs with medical data can expose the training data used during learning to adversarial attacks. This issue is particularly important as medical data contain sensitive and identifiable patient data. The prompt-based adversarial attack approach was employed to assess the potential for medical privacy breaches in LLMs. The success rate of the attack was evaluated by categorizing 71 medical questions into three key metrics. To confirm the exposure of LLMs training data, each case was compared with the original electronic medical record. The security of the model was confirmed to be compromised by the prompt attack method, resulting in a jailbreak (i.e., security breach). The American Standard Code for Information Interchange code encoding method had a success rate of up to 80.8% in disabling the guardrail. The success rate of attacks that caused the model to expose part of the training data was up to 21.8%. These findings underscore the critical need for robust defense strategies to protect patient privacy and maintain the integrity of medical information. Addressing these vulnerabilities is crucial for integrating LLMs into clinical workflows safely, balancing the benefits of advanced artificial intelligence technologies with the need to protect sensitive patient data. (Funded by the Korea Health Industry Development Institute and the Ministry of Health &amp; Welfare, Republic of Korea.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400360",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400360",
      "title": "Zero-Shot Clinical Trial Patient Matching with LLMs",
      "summary": "We investigate the accuracy, efficiency, and interpretability of using large language models for clinical trial patient matching. While prior work has focused mostly on fine-tuning and few-shot prompting, we are specifically interested in scalable approaches with an eye toward real-world deployment, and thus our study focuses on the zero-shot performance of these models to scale to arbitrary trials and we develop a two-stage retrieval pipeline to scale to arbitrary length patient records.",
      "content_text": "## Abstract\n\nMatching patients to clinical trials is a key challenge in bringing new drugs\nto market. Identifying patients who meet eligibility criteria for a trial is\nhighly manual, taking up to 1 hour per patient. Automated screening is\nchallenging, however, as it requires the ability to understand unstructured\nclinical text. To address this, we have designed a zero-shot large language\nmodel (LLM)\u2013based system that evaluates a patient\u2019s medical history (as\nunstructured clinical text) against trial inclusion criteria (also specified\nas free text). We investigate different prompting strategies and design a\nnovel two-stage retrieval pipeline to reduce the number of tokens processed by\nup to a third while sustaining high performance. Our contributions are\nthreefold. First, we achieve state-of-the-art performance on the 2018 n2c2\ncohort selection challenge, the largest public benchmark for clinical trial\npatient matching. Second, this system can improve the data and cost efficiency\nof matching patients an order of magnitude faster and more affordably than the\nstatus quo. Third, we demonstrate the interpretability of our system by\ngenerating natural language justifications for each eligibility decision,\nwhich clinicians found coherent in 97% of correct decisions and 75% of\nincorrect ones. These results establish the feasibility of using LLMs to\naccelerate clinical trial operations, with the zero-shot retrieval\narchitecture scalable to arbitrary trials and patient record length with\nminimal reconfiguration. (Funded by the Clinical Excellence Research Center at\nStanford Medicine and others.)\n\n",
      "date_published": "2024-12-24T00:00:00+00:00",
      "authors": [
        {
          "name": "M. Wornow and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400360",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/b597e4ef-ea7f-423d-98b6-e64c0d8f7d72/aics2400360_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Matching patients to clinical trials is a key challenge in bringing new drugs to market. Identifying patients who meet eligibility criteria for a trial is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires the ability to understand unstructured clinical text. To address this, we have designed a zero-shot large language model (LLM)\u2013based system that evaluates a patient\u2019s medical history (as unstructured clinical text) against trial inclusion criteria (also specified as free text). We investigate different prompting strategies and design a novel two-stage retrieval pipeline to reduce the number of tokens processed by up to a third while sustaining high performance. Our contributions are threefold. First, we achieve state-of-the-art performance on the 2018 n2c2 cohort selection challenge, the largest public benchmark for clinical trial patient matching. Second, this system can improve the data and cost efficiency of matching patients an order of magnitude faster and more affordably than the status quo. Third, we demonstrate the interpretability of our system by generating natural language justifications for each eligibility decision, which clinicians found coherent in 97% of correct decisions and 75% of incorrect ones. These results establish the feasibility of using LLMs to accelerate clinical trial operations, with the zero-shot retrieval architecture scalable to arbitrary trials and patient record length with minimal reconfiguration. (Funded by the Clinical Excellence Research Center at Stanford Medicine and others.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "title": "RAG in Health Care: A Novel Framework for Improving Communication and Decision-Making by Addressing LLM Limitations",
      "summary": "This review article explores and evaluates the opportunities in using retrieval augmented generation (RAG) to overcome existing LLM limitations when used within the healthcare and pharmaceutical domain. It provides a framework of how RAG works and describes its strengths and weaknesses when applied to the field of medicine.",
      "content_text": "## Abstract\n\nWithin the field of artificial intelligence (AI), large language models (LLMs)\nhave the potential to transform the delivery of medical information. LLMs, as\na subset of generative AI, have demonstrated value in content creation, idea\ngeneration, and interactive communication. However, their inherent\nlimitations, such as the need for up-to-date information, hallucinations of\nincorrect facts, and a reliance on public-domain data, restrict the full\npotential of generative AI within the health care setting. To address these\nlimitations, retrieval-augmented generation (RAG) offers a novel framework by\nconnecting LLMs with external knowledge, enabling them to access information\nbeyond their training data. Within the health care domain, additional datasets\ncould include peer-reviewed studies, gated medical compendiums, and the\ninternal policies of health care organizations such as hospitals or\npharmaceutical companies. By leveraging RAG, existing generative AI tools gain\nthe capability to consider both public and private information, expanding\ntheir application and enhancing accuracy and relevance within the health care\nsetting. The utility of RAG in the health care setting has yet to be fully\nexplored, but it has the potential to revolutionize the industry. This article\nseeks to outline present and future use cases of RAG for health care\ninformation exchange within both clinical and industrial settings.\n\n",
      "date_published": "2024-12-23T00:00:00+00:00",
      "authors": [
        {
          "name": "K.K.Y. Ng, I. Matsuba, and P.C. Zhang"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2400380",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIra2400380_Zhang-1734963181703.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Within the field of artificial intelligence (AI), large language models (LLMs) have the potential to transform the delivery of medical information. LLMs, as a subset of generative AI, have demonstrated value in content creation, idea generation, and interactive communication. However, their inherent limitations, such as the need for up-to-date information, hallucinations of incorrect facts, and a reliance on public-domain data, restrict the full potential of generative AI within the health care setting. To address these limitations, retrieval-augmented generation (RAG) offers a novel framework by connecting LLMs with external knowledge, enabling them to access information beyond their training data. Within the health care domain, additional datasets could include peer-reviewed studies, gated medical compendiums, and the internal policies of health care organizations such as hospitals or pharmaceutical companies. By leveraging RAG, existing generative AI tools gain the capability to consider both public and private information, expanding their application and enhancing accuracy and relevance within the health care setting. The utility of RAG in the health care setting has yet to be fully explored, but it has the potential to revolutionize the industry. This article seeks to outline present and future use cases of RAG for health care information exchange within both clinical and industrial settings.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2400927",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2400927",
      "title": "Managing Patient Use of Generative Health AI",
      "summary": "In reviewing the benefits and risks of patients\u2019 use of generative artificial intelligence and policy approaches to increasing trust and containing risks associated with this use, the authors conclude that while government will play a role in helping patients use generative AI effectively, major responsibility will fall to the private sector.",
      "content_text": "## Abstract\n\nPatients\u2019 use of artificial intelligence, or patient AI (PAI), is now\nwidespread, promising both benefits and risks. There is an urgent need to\nassist patients with using these new technologies as safely and effectively as\npossible. Focusing on patients\u2019 use of large language models (LLMs) for health\ncare purposes, this article explores critical issues in the management of\ngenerative PAI in the United States, including constitutional limits on\ngovernment\u2019s regulatory authority, and identifies opportunities for public and\nprivate actors to help patients take advantage of generative PAI safely. With\nrespect to the public sector, there is a critical need for federal action to\nfund research on the benefits and risks of PAI and on the development of valid\nmetrics and performance standards for PAI. The federal government also needs\nto better protect the privacy and security of patient data shared with PAI,\nand to require transparency with respect to how LLMs used for health care\npurposes are funded, how they are developed, and how they perform. (Supported\nby the Commonwealth Fund.)\n\n",
      "date_published": "2024-12-20T00:00:00+00:00",
      "authors": [
        {
          "name": "D. Blumenthal and C. Goldberg"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2400927",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Patients\u2019 use of artificial intelligence, or patient AI (PAI), is now widespread, promising both benefits and risks. There is an urgent need to assist patients with using these new technologies as safely and effectively as possible. Focusing on patients\u2019 use of large language models (LLMs) for health care purposes, this article explores critical issues in the management of generative PAI in the United States, including constitutional limits on government\u2019s regulatory authority, and identifies opportunities for public and private actors to help patients take advantage of generative PAI safely. With respect to the public sector, there is a critical need for federal action to fund research on the benefits and risks of PAI and on the development of valid metrics and performance standards for PAI. The federal government also needs to better protect the privacy and security of patient data shared with PAI, and to require transparency with respect to how LLMs used for health care purposes are funded, how they are developed, and how they perform. (Supported by the Commonwealth Fund.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400640",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400640",
      "title": "A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image\u2013Text Pairs",
      "summary": "BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications.",
      "content_text": "## Abstract\n\n### Background\n\nBiomedical data are inherently multimodal, comprising physical measurements\nand natural-language narratives. A generalist biomedical artificial\nintelligence (AI) model needs to simultaneously process different modalities\nof data, including text and images. Therefore, training an effective\ngeneralist biomedical model requires high-quality multimodal data, such as\nparallel image\u2013text pairs.\n\n### Methods\n\nHere, we present PMC-15M, a novel dataset that is two orders of magnitude\nlarger than existing biomedical multimodal datasets, such as MIMIC-CXR, and\nspans a diverse range of biomedical image types. PMC-15M contains 15 million\nbiomedical image\u2013text pairs collected from 4.4 million scientific articles.\nBased on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation\nmodel, with domain-specific adaptations tailored to biomedical vision\u2013language\nprocessing.\n\n### Results\n\nWe conducted extensive experiments and ablation studies on standard biomedical\nimaging tasks from retrieval to classification to visual question answering\n(VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of\nstandard datasets, substantially outperforming prior approaches. Intriguingly,\nby large-scale pretraining on diverse biomedical image types, BiomedCLIP even\noutperforms state-of-the-art radiology-specific models, such as BioViL, in\nradiology-specific tasks such as Radiological Society of North America (RSNA)\npneumonia detection.\n\n### Conclusions\n\nBiomedCLIP is a fully open-access foundation model that achieves state-of-the-\nart performance on various biomedical tasks, paving the way for transformative\nmultimodal biomedical discovery and applications. We release our models at\naka.ms/biomedclip to facilitate future research in multimodal biomedical AI.\n\n",
      "date_published": "2024-12-20T00:00:00+00:00",
      "authors": [
        {
          "name": "S. Zhang and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400640",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/d8ab9f42-1745-4f0b-b237-280d2f132674/aioa2400640_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Biomedical data are inherently multimodal, comprising physical measurements and natural-language narratives. A generalist biomedical artificial intelligence (AI) model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image\u2013text pairs.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets, such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image\u2013text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision\u2013language processing.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models, such as BioViL, in radiology-specific tasks such as Radiological Society of North America (RSNA) pneumonia detection.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "title": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations",
      "summary": "We set out recommendations enabling those creating and/or using health care datasets to identify and transparently acknowledge underlying biases.",
      "content_text": "## Abstract\n\nWithout careful dissection of the ways in which biases can be encoded into\nartificial intelligence (AI) health technologies, there is a risk of\nperpetuating existing health inequalities at scale. One major source of bias\nis the data that underpins such technologies. The STANDING Together\nrecommendations aim to encourage transparency regarding limitations of health\ndatasets and proactive evaluation of their effect across population groups.\nDraft recommendation items were informed by a systematic review and\nstakeholder survey. The recommendations were developed using a Delphi\napproach, supplemented by a public consultation and international interview\nstudy. Overall, more than 350 representatives from 58 countries provided input\ninto this initiative. 194 Delphi participants from 25 countries voted and\nprovided comments on 32 candidate items across three electronic survey rounds\nand one in-person consensus meeting. The 29 STANDING Together consensus\nrecommendations are presented here in two parts. Recommendations for\nDocumentation of Health Datasets provide guidance for dataset curators to\nenable transparency around data composition and limitations. Recommendations\nfor Use of Health Datasets aim to enable identification and mitigation of\nalgorithmic biases that might exacerbate health inequalities. These\nrecommendations are intended to prompt proactive inquiry rather than acting as\na checklist. We hope to raise awareness that no dataset is free of\nlimitations, so transparent communication of data limitations should be\nperceived as valuable, and absence of this information as a limitation. We\nhope that adoption of the STANDING Together recommendations by stakeholders\nacross the AI health technology lifecycle will enable everyone in society to\nbenefit from technologies which are safe and effective. (Funded by The NHS AI\nLab and The Health Foundation, and supported by the National Institute for\nHealth and Care Research [NIHR].)\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "J.E. Alderman and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401088",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective. (Funded by The NHS AI Lab and The Health Foundation, and supported by the National Institute for Health and Care Research [NIHR].)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "title": "How Generalizable Are Foundation Models When Applied to Different Demographic Groups and Settings?",
      "summary": "An evaluation of the foundational model RETFound on an Asian-specific dataset indicates the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need to include more diverse data and collaborate globally on research.",
      "content_text": "## Abstract\n\nRETFound is a retinal image\u2013based foundational artificial intelligence (AI)\nmodel that can be fine-tuned to downstream tasks. However, its\ngeneralizability to Asian populations remains unclear. In this study, we fine-\ntuned RETFound on an Asian-specific dataset. We then evaluated the performance\nof RETFound versus a conventional Vision Transformer model (pretrained on\nImageNet) in diagnosing glaucoma and coronary heart disease and predicting the\n3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d\ndataset, RETFound showed no significant improvement compared with a\nconventional Vision Transformer model (area under the curves [AUCs] of 0.863,\n0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2).\nFurthermore, in scenarios with limited training data (fine-tuned on \u226425% of\nthe full dataset), RETFound showed a slight advantage (up to a maximum AUC\nincrease of 0.03). However, these improvements were not statistically\nsignificant (all P\u22650.2). These findings indicate the challenges foundational\nAI models face in adapting to diverse demographics, emphasizing the need for\nmore diverse data in current foundation models and the importance of global\ncollaboration on foundation model research.\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "Z. Xiong and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400497",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/925aa4d2-c256-45bf-ade8-81f317c70045/aics2400497_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">RETFound is a retinal image\u2013based foundational artificial intelligence (AI) model that can be fine-tuned to downstream tasks. However, its generalizability to Asian populations remains unclear. In this study, we fine-tuned RETFound on an Asian-specific dataset. We then evaluated the performance of RETFound versus a conventional Vision Transformer model (pretrained on ImageNet) in diagnosing glaucoma and coronary heart disease and predicting the 3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d dataset, RETFound showed no significant improvement compared with a conventional Vision Transformer model (area under the curves [AUCs] of 0.863, 0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2). Furthermore, in scenarios with limited training data (fine-tuned on \u226425% of the full dataset), RETFound showed a slight advantage (up to a maximum AUC increase of 0.03). However, these improvements were not statistically significant (all P\u22650.2). These findings indicate the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need for more diverse data in current foundation models and the importance of global collaboration on foundation model research.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401073",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401073",
      "title": "The Promise and Perils of Autonomous AI in Science",
      "summary": "We recommend the development of clearer guidelines and ethical standards for the use of artificial intelligence (AI) in research, fostering human-AI collaboration to enhance research quality while preserving human oversight and integrating the innovative \u201cdata-chaining\u201d transparency mechanisms more broadly to support reproducibility and traceability.",
      "content_text": "## Abstract\n\nIn this edition of _NEJM AI_ , Ifargan and colleagues present data-to-paper,\nan autonomous platform designed to mimic human scientific practice by guiding\na large language model through a stepwise research process to produce complete\nresearch papers. While the platform shows promise, significant errors\nfrequently occur with complex datasets, requiring human intervention for\ncorrection. Here, we explore the risks of over-reliance on such automated\ntools, echoing potential threats to scientific integrity and a surge in low-\nquality publications, while also considering their potential role in the\nreproduction and verification of scientific findings. To address these\nchallenges, we recommend the development of clearer guidelines and ethical\nstandards for the use of artificial intelligence (AI) in research, fostering\nhuman\u2013AI collaboration to enhance research quality while preserving human\noversight, and integrating the innovative \u201cdata-chaining\u201d transparency\nmechanisms more broadly to support reproducibility and traceability.\n\n",
      "date_published": "2024-12-16T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Gao and E.M. Harrison"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401073",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In this edition of <i>NEJM AI</i>, Ifargan and colleagues present data-to-paper, an autonomous platform designed to mimic human scientific practice by guiding a large language model through a stepwise research process to produce complete research papers. While the platform shows promise, significant errors frequently occur with complex datasets, requiring human intervention for correction. Here, we explore the risks of over-reliance on such automated tools, echoing potential threats to scientific integrity and a surge in low-quality publications, while also considering their potential role in the reproduction and verification of scientific findings. To address these challenges, we recommend the development of clearer guidelines and ethical standards for the use of artificial intelligence (AI) in research, fostering human\u2013AI collaboration to enhance research quality while preserving human oversight, and integrating the innovative \u201cdata-chaining\u201d transparency mechanisms more broadly to support reproducibility and traceability.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "title": "Ethical Use of Artificial Intelligence in Medical Diagnostics Demands a Focus on Accuracy, Not Fairness",
      "summary": "We argue that in developing and evaluating AI tools for medical diagnostics, rather than focusing on fairness criteria that quantify disparities between protected subpopulations, our first and most important objective should be to track and maximize diagnostic accuracy for all subpopulations.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) promises to be a transformative technology for\nmedicine and health care. As such, there is an increasing interest in ensuring\nits ethical use. In this perspective, we consider the employment of AI for\nmedical diagnostics, where the goal is the detection and classification of an\nunderlying pathology, based on data such as patient information, clinical\npresentation, tests, and imaging. We argue that instead of prioritizing\nfairness criteria that measure disparities between protected groups, the\nprimary goal should be to assess and enhance diagnostic accuracy within each\nsubpopulation. This approach shifts the focus from optimizing overall\npopulation accuracy to ensuring maximal accuracy in each subpopulation. Our\nperspective implies that we should be using all available information,\nincluding protected group identity, in our methods. We decouple the goal of\naccurate diagnosis from fairness considerations in screening and postdiagnosis\nclinical decisions, which often require the allocation of finite resources.\nFurthermore, we underscore the importance of collecting high-quality and\nrepresentative datasets for each subpopulation, including ensuring that the\nground truth labels we train and with which we evaluate are unbiased.\n\n",
      "date_published": "2024-12-13T00:00:00+00:00",
      "authors": [
        {
          "name": "M.R. Sabuncu, A.Q. Wang, and M. Nguyen"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400672",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) promises to be a transformative technology for medicine and health care. As such, there is an increasing interest in ensuring its ethical use. In this perspective, we consider the employment of AI for medical diagnostics, where the goal is the detection and classification of an underlying pathology, based on data such as patient information, clinical presentation, tests, and imaging. We argue that instead of prioritizing fairness criteria that measure disparities between protected groups, the primary goal should be to assess and enhance diagnostic accuracy within each subpopulation. This approach shifts the focus from optimizing overall population accuracy to ensuring maximal accuracy in each subpopulation. Our perspective implies that we should be using all available information, including protected group identity, in our methods. We decouple the goal of accurate diagnosis from fairness considerations in screening and postdiagnosis clinical decisions, which often require the allocation of finite resources. Furthermore, we underscore the importance of collecting high-quality and representative datasets for each subpopulation, including ensuring that the ground truth labels we train and with which we evaluate are unbiased.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "title": "Autonomous LLM-Driven Research \u2014 from Data to Human-Verifiable Research Papers",
      "summary": "Our study demonstrates a potential for AI-driven acceleration of scientific discovery in biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.",
      "content_text": "## Abstract\n\n### Background\n\nArtificial intelligence (AI) promises to accelerate scientific discovery, but\nit remains unclear whether AI systems can perform fully autonomous research,\nand whether they can do so while adhering to key scientific values, such as\ntransparency, traceability, and verifiability. The aim of this study was to\ndevelop and evaluate an AI-automation platform that performs transparent,\ntraceable, and human-verifiable scientific research.\n\n### Methods\n\nTo mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation\nplatform that guides interacting large language model (LLM) agents through a\ncomplete stepwise research process that starts with annotated data and results\nin comprehensive research papers, while programmatically backtracing\ninformation flow and allowing human oversight and interactions. The platform\ncan run fully autonomously (in autopilot mode) or with human intervention (in\ncopilot mode).\n\n### Results\n\nIn autopilot mode, provided only with annotated data, data-to-paper raised\nhypotheses; designed research plans; wrote and debugged analysis codes;\ngenerated and interpreted results; and created complete, information-traceable\nresearch papers. Even though the research novelty of manuscripts created by\ndata-to-paper was relatively limited, the process demonstrated the autonomous\ngeneration of de novo quantitative insights from data, such as unraveling\nassociations between health indicators and clinical outcomes. For simple\nresearch goals and datasets, a fully autonomous cycle can create manuscripts\nthat independently recapitulate the findings of peer-reviewed biomedical\npublications without major errors in about 80 to 90% of cases. Yet, as goal or\ndata complexity increases, human copiloting becomes critical for ensuring\naccuracy and overall quality. By tracking information flow through the steps,\nthe platform creates \u201cdata-chained\u201d manuscripts, in which downstream results\nare programmatically linked to upstream code and data, thus setting a new\nstandard for the verifiability of scientific outputs.\n\n### Conclusions\n\nOur work demonstrates the potential for AI-driven acceleration of scientific\ndiscovery in data-driven biomedical research and beyond, while enhancing,\nrather than jeopardizing, traceability, transparency, and verifiability.\n\n",
      "date_published": "2024-12-03T00:00:00+00:00",
      "authors": [
        {
          "name": "T. Ifargan and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400555",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/06a1651e-79ce-4d95-887b-9fbf5ed07916/aioa2400555_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Artificial intelligence (AI) promises to accelerate scientific discovery, but it remains unclear whether AI systems can perform fully autonomous research, and whether they can do so while adhering to key scientific values, such as transparency, traceability, and verifiability. The aim of this study was to develop and evaluate an AI-automation platform that performs transparent, traceable, and human-verifiable scientific research.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">To mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation platform that guides interacting large language model (LLM) agents through a complete stepwise research process that starts with annotated data and results in comprehensive research papers, while programmatically backtracing information flow and allowing human oversight and interactions. The platform can run fully autonomously (in autopilot mode) or with human intervention (in copilot mode).</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">In autopilot mode, provided only with annotated data, data-to-paper raised hypotheses; designed research plans; wrote and debugged analysis codes; generated and interpreted results; and created complete, information-traceable research papers. Even though the research novelty of manuscripts created by data-to-paper was relatively limited, the process demonstrated the autonomous generation of de novo quantitative insights from data, such as unraveling associations between health indicators and clinical outcomes. For simple research goals and datasets, a fully autonomous cycle can create manuscripts that independently recapitulate the findings of peer-reviewed biomedical publications without major errors in about 80 to 90% of cases. Yet, as goal or data complexity increases, human copiloting becomes critical for ensuring accuracy and overall quality. By tracking information flow through the steps, the platform creates \u201cdata-chained\u201d manuscripts, in which downstream results are programmatically linked to upstream code and data, thus setting a new standard for the verifiability of scientific outputs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Our work demonstrates the potential for AI-driven acceleration of scientific discovery in data-driven biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.</div></section>"
    }
  ]
}