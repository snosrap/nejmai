{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "NEJM AI",
  "home_page_url": "https://ai.nejm.org",
  "favicon": "https://ai.nejm.org/favicon.ico",
  "items": [
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400979",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400979",
      "title": "The Burden of Reviewing LLM-Generated Content",
      "summary": "We propose further study on the use of LLMs to generate clinical documentation and communication, looking in particular at the net benefit on efficiency and cognitive burden when the physician\u2019s efforts are shifted from generating new content to reviewing content created by LLMs.",
      "content_text": "## Abstract\n\nLarge language models (LLMs) in health care aim to reduce the cognitive and\nadministrative burden for health care professionals. Yet the accuracy and\ncompleteness of LLM-generated material is still highly variable. Reviewing\noutput for errors is critical to ensure documentation is accurate and\ncomplete. Unfortunately, humans tend to follow the path of least cognitive\neffort and will quickly become overly reliant on automation aids. Developers\nargue that LLM applications in health care are not automatic solutions as they\nrequire a human-in-the-loop approach, and that the risk of errors is\nadequately mitigated by relying on physician review. However, the perception\nof LLMs as outperforming humans may lead to overestimating the capabilities of\nthe models. When humans begin to rely more heavily on LLM output, the\nautomation argument is no longer valid. While the physician has the final\ndetermination in practice, this approach is grounded in two principles: the\nassumption that every physician will thoroughly review all output, every time,\nfor every patient; and the reliance on disclaimers to cover liability and\nshift accountability from developers to physicians. This creates a new and\ntedious burden on physicians \u2014 proofreading content that was neither written\nnor dictated by the user is difficult to do well. Minimal research has been\npublished to understand the net benefit on efficiency and cognitive burden\nwhen physicians\u2019 efforts are shifted from generating new content to reviewing\ncontent generated by LLMs. Furthermore, the use of LLMs increases physicians\u2019\naccountability for something they did not write. The liability of missing a\nhigh-risk error in LLM output is a novel factor for consideration and not well\nunderstood. Further study on the use of LLMs to generate clinical\ndocumentation or clinical communication should consider the cognitive impact\nof proofreading and the increased accountability of physicians. We recommend\nintegrating implementation science, user experience research, and\ncollaborative input from policy makers, regulators, and professional medical\nassociations to establish a shared accountability structure. This approach\naims to protect patients while empowering physicians to perform their duties\nconfidently, ultimately enhancing both patient safety and physician\nsatisfaction.\n\n",
      "date_published": "2025-01-13T00:00:00+00:00",
      "authors": [
        {
          "name": "J.W. Ohde, L.M. Rost, and J.D. Overgaard"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400979",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Large language models (LLMs) in health care aim to reduce the cognitive and administrative burden for health care professionals. Yet the accuracy and completeness of LLM-generated material is still highly variable. Reviewing output for errors is critical to ensure documentation is accurate and complete. Unfortunately, humans tend to follow the path of least cognitive effort and will quickly become overly reliant on automation aids. Developers argue that LLM applications in health care are not automatic solutions as they require a human-in-the-loop approach, and that the risk of errors is adequately mitigated by relying on physician review. However, the perception of LLMs as outperforming humans may lead to overestimating the capabilities of the models. When humans begin to rely more heavily on LLM output, the automation argument is no longer valid. While the physician has the final determination in practice, this approach is grounded in two principles: the assumption that every physician will thoroughly review all output, every time, for every patient; and the reliance on disclaimers to cover liability and shift accountability from developers to physicians. This creates a new and tedious burden on physicians \u2014 proofreading content that was neither written nor dictated by the user is difficult to do well. Minimal research has been published to understand the net benefit on efficiency and cognitive burden when physicians\u2019 efforts are shifted from generating new content to reviewing content generated by LLMs. Furthermore, the use of LLMs increases physicians\u2019 accountability for something they did not write. The liability of missing a high-risk error in LLM output is a novel factor for consideration and not well understood. Further study on the use of LLMs to generate clinical documentation or clinical communication should consider the cognitive impact of proofreading and the increased accountability of physicians. We recommend integrating implementation science, user experience research, and collaborative input from policy makers, regulators, and professional medical associations to establish a shared accountability structure. This approach aims to protect patients while empowering physicians to perform their duties confidently, ultimately enhancing both patient safety and physician satisfaction.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400889",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400889",
      "title": "Using Large Language Models to Promote Health Equity",
      "summary": "We emphasize the emerging potential of LLMs to promote health equity by highlighting three promising use cases: detecting human biases (e.g., from clinical notes); creating structured equity-relevant databases from unstructured text; and improving equity of access to health information.",
      "content_text": "## Abstract\n\nWhile the discussion about the effects of large language models (LLMs) on\nhealth equity has been largely cautionary, LLMs also present significant\nopportunities for improving health equity. We highlight three such\nopportunities: improving the detection of human bias; creating structured\ndatasets relevant to health equity; and improving equity of access to health\ninformation.\n\n",
      "date_published": "2025-01-13T00:00:00+00:00",
      "authors": [
        {
          "name": "E. Pierson and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400889",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/8b997667-e09c-4dbb-a6ba-7c21b50554f2/aip2400889_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">While the discussion about the effects of large language models (LLMs) on health equity has been largely cautionary, LLMs also present significant opportunities for improving health equity. We highlight three such opportunities: improving the detection of human bias; creating structured datasets relevant to health equity; and improving equity of access to health information.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIra2400380",
      "title": "RAG in Health Care: A Novel Framework for Improving Communication and Decision-Making by Addressing LLM Limitations",
      "summary": "This review article explores and evaluates the opportunities in using retrieval augmented generation (RAG) to overcome existing LLM limitations when used within the healthcare and pharmaceutical domain. It provides a framework of how RAG works and describes its strengths and weaknesses when applied to the field of medicine.",
      "content_text": "## Abstract\n\nWithin the field of artificial intelligence (AI), large language models (LLMs)\nhave the potential to transform the delivery of medical information. LLMs, as\na subset of generative AI, have demonstrated value in content creation, idea\ngeneration, and interactive communication. However, their inherent\nlimitations, such as the need for up-to-date information, hallucinations of\nincorrect facts, and a reliance on public-domain data, restrict the full\npotential of generative AI within the health care setting. To address these\nlimitations, retrieval-augmented generation (RAG) offers a novel framework by\nconnecting LLMs with external knowledge, enabling them to access information\nbeyond their training data. Within the health care domain, additional datasets\ncould include peer-reviewed studies, gated medical compendiums, and the\ninternal policies of health care organizations such as hospitals or\npharmaceutical companies. By leveraging RAG, existing generative AI tools gain\nthe capability to consider both public and private information, expanding\ntheir application and enhancing accuracy and relevance within the health care\nsetting. The utility of RAG in the health care setting has yet to be fully\nexplored, but it has the potential to revolutionize the industry. This article\nseeks to outline present and future use cases of RAG for health care\ninformation exchange within both clinical and industrial settings.\n\n",
      "date_published": "2024-12-23T00:00:00+00:00",
      "authors": [
        {
          "name": "K.K.Y. Ng, I. Matsuba, and P.C. Zhang"
        }
      ],
      "tags": [
        "Review Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIra2400380",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/pb-assets/ai-site/images/AIra2400380_Zhang-1734963181703.jpg",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Within the field of artificial intelligence (AI), large language models (LLMs) have the potential to transform the delivery of medical information. LLMs, as a subset of generative AI, have demonstrated value in content creation, idea generation, and interactive communication. However, their inherent limitations, such as the need for up-to-date information, hallucinations of incorrect facts, and a reliance on public-domain data, restrict the full potential of generative AI within the health care setting. To address these limitations, retrieval-augmented generation (RAG) offers a novel framework by connecting LLMs with external knowledge, enabling them to access information beyond their training data. Within the health care domain, additional datasets could include peer-reviewed studies, gated medical compendiums, and the internal policies of health care organizations such as hospitals or pharmaceutical companies. By leveraging RAG, existing generative AI tools gain the capability to consider both public and private information, expanding their application and enhancing accuracy and relevance within the health care setting. The utility of RAG in the health care setting has yet to be fully explored, but it has the potential to revolutionize the industry. This article seeks to outline present and future use cases of RAG for health care information exchange within both clinical and industrial settings.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIpc2400927",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIpc2400927",
      "title": "Managing Patient Use of Generative Health AI",
      "summary": "In reviewing the benefits and risks of patients\u2019 use of generative artificial intelligence and policy approaches to increasing trust and containing risks associated with this use, the authors conclude that while government will play a role in helping patients use generative AI effectively, major responsibility will fall to the private sector.",
      "content_text": "## Abstract\n\nPatients\u2019 use of artificial intelligence, or patient AI (PAI), is now\nwidespread, promising both benefits and risks. There is an urgent need to\nassist patients with using these new technologies as safely and effectively as\npossible. Focusing on patients\u2019 use of large language models (LLMs) for health\ncare purposes, this article explores critical issues in the management of\ngenerative PAI in the United States, including constitutional limits on\ngovernment\u2019s regulatory authority, and identifies opportunities for public and\nprivate actors to help patients take advantage of generative PAI safely. With\nrespect to the public sector, there is a critical need for federal action to\nfund research on the benefits and risks of PAI and on the development of valid\nmetrics and performance standards for PAI. The federal government also needs\nto better protect the privacy and security of patient data shared with PAI,\nand to require transparency with respect to how LLMs used for health care\npurposes are funded, how they are developed, and how they perform. (Supported\nby the Commonwealth Fund.)\n\n",
      "date_published": "2024-12-20T00:00:00+00:00",
      "authors": [
        {
          "name": "D. Blumenthal and C. Goldberg"
        }
      ],
      "tags": [
        "Policy Corner"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIpc2400927",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Patients\u2019 use of artificial intelligence, or patient AI (PAI), is now widespread, promising both benefits and risks. There is an urgent need to assist patients with using these new technologies as safely and effectively as possible. Focusing on patients\u2019 use of large language models (LLMs) for health care purposes, this article explores critical issues in the management of generative PAI in the United States, including constitutional limits on government\u2019s regulatory authority, and identifies opportunities for public and private actors to help patients take advantage of generative PAI safely. With respect to the public sector, there is a critical need for federal action to fund research on the benefits and risks of PAI and on the development of valid metrics and performance standards for PAI. The federal government also needs to better protect the privacy and security of patient data shared with PAI, and to require transparency with respect to how LLMs used for health care purposes are funded, how they are developed, and how they perform. (Supported by the Commonwealth Fund.)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400640",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400640",
      "title": "A Multimodal Biomedical Foundation Model Trained from Fifteen Million Image\u2013Text Pairs",
      "summary": "BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications.",
      "content_text": "## Abstract\n\n### Background\n\nBiomedical data are inherently multimodal, comprising physical measurements\nand natural-language narratives. A generalist biomedical artificial\nintelligence (AI) model needs to simultaneously process different modalities\nof data, including text and images. Therefore, training an effective\ngeneralist biomedical model requires high-quality multimodal data, such as\nparallel image\u2013text pairs.\n\n### Methods\n\nHere, we present PMC-15M, a novel dataset that is two orders of magnitude\nlarger than existing biomedical multimodal datasets, such as MIMIC-CXR, and\nspans a diverse range of biomedical image types. PMC-15M contains 15 million\nbiomedical image\u2013text pairs collected from 4.4 million scientific articles.\nBased on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation\nmodel, with domain-specific adaptations tailored to biomedical vision\u2013language\nprocessing.\n\n### Results\n\nWe conducted extensive experiments and ablation studies on standard biomedical\nimaging tasks from retrieval to classification to visual question answering\n(VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of\nstandard datasets, substantially outperforming prior approaches. Intriguingly,\nby large-scale pretraining on diverse biomedical image types, BiomedCLIP even\noutperforms state-of-the-art radiology-specific models, such as BioViL, in\nradiology-specific tasks such as Radiological Society of North America (RSNA)\npneumonia detection.\n\n### Conclusions\n\nBiomedCLIP is a fully open-access foundation model that achieves state-of-the-\nart performance on various biomedical tasks, paving the way for transformative\nmultimodal biomedical discovery and applications. We release our models at\naka.ms/biomedclip to facilitate future research in multimodal biomedical AI.\n\n",
      "date_published": "2024-12-20T00:00:00+00:00",
      "authors": [
        {
          "name": "S. Zhang and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400640",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/d8ab9f42-1745-4f0b-b237-280d2f132674/aioa2400640_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Biomedical data are inherently multimodal, comprising physical measurements and natural-language narratives. A generalist biomedical artificial intelligence (AI) model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image\u2013text pairs.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets, such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image\u2013text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision\u2013language processing.</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question answering (VQA). BiomedCLIP achieved new state-of-the-art results in a wide range of standard datasets, substantially outperforming prior approaches. Intriguingly, by large-scale pretraining on diverse biomedical image types, BiomedCLIP even outperforms state-of-the-art radiology-specific models, such as BioViL, in radiology-specific tasks such as Radiological Society of North America (RSNA) pneumonia detection.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">BiomedCLIP is a fully open-access foundation model that achieves state-of-the-art performance on various biomedical tasks, paving the way for transformative multimodal biomedical discovery and applications. We release our models at aka.ms/biomedclip to facilitate future research in multimodal biomedical AI.</div></section>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2401088",
      "title": "Tackling Algorithmic Bias and Promoting Transparency in Health Datasets: The STANDING Together Consensus Recommendations",
      "summary": "We set out recommendations enabling those creating and/or using health care datasets to identify and transparently acknowledge underlying biases.",
      "content_text": "## Abstract\n\nWithout careful dissection of the ways in which biases can be encoded into\nartificial intelligence (AI) health technologies, there is a risk of\nperpetuating existing health inequalities at scale. One major source of bias\nis the data that underpins such technologies. The STANDING Together\nrecommendations aim to encourage transparency regarding limitations of health\ndatasets and proactive evaluation of their effect across population groups.\nDraft recommendation items were informed by a systematic review and\nstakeholder survey. The recommendations were developed using a Delphi\napproach, supplemented by a public consultation and international interview\nstudy. Overall, more than 350 representatives from 58 countries provided input\ninto this initiative. 194 Delphi participants from 25 countries voted and\nprovided comments on 32 candidate items across three electronic survey rounds\nand one in-person consensus meeting. The 29 STANDING Together consensus\nrecommendations are presented here in two parts. Recommendations for\nDocumentation of Health Datasets provide guidance for dataset curators to\nenable transparency around data composition and limitations. Recommendations\nfor Use of Health Datasets aim to enable identification and mitigation of\nalgorithmic biases that might exacerbate health inequalities. These\nrecommendations are intended to prompt proactive inquiry rather than acting as\na checklist. We hope to raise awareness that no dataset is free of\nlimitations, so transparent communication of data limitations should be\nperceived as valuable, and absence of this information as a limitation. We\nhope that adoption of the STANDING Together recommendations by stakeholders\nacross the AI health technology lifecycle will enable everyone in society to\nbenefit from technologies which are safe and effective. (Funded by The NHS AI\nLab and The Health Foundation, and supported by the National Institute for\nHealth and Care Research [NIHR].)\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "J.E. Alderman and Others"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2401088",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Without careful dissection of the ways in which biases can be encoded into artificial intelligence (AI) health technologies, there is a risk of perpetuating existing health inequalities at scale. One major source of bias is the data that underpins such technologies. The STANDING Together recommendations aim to encourage transparency regarding limitations of health datasets and proactive evaluation of their effect across population groups. Draft recommendation items were informed by a systematic review and stakeholder survey. The recommendations were developed using a Delphi approach, supplemented by a public consultation and international interview study. Overall, more than 350 representatives from 58 countries provided input into this initiative. 194 Delphi participants from 25 countries voted and provided comments on 32 candidate items across three electronic survey rounds and one in-person consensus meeting. The 29 STANDING Together consensus recommendations are presented here in two parts. Recommendations for Documentation of Health Datasets provide guidance for dataset curators to enable transparency around data composition and limitations. Recommendations for Use of Health Datasets aim to enable identification and mitigation of algorithmic biases that might exacerbate health inequalities. These recommendations are intended to prompt proactive inquiry rather than acting as a checklist. We hope to raise awareness that no dataset is free of limitations, so transparent communication of data limitations should be perceived as valuable, and absence of this information as a limitation. We hope that adoption of the STANDING Together recommendations by stakeholders across the AI health technology lifecycle will enable everyone in society to benefit from technologies which are safe and effective. (Funded by The NHS AI Lab and The Health Foundation, and supported by the National Institute for Health and Care Research [NIHR].)</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIcs2400497",
      "title": "How Generalizable Are Foundation Models When Applied to Different Demographic Groups and Settings?",
      "summary": "An evaluation of the foundational model RETFound on an Asian-specific dataset indicates the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need to include more diverse data and collaborate globally on research.",
      "content_text": "## Abstract\n\nRETFound is a retinal image\u2013based foundational artificial intelligence (AI)\nmodel that can be fine-tuned to downstream tasks. However, its\ngeneralizability to Asian populations remains unclear. In this study, we fine-\ntuned RETFound on an Asian-specific dataset. We then evaluated the performance\nof RETFound versus a conventional Vision Transformer model (pretrained on\nImageNet) in diagnosing glaucoma and coronary heart disease and predicting the\n3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d\ndataset, RETFound showed no significant improvement compared with a\nconventional Vision Transformer model (area under the curves [AUCs] of 0.863,\n0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2).\nFurthermore, in scenarios with limited training data (fine-tuned on \u226425% of\nthe full dataset), RETFound showed a slight advantage (up to a maximum AUC\nincrease of 0.03). However, these improvements were not statistically\nsignificant (all P\u22650.2). These findings indicate the challenges foundational\nAI models face in adapting to diverse demographics, emphasizing the need for\nmore diverse data in current foundation models and the importance of global\ncollaboration on foundation model research.\n\n",
      "date_published": "2024-12-18T00:00:00+00:00",
      "authors": [
        {
          "name": "Z. Xiong and Others"
        }
      ],
      "tags": [
        "Case Study"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIcs2400497",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/925aa4d2-c256-45bf-ade8-81f317c70045/aics2400497_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">RETFound is a retinal image\u2013based foundational artificial intelligence (AI) model that can be fine-tuned to downstream tasks. However, its generalizability to Asian populations remains unclear. In this study, we fine-tuned RETFound on an Asian-specific dataset. We then evaluated the performance of RETFound versus a conventional Vision Transformer model (pretrained on ImageNet) in diagnosing glaucoma and coronary heart disease and predicting the 3-year risk of stroke in an Asian population. When fine-tuned on a \u201cfull\u201d dataset, RETFound showed no significant improvement compared with a conventional Vision Transformer model (area under the curves [AUCs] of 0.863, 0.628, and 0.557 vs. 0.853, 0.621, and 0.543, respectively; all P\u22650.2). Furthermore, in scenarios with limited training data (fine-tuned on \u226425% of the full dataset), RETFound showed a slight advantage (up to a maximum AUC increase of 0.03). However, these improvements were not statistically significant (all P\u22650.2). These findings indicate the challenges foundational AI models face in adapting to diverse demographics, emphasizing the need for more diverse data in current foundation models and the importance of global collaboration on foundation model research.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIe2401073",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIe2401073",
      "title": "The Promise and Perils of Autonomous AI in Science",
      "summary": "We recommend the development of clearer guidelines and ethical standards for the use of artificial intelligence (AI) in research, fostering human-AI collaboration to enhance research quality while preserving human oversight and integrating the innovative \u201cdata-chaining\u201d transparency mechanisms more broadly to support reproducibility and traceability.",
      "content_text": "## Abstract\n\nIn this edition of _NEJM AI_ , Ifargan and colleagues present data-to-paper,\nan autonomous platform designed to mimic human scientific practice by guiding\na large language model through a stepwise research process to produce complete\nresearch papers. While the platform shows promise, significant errors\nfrequently occur with complex datasets, requiring human intervention for\ncorrection. Here, we explore the risks of over-reliance on such automated\ntools, echoing potential threats to scientific integrity and a surge in low-\nquality publications, while also considering their potential role in the\nreproduction and verification of scientific findings. To address these\nchallenges, we recommend the development of clearer guidelines and ethical\nstandards for the use of artificial intelligence (AI) in research, fostering\nhuman\u2013AI collaboration to enhance research quality while preserving human\noversight, and integrating the innovative \u201cdata-chaining\u201d transparency\nmechanisms more broadly to support reproducibility and traceability.\n\n",
      "date_published": "2024-12-16T00:00:00+00:00",
      "authors": [
        {
          "name": "J. Gao and E.M. Harrison"
        }
      ],
      "tags": [
        "Editorial"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIe2401073",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">In this edition of <i>NEJM AI</i>, Ifargan and colleagues present data-to-paper, an autonomous platform designed to mimic human scientific practice by guiding a large language model through a stepwise research process to produce complete research papers. While the platform shows promise, significant errors frequently occur with complex datasets, requiring human intervention for correction. Here, we explore the risks of over-reliance on such automated tools, echoing potential threats to scientific integrity and a surge in low-quality publications, while also considering their potential role in the reproduction and verification of scientific findings. To address these challenges, we recommend the development of clearer guidelines and ethical standards for the use of artificial intelligence (AI) in research, fostering human\u2013AI collaboration to enhance research quality while preserving human oversight, and integrating the innovative \u201cdata-chaining\u201d transparency mechanisms more broadly to support reproducibility and traceability.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIp2400672",
      "title": "Ethical Use of Artificial Intelligence in Medical Diagnostics Demands a Focus on Accuracy, Not Fairness",
      "summary": "We argue that in developing and evaluating AI tools for medical diagnostics, rather than focusing on fairness criteria that quantify disparities between protected subpopulations, our first and most important objective should be to track and maximize diagnostic accuracy for all subpopulations.",
      "content_text": "## Abstract\n\nArtificial intelligence (AI) promises to be a transformative technology for\nmedicine and health care. As such, there is an increasing interest in ensuring\nits ethical use. In this perspective, we consider the employment of AI for\nmedical diagnostics, where the goal is the detection and classification of an\nunderlying pathology, based on data such as patient information, clinical\npresentation, tests, and imaging. We argue that instead of prioritizing\nfairness criteria that measure disparities between protected groups, the\nprimary goal should be to assess and enhance diagnostic accuracy within each\nsubpopulation. This approach shifts the focus from optimizing overall\npopulation accuracy to ensuring maximal accuracy in each subpopulation. Our\nperspective implies that we should be using all available information,\nincluding protected group identity, in our methods. We decouple the goal of\naccurate diagnosis from fairness considerations in screening and postdiagnosis\nclinical decisions, which often require the allocation of finite resources.\nFurthermore, we underscore the importance of collecting high-quality and\nrepresentative datasets for each subpopulation, including ensuring that the\nground truth labels we train and with which we evaluate are unbiased.\n\n",
      "date_published": "2024-12-13T00:00:00+00:00",
      "authors": [
        {
          "name": "M.R. Sabuncu, A.Q. Wang, and M. Nguyen"
        }
      ],
      "tags": [
        "Perspective"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIp2400672",
          "mime_type": "application/pdf"
        }
      ],
      "content_html": "<h2 property=\"name\">Abstract</h2><div role=\"paragraph\">Artificial intelligence (AI) promises to be a transformative technology for medicine and health care. As such, there is an increasing interest in ensuring its ethical use. In this perspective, we consider the employment of AI for medical diagnostics, where the goal is the detection and classification of an underlying pathology, based on data such as patient information, clinical presentation, tests, and imaging. We argue that instead of prioritizing fairness criteria that measure disparities between protected groups, the primary goal should be to assess and enhance diagnostic accuracy within each subpopulation. This approach shifts the focus from optimizing overall population accuracy to ensuring maximal accuracy in each subpopulation. Our perspective implies that we should be using all available information, including protected group identity, in our methods. We decouple the goal of accurate diagnosis from fairness considerations in screening and postdiagnosis clinical decisions, which often require the allocation of finite resources. Furthermore, we underscore the importance of collecting high-quality and representative datasets for each subpopulation, including ensuring that the ground truth labels we train and with which we evaluate are unbiased.</div>"
    },
    {
      "id": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "url": "https://ai.nejm.org/doi/full/10.1056/AIoa2400555",
      "title": "Autonomous LLM-Driven Research \u2014 from Data to Human-Verifiable Research Papers",
      "summary": "Our study demonstrates a potential for AI-driven acceleration of scientific discovery in biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.",
      "content_text": "## Abstract\n\n### Background\n\nArtificial intelligence (AI) promises to accelerate scientific discovery, but\nit remains unclear whether AI systems can perform fully autonomous research,\nand whether they can do so while adhering to key scientific values, such as\ntransparency, traceability, and verifiability. The aim of this study was to\ndevelop and evaluate an AI-automation platform that performs transparent,\ntraceable, and human-verifiable scientific research.\n\n### Methods\n\nTo mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation\nplatform that guides interacting large language model (LLM) agents through a\ncomplete stepwise research process that starts with annotated data and results\nin comprehensive research papers, while programmatically backtracing\ninformation flow and allowing human oversight and interactions. The platform\ncan run fully autonomously (in autopilot mode) or with human intervention (in\ncopilot mode).\n\n### Results\n\nIn autopilot mode, provided only with annotated data, data-to-paper raised\nhypotheses; designed research plans; wrote and debugged analysis codes;\ngenerated and interpreted results; and created complete, information-traceable\nresearch papers. Even though the research novelty of manuscripts created by\ndata-to-paper was relatively limited, the process demonstrated the autonomous\ngeneration of de novo quantitative insights from data, such as unraveling\nassociations between health indicators and clinical outcomes. For simple\nresearch goals and datasets, a fully autonomous cycle can create manuscripts\nthat independently recapitulate the findings of peer-reviewed biomedical\npublications without major errors in about 80 to 90% of cases. Yet, as goal or\ndata complexity increases, human copiloting becomes critical for ensuring\naccuracy and overall quality. By tracking information flow through the steps,\nthe platform creates \u201cdata-chained\u201d manuscripts, in which downstream results\nare programmatically linked to upstream code and data, thus setting a new\nstandard for the verifiability of scientific outputs.\n\n### Conclusions\n\nOur work demonstrates the potential for AI-driven acceleration of scientific\ndiscovery in data-driven biomedical research and beyond, while enhancing,\nrather than jeopardizing, traceability, transparency, and verifiability.\n\n",
      "date_published": "2024-12-03T00:00:00+00:00",
      "authors": [
        {
          "name": "T. Ifargan and Others"
        }
      ],
      "tags": [
        "Original Article"
      ],
      "attachments": [
        {
          "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400555",
          "mime_type": "application/pdf"
        }
      ],
      "image": "https://ai.nejm.org/cms/asset/06a1651e-79ce-4d95-887b-9fbf5ed07916/aioa2400555_f1.gif",
      "content_html": "<h2 property=\"name\">Abstract</h2><section id=\"abs-sec-1\"><h3>Background</h3><div role=\"paragraph\">Artificial intelligence (AI) promises to accelerate scientific discovery, but it remains unclear whether AI systems can perform fully autonomous research, and whether they can do so while adhering to key scientific values, such as transparency, traceability, and verifiability. The aim of this study was to develop and evaluate an AI-automation platform that performs transparent, traceable, and human-verifiable scientific research.</div></section><section id=\"abs-sec-2\"><h3>Methods</h3><div role=\"paragraph\">To mimic human scientific practices, we built \u201cdata-to-paper,\u201d an automation platform that guides interacting large language model (LLM) agents through a complete stepwise research process that starts with annotated data and results in comprehensive research papers, while programmatically backtracing information flow and allowing human oversight and interactions. The platform can run fully autonomously (in autopilot mode) or with human intervention (in copilot mode).</div></section><section id=\"abs-sec-3\"><h3>Results</h3><div role=\"paragraph\">In autopilot mode, provided only with annotated data, data-to-paper raised hypotheses; designed research plans; wrote and debugged analysis codes; generated and interpreted results; and created complete, information-traceable research papers. Even though the research novelty of manuscripts created by data-to-paper was relatively limited, the process demonstrated the autonomous generation of de novo quantitative insights from data, such as unraveling associations between health indicators and clinical outcomes. For simple research goals and datasets, a fully autonomous cycle can create manuscripts that independently recapitulate the findings of peer-reviewed biomedical publications without major errors in about 80 to 90% of cases. Yet, as goal or data complexity increases, human copiloting becomes critical for ensuring accuracy and overall quality. By tracking information flow through the steps, the platform creates \u201cdata-chained\u201d manuscripts, in which downstream results are programmatically linked to upstream code and data, thus setting a new standard for the verifiability of scientific outputs.</div></section><section id=\"abs-sec-4\"><h3>Conclusions</h3><div role=\"paragraph\">Our work demonstrates the potential for AI-driven acceleration of scientific discovery in data-driven biomedical research and beyond, while enhancing, rather than jeopardizing, traceability, transparency, and verifiability.</div></section>"
    }
  ]
}